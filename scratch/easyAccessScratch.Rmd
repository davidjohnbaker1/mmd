---
title: "ScratchMaterial"
author: "David John Baker"
date: "August 29, 2018"
output: html_document
---

Is there soemthing to be said about the change point access for when there is reentry of the transcription function

## Intro 



I argue that tools for understanding melodic dictation best because as we currently understand it, I see us operating in a Kuhnian normal science where much can be learned by just using the tools in front of us.

While learning aural skills has been a hallmark of being educated within the Western conservatory tradition, the rationale behind both the how and why of aural skills is often thought of as being esoteric.
Throughout the past century, people have disagreed on exactly how one does go about learning a melody with different areas of research each attacking the problem from a different angle.

Some line here about if we really want to understand what is happening we need to know about causal factors going on here and have experimental manipulation and things like making models of the whole thing or talk about what Judea Pearl thinks about the ability to do some sort of causal modeling with diagrams.
Great to rely on some sort of anecdotal evidence, but if we are going to put things on the line with our education then we need to be able to make some sort of falsifiable claims about what we are doing.
Can only do that through the lens of science. 


## Chapter 1

The sequence of steps occurs for each chunk of music that is extracted during dictation and in the case of TABLE X, happens twice.
In his text, Karpinski presumes this process could happen with a melody that can be extracted in two parts, which according to his prescribed formulas^[In the third chapter of _Aural Skills Acquisition_, Karpinski suggests that the number of playings should be calculated based on the formula FORMULA, where DEFINE WHAT HE SAYS] would consist of a melody from twelve to twenty notes long for listeners with few or no chunking abilities. 
As stated above, reviewing the Karpinksi four step model will establish the current state of thinking on melodic dictation and thus create a groundwork from which this current study will build.

One could even go further and borrow a thought experiment from the world of neuroscience and imagine a double dissociation with melodic dictation ability and musical skill.
Thinking about one's colleagues 

Even using a simple thought experiment using the old double dissicoation thign that htey love in neuroscience where you could imagine both situations where someone was probably a very good musician, but not that great at melodic dictation, or the converse where someone might be very good at melodic dictation, but is not that good of a musician (whatever that means) would indicate that there is probably not that strong of a casusal link between dictation abilities as measured by grades in an aural skills class and the infinite possiblities of musical activities that are even possible.  

This similarity between studies should still be interpreted in light of other research that argued against a need for higher working memory functionality in expert level performances (e.g., HOWE,DAVIDSON, & SLOBODA, 1998). 


In her textbook _Practical Ear Training_ Janet McLoud McGaughy notes that

> qualified musicians must develop reading, singing, and notational skills in order to achieve acuity of aural perception and to make effective use of that acuity" (p. i)."

which makes the point that the development of ear training is about developing acuity in aural percepiton which then is then transfered over for other musical activity.
In his forthcoming article in the Journal of Music Theory Pedgagogy, UTAH MAN SAYS.....

UTAH MAN SAYS IT BEST WHEN HE SAYS 

UTAH MAN QUOTE

#### Verbal model, has problems, OK for pedagogy

As a verbal model, Karpinski's model of dictation makes sense and breaks down a very complicated cognitive process into discrete, sequential steps.
This model, whether or not Karpinski intented it to be an actual cognitive model, I don't know, assumes serial processing of information and does not specify any of the actual parameters for each of the steps.
This model serves as a great stepping off point for this dissertation, and its good for me that he didn't actually specificy any of the actual parameters, he just said certain things happen and gave an approximate order of the whole thing.
He talks about a lot of the parameters leading up to it all,
Even talks about things like representation and other paramters that are going to influence it all.
Did not attempt to pin down any of the exact parameters of his model, but this is just me wanting to forshadown Chapter 6 where I take each of these issues and implement them in a computational model. 
While covering many bases, one of the problems with using a verbal model as opposed to a computational one is that by not specificying exaclty what happens when and how everything is represented, the model can manifest itself in a multiude of different ways.

Take for example the often cited model of working memory by Alan Baddely and Grahm Hitch which posits a modular view of workign memory capcacity [@baddeleyWorkingMemory1974].
As discussed in CITE, although the model only has X amount of distinctive systems, the fact that they left it to be a verbal model of memory, and did not tack down exactly how each parameter functions, when built as computational model actually ends up yielding 156 different models. 
While I will reseve discussing how many possible models might exists as derrived from the Karpinski for Chapter 6, it is suffice to say that clearly establishing the degree to which part of the model contributes in concrete way only helps with furthering the literature.

When viewed from a pedagogical stand point, espcielaly given how litle time people have, it would basically be impossible to attend to each of these things in a gradient fashion in the standard 15 week semester that people have. 
When people teach this, need to rely very much on intuition and basically adjust to the level of your class and syllabus and whatever on the fly in order to convey the most amount of musical materials possible. 

Conversly, when viewed from an experimental standpoint, each of the above mentioned processes is bascially an experimental parameter waiting to be investigated.
Additionally, the process that is put forth by KARPINSKI 1999 provides a stepping off point for positing some sort of comutational model.
Going to add substatinally to it in that going to take into account both how difficulty the melodies are, something we have intiutive understanding of, but could be operationalized, and also going to try to model individual differences based on factors that prior literature would suggest are different in individuals and should be considered in any sort of modeling going forward. 
In addition to these above parameters discussed by Karpinski, now review other factors that could then contribute to this process.

Clearly, would be worth understanding the concept at a deeper level since clealry they have done a lot of great work on the topic. 
Adoptiong chrnological take of the history of research the heavy hitters are 
* Attkinson and Shriff 
* Miller 1956 and why that is a bad idea
* Baddely and Hitch 
* Nelson Cowan + need for complex span

For reasons discussed on p 18 of cowan 2005 and page 42 , choose to adopot a Cowanian view where WMC is basically the window of attention.
Additinally it's worth noting initially that you can't directly apply these frameworks to memory for musical material because after reading page 109 in Cowan, important to note that all melodies appear in serial order. 
To make analogous task based on something like complex span tasks, would need to take certain set of notes and then always a finate setb be played back in arandom order (see chatper 3)

## Old Drafts of Chapter Structure BELOW THIS LINE 
## Theoretical Background and Rationale

As stated above, melodic dictation is a hugely complex process.


What am I going to do in this chapter?
  + Intro to aural skills and melodic dictation 
  + Talk about the factors that go into it (basically real time suduko)
    + Cognitive
    + Musical 
  + Note that reserach is messy, need to think of it polymorphically
 

Even anecdoatal evidence suggests that people who are good at this tend to stay good and people who are bad at it tend to stay bad

Other things that I might not want to bring up right now is that a lot of this could just be thought of as relating to a big thought experiemnt where you could imagine that there are many musicians from lots of different muscial cultures (bring in the ethnos!) that could imagine being sucessful at what they do without having this ability to take melodic dicatation.
Is it just a party trick? 
Also here would be a good place to jump off of and talk about the lack of transfer.

Where do people learn it? -- Music school
Part of large thing of aural skills 
Absoulte ubiquity, but for the amount that it is done, relatively understudied

   

Not only is this process intenstive, but also ubiquitous 

## 

@karpinskiAuralSkillsAcquisition2000 schematizes this ability into a four step process of hear, remember, understand, notate (See Figure 1?).

While 
Some researchers have estimated that XX amount of processes are needed to sucessfully execute this task FIND CITATION. 

-- Should be drawing here from ICMPC paper?

## Order of Chapter 1

* Hook on why this is important
* What is melodic dictation?
  + Process (reasons of why factors)
  + Who has talked about it before?
  + Why is it important -- training ear and transfer
* Most of the people who are talking about this are pedagogical, really it's perceptual 
* Here is huge list of literature of things that might contribute
  + Pedagogical factors
  + Cognitive Factors
  + Musical Factors 
  + All of these are moving parts, need better tools to describe
* While this is the narrative, does not conform to updated view of polymorphism of musicianship

## Order of Chapter 2

# Individual Differences

### Dictation without Dictation

One of the most popular paradigms used in music perception research, specifically the memory for melody literature, are same-different memory tasks [@halpernMemoryMelodies2010].
These tasks require individuals to hear a melody, retain it in memory, then hear a second melody either at the exact same pitch level, or transposed, and then make a judgment if the two melodies were the same or different.
It is important to note this type of task requires both retention of musical material and a secondary cognitive task: deciding if the melodies were identical or not.
In many ways, this type of task mirrors the first two steps of melodic dictation.
Same-different paradigms require individuals to first hear a finite amount of musical material, then while held in conscious representation, perform some sort of mental action on the contents of memory by making a similarity judgment.
If one were to acknowledge these similarities in mental processes, using same-different paradigms could provide a way to investigate the first two steps of Karpinski's model of melodic dictation in the general population which may or may not have musical training.

## Literature Review


II. Way MD works 
  As stated bove, all expect the same baseline
  Get better with extractive lsitening and chunking
    Def extractive lsitening, this also would be question of AUDIOTRY DISTRACTION ALA EMILY
    Def Chunking as well
  Logic increases the both, get better at melodic dication.
  But does the literature support that?

III. Mem 4 melodies
  M4M says that no musical advantage in this, tho not dictation
    example 1 2 3
  But bc m4m is not MD, can't jsut go for it
    MD is layered
    m4m is jsut a chocie
  Looking at other lit, yes, there is an effect of training
    see this with general ingelligecne (examples)
    see this with memory (talamin)
    Mem exp when short an tonal, just like MD! 
    has been interpreted as fx of traning
    but probably reversed in direciton
  Cognition
    Obvious a link here, assumed to be FX
    could be some combination of Train, int, wmc
    expecially imporant to consider effects of WM 
    see all of bersz
    really need ot look at all at once to distentangle
    need paradigm that does xyz
    Other people have started to do this, okda slevc
  How
    first assume that M4M is MD 12 
    Experiment with individual factors
    Path analysis 


### Hook

Academic interest in the relationship between musical and cognitive ability dates back over a century.
In 1904, Charles Spearman [@spearmanGeneralIntelligenceObjectively1904] reported that Carl Seashore's pitch discrimination test (Seashore) correlated with his conception of general intelligence (.70, p.276), yet used an extensive footnote to highlight that there were "many notable instances will easily be found where musical ability is apparently divorced from General Intelligence"(p 275) suggesting that the relationship between music and cognitive ability is perhaps more complex than he originally reports.
He thought that it could be accounted for with......
But accounting for the last 100 years of research in this area demonstrates that great progress actually has been made in attempting to understand the relationship between music and cognitive ability. 
-- straight into polymorphism of ability

People tend to frame the important studying of the link between two as possible way to improve cognition, thus improve quality.
Note here that this tethering is danger zone IMO bc first devalues music as cultural, second if connection goes based funding so do the arts. 

Audience: Individuals familiar with cognitive science that would think music is good way to talk about cognitive abilites

Eventually i will say that 
* WMC is really governing a lot of this work and people need to control for this pre-existing difference
* 
## Introduction

hile musical ability is perhaps more affected by training than the other measures that were of interest to Spearman (i.e. language and mathematics), the relationship between musical and cognitive ability, and the factors that mediate their relationship still has yet to be fully understood.

* Current state suggests that in some cases, musicians tend to do better than non musical peers
* Okada and Slevc see this as explained by three possible hypotheses

1. Music makes you smarter
2. Smarter People take music lessons
3. Higher functioning people tend to take music lessons and music training amplifys already pre-existing differences.

It's important to get at this issue because if there were any causal elements found, this would have big implications since enhancing cognitive abilities would bring along with it the other benefits like X Y Z
While this might be a nice side fiding, important to note that even if a causal link were to be established, tethering music engagement to its ability to make someone smarter is a bad idea for numerous reasons probably given by schellenberg. 

Really the interesting reason to look into this relationship is that music, unlike other stimuli, provides a unique insight into looking things like gf and wmc bc no semantic information
For example, if what is more likley the third hypothesis is true and there are just pre-existing differences that do not change but are mabye amplified with music lessons, this would have serious implications for a lot of the findings in the music psychology literature. 


## Individual Differences

While explicit musical training is proposed to get people better at this...
Literature on individual differences says other things might contribute.
For example xyz
So while pedagogically, yes training, a full model of md needs to acknowledge the diversity of listening abilities.
This is especially relvant bc some literature does not suggest musician advantage

As stated above, the first two steps of Karpinski's model of melodic dictation are not exclusive to trained musicians.
These steps involve first hearing the melody and then retaining the melody in conscious awareness.
From a pedagogical and psychological standpoint, it would be safe to assume that individuals differ in their ability to memorize musical material.
Some individuals perform well, while others do not.
Presumably these differences in performance can be attributed to a number of factors.

One of the first factors --and perhaps most obvious to consider-- that might explain differences in ability would be an individual's musical training.

Musicans tend to have better memory
they also do better with other cognitive tasks
but really this is reversed
If you look at memory for melodies, musicians do not have an advantage
Seems to be much more like domain expertise thing

As noted in the previous chapter, people with some degree of musical training tend to outperform their less musically trained peers on many cognitive tasks both in terms of memory and general problem solving [@schellenbergMusicNonmusicalAbilities2017].

A meta-analysis by Talamini and colleagues [@talaminiMusiciansHaveBetter2017] highlighted differences in memory are especially pronouced when the stimuli used in the experiments was tonal.
This finding suggests a relationship between muscial training and an increase in memory for musical material.

Additionally^[Could drop the stuff on intelligence], musically trained individuals perform better on other tests of cognitive ability.
Most prominant in many of these studies are findings that link intelligence and musical training.


--- there is a reverse here

So while the literatre above provides a body of evidence to suggest that musically trained individuals outperform their non-muscial peers-- with the direction of causality yet to be firmly established-- musical training is not and imporantly cannot be the sole determining factor in how well an individual remembers a melody.
Presumably other factors contribute in people's ability to retain musical information.
One could consider a simple thought experiment where there might be people without musical training who, just due to their innate abilities, are able to outperform people with musical training based on their cognitive ability.

These are the first steps as well in any melodic dictation task.
this is cognitive ability, defined as XXXX to do task
But in addition to congitive ability, reason to believe that musical training will also affect this

So in order to get a better idea of melodic dictation, need to look at how most people do when having to do short term melody memory and eventually manipulate it.
In order to do this, need to look at variables that might be at play.

### Musical Training

Tacit in many pedagogical assumptions is that most people start out roughly with the same ability to remember musical material and that this capacity increases with musical training as noted above.
Karpinski defines this baseline limit of the amount of melodic information that can enter _short-term melodic memory_ as "somewhere between five and nine notes", referencing Miller's magic number seven^[Should I footnote here saying why this was not meant to be taken literally?] [@millerMagicalNumberSeven1956].
Karpinski supports this claim with one experimental study by Marple (n.d.!!!) who corroborated Miller's limits (p.78) and additionally noted that the binding of musical features beyond that of pitch extended the range to approximelty six to ten notes.
Similar findings were reported by Tallarico (XXX), Long (XXXX), and Pembrook (XXXX) who claimed the note limit to be between seven and eleven notes.^[For reasons noted in previous chapters (SECTION), it is important to stress the maximum about of notes able to be memorized should not be direclty interpreted as a one to one mapping of the amount of items that can be held in short term memory.
Breifly reviewing earlier points, the first reason is that Miller's number 7 was never meant to be taken literally as a number used to investigate the items of memory, but rather a rheatorical device for a keynote address he was asked to give.
Secondly, lieterature on the capacity limits of memory need to account for chunking mechanisms, most of which are bountiful in the musical domain as noted in SECTION.]

In order to extend the capacity for musical memory, Karpinski puts forward two possible stragegies.
The first is extractive listening and the other is chunking.
Extrative listening is a processes described as ...
Chunking on the other hand is a listener's ability to group certain parts of sensory information together, so that more information can take up less of the finite space in short-term muscial memory.
According to Karpinkski, chunking increases with increases in musical understanding.


This is also backed up with a host of literature showing htat musicians tend to outperform
Especially when the task in musical 
-- look at these examples here

Taken as a whole, much of the literature could be interpreted to suggest that listeners begin as a tabula rasa, then as they engage with more musical activity, this activity affords them the ability to in perform better on musical tasks.
While this might seem like an intuitive explanation for the findings, three large problems exist with this interpretation.

The first is that how musicianship is measured varies from study to study, thus making interpreation of the data much harder.
In fact, music psychology in general suffers from having not standardized a way to measure musical enagement.
Talamini note this problem in the aforementioned meta-analysis saying that 

> Quote

and suggest that music psychology might consider moving towards more "catch all measures" of musicality such as the Goldsmiths Musical Sophistication Index CITE or the MUSEBAQ.
Both tools use latent variable approaches that attempt to smooth over some of the problems with measuring such a messy construct.
But one problem with adopting these tools is that it forces some sort of ontological commitment CITE FROM POT that has been critized by other CITE MEEE.


- but one problem is that maybe that there more domain general processes accounting for this
- one way that other people have looked at it os OS
- They think about it as LV with three separate sets they define as xyz
- thing is, this is very close to WM (what they call separate)


The second problem with interpreting the literature as practice driving all these effects is that this might confound the direction of causalitiy.
As noted above, most of the evidence suggesting a relationship here derrives from cross sectional designs and is limited in its ability to posit causal relationships.

Thirdly, these studies as a whole lack consistent control of covariates that might confound the findings (OS).
Factors such as ... (Stuff from OS)

Considering these three confounds, it is only possible then to consider literature in one of three ways according to OS.

1
2
3

Thing that makes the mnost sense is probably what GS says that it exacerbates pre-existing differences.

but note a lot of this is general fluid intelligence, not WM! 

So if this is the case, we need to look into other factors that might then also play a role

OS looked at this a bit ago with the Gold-MSI and the idea of executive functioning
-- paper summary here

Here they break up EF into a few different categories,
-- note here the parallels on WM and melodic dictation.

One of the three skills of EF is updating.
Defined as x
And has been conceptualized by some as WMC.
And if you think about it is basically MD.

In fact, this parallel is not even that new 

Whether conceptualzed as the updating component of executive function (MIYAKE) or as working memory capacity (COWAN), working memory tasks share many similarities with tasks used in the music perception literature.
Berz [@berzWorkingMemoryMusic1995], in his 1995 article pointed this out.

Looking at working memory we are not the first to point this out.
Berz did it here and noted that could just be wmc.
Berz contiues in vain of Baddely and Hitch
They solve the problem with having loops.
Berz continues in this tradiition by setting up another loop for music
But this is a problem for a few reasons.

if you define it as complex, need to be doing complex

For those reasons, decide instead to adopt the Embedded Process Model by Cowan.
It works like this.
And will eventually be more suited for what we are trying to do.
Also mention here the problem with Karpinski model that it doesnt say waht is actively rehearsed.

So the task now is now to investrigate how all of these factors come together.
First is to get a task that uses the first two steps of the Karpinski model that is accesible to the non-musically trained.
Then idea is to parse out the data to see which of the variables contributes to this task.

Having established a link between wmc tasks and md, need to now find a task that will mirror this bc it contributes
Link back to MMD! 


HOLDER 

### General intelligence and WMC 

* His rationale was that he thinks there was a musical loop in the context of modular models of wmc ala baddely 
* Not nessecarily the case since there are other theoretical frameworks to talk about music 
* As stated above, looking at music provides a unique insight into contributing to discussions of wmc

### Defining of terms

* Having established this is a good problem, time to talk about what I think these terms mean
* This is what gf is 
* These are the people that have studied it and what they have found 
* This is what wmc is as defined by Cowan 
* Again, very clear parallels to wmc and if we can answer them w good evidence on large scale, good reason to think otherwise small
* We see that in this.... literature
* Could it be that there is a musical loop here ala Berz, Williamson, and would that explain Li Cowan Saults?

* See, it's a great qustion to investigate 
* Tho one big problem with music is that it's a lot harrier than other stimuli 
* Every problem with chunking that Cowan 2005 talks about could be applied to music
* Cant assume one note means one unit of memory and so on 
* On top of that you have Pearce suggestion that things are encoded differently 
* Also have problems of corpus distributions similar to words, good evidence for SLR and PPH 

* regardless of this hariness, need to get further into it
* Okada and slevec show that EF is also really important to make picture clearer
* tho if you really want to to do working memory capacity, need to do complex span
* the reason for this is xyz, can continue on from there
* Again, WMC is underlying a lot of music stuff, we should see it on large scale 

* Start setting up hypotheses 
* What best predicts someone's ability to do well on a musical memory test? 
* Not going to go straight to melodic dictation, first look into Berz 
* If it really is musical training, we should be able to see that using measures of musical survey
* If it's actually some sort of cognitive process, then that should have more predictive ability 
* We can figure this out by fitting a series of models to the data and seeing which matches up best and it's measurements
* Great ensemble of tests is the Gold-MSI bc it has self report and objective, and people have replicated it 
* Specifically there are two objective tests
* Describe them here
* not exactly mmd, but similr skill set 
* IF we accept these DVs, THEN we should be able to predict them with self reports and measures of WMC and gf
* Do this with hierarchical LVM ala Elliott paper 

* List hypotheses and predictions


Two questions here 

* Was Berz 1995 correct in asserting that there is a musical loop using the modular view of WM? 
* To address this, need to talk about modular vs domain general models 
* And music is great to look into this because maybe it would act differently than words
* Also you can manipulate different kinds of musical material which would help out 

Questiosn to be answered in this chapter 
* Is, as berz said, WM something very important for music perception literature, are we just measuring that?
* Can address this by doing a series of structrual equation models that see how things are predictive
* Also would assume that you could see statistically identical measurments for evidence that they are the same. 

Conclusions to be drawn 

* Yes, WM is important, especially at the higher level
* Are there span differences responsible for any predictive things?
* How does this differ from gf intelligence 
* Can you fit this at all in with process overlap theory? 


* Most of this literature comes from pedagogy
* All quotes about people should be able to do this
* Long history of Aural Skills 
  + Older people
  + Current Methods
  + Their goals 
  + Their assumptions 

+ People are not good at this 
  + Wennerstronm (1989) said people at IU are not good at aural skills and sight singing p.163 (k7)
  + Julliard (1953,p. 48) incoming students have untrained ear (k7)
  
+ nice quote from chittum 1967 "the da is past when teachers can say, you either have it or you don't" (p.73)
+ Start of a big Journey back then, even more now (almost 2 decades since Karpinski, 2000)
 
 
### Theoretical Background

Lots of older studies looking at this listed on page 2 of Taylor and Pembrook 1983 (List here)

#### Computational Musicology 

#### Music Psychology and Memory for Melody

### Rationale 

#### Computational Musicology

#### Music Psychology 

### Factors

This section will list factors that are believed to be important to modeling melodic dictation.
Need to have both individual and musical parameters.
Ends with polymorphic view of musicianship.
  + Nichols, Wolner, Halpern 2018
  + Niels paper on Jazz similarity
  + My paper on Wagner 


+++++++++++++
Pitch in Karpinki 

+ Pitch Matching (Work being done with Seatle, Pfordresher)
+ Pitch Memory (See Snyder)
+ Pitch Collection and Chunking 
+ Infereing Tonic
+ Melodic Contour 
+ Scale Degrees 
+ Identification of Intervals
+ Identifcaiton of Scale Types 
+ Solimization Systems 
+ AP

+++++++++++++++++++
Melodic Dictation 

Kraft 1999 -- First part of book is for melodic dictation
Benward and Lolosick 1996a -- Melodic Dictation

Karpinski 1990 Has its own model for melodic perception?? 

Page 66 of K -- Relevant Melodic Contour Information 
page 68 of K -- Deutsch -- Familiar systems are better (now we have IDyOM)
Dowling and Harwood 1986 124-44 Melodic expectancy (probably better for Pearce)
READ Sloboda and Parker 1985

THURSDAY FOLLOW UP!
Follow Ups of Note Max 
  + Tallarico 1974
  + Long 1977
  + Pembrook 1983
  
7-11 Notes? 
Marple ND reports that most things land in 7 +/-2, But not mention of IC! 
Found that if you make it rhythmic, goes up to 6-10, rhythm helps, but maybe it's just shorter time

Potter 1980 -- most people chunk (obvs question of segmentation)
Deutsch 1980 rhtym alines with pich, people do better than non-hierarchy 
Oura 1991 -- MODEL OF HOW MANY PITCHES PEOPLE REMEMBER, use for corpus question on pattern matching

Hofsetter 1981 -- people do better in bottom 4 time than 8 (again maybe confounded by IC)

Page 98 in Karpinksi has lenght and number of playings, 

Effect of tempo in  Unks, Bowers, and Eagle 1993

figure 3.1 is Karpinski method to understand dictation process



+++++++++++++++
Define the rationale and significance for this study
talk about what the processes are that go into this 
What are the implicit transfer claims of this? 
    + discussed in chapter 2 (history and rationale, Karpinski)
    + transfer literature also dicussed in chapter 2
Is there literature specifically on this? Yes, but scant.


what contributes to this whole process?

Note that there are two fields, both of which's literature can help out.


===================
63 words at start 

## Chapter 2

## First Establish Others think this is Important 

### Historical Evidence 

### Current Evidence 

## Solimization

### What is Solimization 

### Brief History of it 

## Current State of Aural Skills

+ Guy that just wrote that dissertation on it in England 


History of Aural Skills a lot from other sources
  + Karpinski (Schumann, Smith, Benward and Car, Benward and Kolosick, Butler 1997)
  + Wolf and Colleagues
  + Best-- thinking 'in' music 1992
  + Serafine 1988 thinking in or with sound / 
  + Elliott 1996 thinking about music without 'understanding'
  === These all really just have to do with mental representation? 
  
  Karpinski -- "Music listeners who understaqnd what htey hear are thinking in music" <-- claim page 4
    + could imagine a thought experiment where understanding implicit and explicit knowledge of this

Karpinski notes that (Butler and Lochstampfor, 1993) no link between pedagogy and music cognition.


## Old Farts To Talk About 

- Guido of Arezzo
- Gioseffo Zarlino 
- Franchinus Gaffurius
- C.P.E. Bach
- Jean-Philippe Rameou
- Arnold Schoenberg
- Heinrich Schenker 

- Adriano Banchieri <-- first to fix guido's hexachord 
- Hubert Walerant (via Calvisius)

- Timothy Johnson's Article on solimization 
- Gregory Barnett - cambridge guide, 17th Century Organization
- Lorenzo Penna 

  
* Compare and contrast goals in terms of pedagogy and teaching. 

## Current State 

* Books and what not. 

## Chapter 3
### Cognitive Aparatus

### Training Effects 

## Transfere Literature 

### Memory for Melodies Literature 

### WMC 

* Nichols, Wollner Halpern, 2018

### Gf 


###Outline Notes

* Melodic Dictation is clearly something that uses WM
* Also dependent on WMC
* Problem is that there is not a direct comparison
* WMC tasks, simple and complex are with novel stimuli
* Even in cases of WMC, need to control for word frequency
* This allows us to know that exposure effects this ability
* Huge amount of work on statistical exposure (Saffran, Huron, Margulis, Pearce)

* Could take theories of WMC but capacitity limits and WMC task do not medoel the sequential and hierarchical nature of melodies
* Obvious alternative to this is to look at n-grams in melodies
* Image chart where overlay of n-gram is reflected in frequency distribution of chart
* Clealry we have task where complex entertaining of information and working

* What is emlodic equivlient to the word length effect?
* Is there such a thing with decay and melodies (Lewandasky + Corey's work)
* Hard to model idea of repeating things bc of rhythm problem and temporal properties

* STOP IT WIHT THE MILLER (Articles that reference Miller but get it wrong list)

* Question of how is music represented, is there some sort of pholoogical representation (is then the point to split the intervals with each phonological rep with the episodic buffer playing a role?)
* Do students need to know what their WMC maximum is

### Cowan Reading Notes

* It's helpful to assume there are constants in psychology 
* Why prefer Cowan over Baddeley, boat example 
* Stop with the miller 7 quotes, should be listing of these
* WMC as form of attention
* History of WMC models (Process Model Attkinson, Baddely, Berz, Cowan)
* Rapid formation of new episodic buffer
* Is solfege assigment automatic? Transferred to new realm of representation
* Need to get away from idea of chunking for MMD
* Melodies are always in serial order 
* Aka from above need to look at n-gram representation
* could imagine making some sort of sensory memory experiemnt ala page 114
* More options to write down, more error on recall, clearly not free recall situation, pattern matching

- I want examples of frequency distribution confounding memory experiemnts

## Baker Model of Melodic Dictation (Over Karpinski)

* Divert attention to maximum n-gram 
  * Open n-gram/attn to LTM matching
  * When LTM match maxed, put n-gram in phological loop 
  * Transcribe all matched n-gram with LTM match ups (effecient processing? sd)
  
  - {Warning if beyond capacity limit} (tho how does this relate to LTM)
  * IF {Chunk is understandable => Transcribe}
  * IF {Chunk is not understandable --> Segment to smaller unit }
    * LTM as first option (contextualized)
    * Rhythmic and Contour framework 
      -- SD options first on contour
      -- Avoid Interval (last resort) requires LTM single interval pattern match with n-gram of loop
      -- Also avoid interval because it disgards maximum of n-gram
  * Only patterns with LTM match will be transcribed (aka do sight singing)
  * If not LTM match, then need recursive process to figure out new schema for looped n-gram
  
* Also, model that took n-gram funcitonality into the description would also account for weird atonal melodies where they have no function, but rather are built upon diatonic sequencs that are linked together with atonal gestures
      
* Theoretical reasons for stopping atomistic sight singing
  - It's only small block of more fluid and complex process
  - Could AP mapping be brought in here? 

* Span speed correlation and doing some sort of melody thing wiht an experiment 

* Eventually do POT and Music for this 

## Chapter 4

--------- EDIT HERE
It is important to highlight that despite the level of agreement at lower levels of difficulty, even experts do not seem to agree on the appropriateness of melodies as they become more complex for classroom use.
Of course perfect levels of agreement are not expected, but due ot the fact that people need to be graded and assessed fairly on melodies, this level of disagreement is cause for alarm.
This level of disagreement might also help explain student's responses to aural skills pedagogy...
 
And to compound this problem, literature from psychology (Kanneman and Tversky, that medicice doctor study) importantly highlight the fact that it's important to be skeptical of expert opinions.
Drawing from the medical liteature-- a body of research where getting somethign wrong probably matters more-- have evidence of stuff happenign with heart attcks.
Undelrying logic being that people think they have the key, but really they do not.
And what solved this was a computaional problem. 
------------

In this chapter I demonstrate how difficulty a melody is to dictate can be formalized with tools from computational musicology.
It is important to be able to formalize these intutitions in order to begin to build a systematic and fair way to both teach and asses students as they progress through their aural skills training.^[Could add a bit here about how some people will just say leave it to the experts, but if you look at research from psychology, models almost always peform better than individuals who think they have some sort of domain expertise. I think that effect even gets worth with experience?] 

Using an acoustical representation like a melody's contour is an abstracted feature of the melody.
Abstracted features of melodys assume some sort of suspended animation of listening meaning that what is abstracted has some sort of meaning that is related to the listening experience.
While if an abstracted measure maps directly to any sort of phenomenological experience in time, might be contested, these types of measures have been becoming more common in music psycholgy research.




THER PEOPLE WHO HAVE DONE THIS

* look into Wiggins et al., 1993, for history of representation 

Folk music

* Bartok 1936?
* Bartok and Lord 1951
* Lomax 1977 ; Lomax, A. (1977). Universals in song. The World of Music, 19, 117–129.
* Steinbeck 1982 
* Jesser 1992
* Sagrillo 1999

* GET AND READ PAT SAVAGE ARTICLE

Popular Music

* Moor 2006
* Kramarz 2006
* Furnes 2006
* Riedemann ????

Computational Musicology

* Eerola eta al 2007  and 2007
* McCay 2005
* Huron 2006
* Frieler 2008
* JAZZOMAT PROJECRT OUTPUT



### Musical Parameters

#### Inspiration from Computational Linguistics 

#### Feature Extraction in Music 

##### Symbolic Approaches (Static)

##### Symbolic Appoaches (Dynamic)

##### Behavioral Results 

#### Point is that these features can stand in for intuition


## Chapter 5

## Brief review of Chapter 4 on corpus (Language to reflect journal submission)
### Corpus outside of music
### Corpus in Music
### The point is that it implicitly represents humand knowledge
### IDyOM 1
### IDyOM 2
### IDyOM 3
### Huron suggestions that starts of melodies relate to mental rotaiton 
### Other Huron claims
## Note problem with using corpus is making corpus
### Many are used on Essen
### Brinkman says Essen Sucks
### If going to make generlizable claims, need to always have new data 
## Solem duty to encode and report on corpus
### Justin London Article on what makes it into a corpsu
### Though I just encoded the whole thing because in my heart of hearts I'm a Bayesian
## The Corpus
### History of Sight Singign books
### Assumed to be where long term store comes from (adumbrate computational model)
### Lots of melodies in ascending order of difficulty, grouped appropriately though? Utah guy
### Why I encoded it in XML  
### Is it legal?
## Descriptive Stats of Corpus
### Why?
#### For pedagogical purposes
#### For experimental purposes
#### For computational idexing (get me melody with x tonal score)
#### Could serve as representation of implicitly learned expectations for future modeling
### Feature Level
#### What features are normally distributed
#### Correlated feature problem
#### big ~facet wrap of the whole thing
#### Could do dimensonality reduction (Baker, Harrison, others) but then loose understanding
### n-gram
#### Big solfege n-gram table
#### Dependent on representation (notes, solfege, mint)
#### Shiny app of n-gram heatmap with Peter
#### Idea would be that hotter n-grams lend them selves to better chunking (but need better word than chunking)


### Why need new data 


### History of Corpus Studies 

### Current State in Music

### Limitations

### Boring Corpus Stuff

#### Encoding Process

#### Sampling Criteria

#### Situation of Corpus Methods

### Descriptives of the Corpus compared to Essen/Dutch/Whatever

## Chapter 6

### Rationale for Experiment

### Selection of Melodies

* Tonalness, Countour, Number of Pitches - Long 1997 Tonalness good predictor
* Tonalness better than atonal (Frances 1958) Zenatti 1969 -- from Long 1997
* Taylor 1977 -- IC predicts when contour and lenght constant 
* Long 1997 -- IC affects information 



### Experiment I and II

### Experiment III?

### Limitations

#### How to Score 

#### Reasons for making everything open source

### Summaries

#### Applications to Pedagoges

#### Conceptual Frameworks 

### Conclusions

#### What can we really expect of undergrads?


### Why?
##### Better than verbal models
##### Sometimes even mathematically infesable proposed theory
##### Beyond Karpinski in that it doesn't just schematize, says exactly when each thing is happening when
##### Lends itself to better discussions that don't just rely on personal anecdotes
##### Can tweak the parameters 
##### Can collect different types of data (corpus or experimental) and use the model
##### This model suggests that atomism approach is actual just subprocess of larger pattern 
#### Theoretical Justification
##### Marries literature on LTM and prior knowledge, information theory, WMC, computation, representation
##### Also can be implemented in computer
##### represntation of rhythm too?
##### inspired by people like margulis 2005, albrecht and shanahan key finding, want something to contribute
##### Really Made me think
#### The Model (note many parameters can be changed in R package)
#### Prior
###### Corpus of music represented in form of n-grams
###### IDyOM extracts all possible n-gram permutations as learned corpus
##### Music notation fed into processing window where incoming n-gram is matched based on WMC window OR IT maximum 
###### Information builds until approaches critical threhold 
###### Upon maximum, model puts n-gram into focus of attention (Cowan 1988) and note why this is better than Baddely Hitch
###### Recursive transcribe function looks for LTM matches 
####### Option 1: Pattern Matched and Pattern Transcribed, success?
####### Option 2: Pattern not matched in full, truncated and use match option again (should be higher probability of match with corpus)
####### Option 3: Pattern not matched downsize again until at interval level and relying on 2-gram (atomism)
####### On sucess of option, reopen gate at nearest long implicit n-gram LTM Match (start or end problem)
###### Put time contraints on search features 
###### Transcribe process resets with trace image of melody after each dictation 
###### Transcribe process ends when all notes accounted for 
#### Model Output
##### Based on leanrning, times needed to hear it
##### Completion percentage
##### Rank order of easier to transcribe parts based on learning
#### Model Compared to Data
##### With Experimental Data 
### Future Suggestions for Aural Skills Pedagoges and Research
#### Use model as teaching stepping off point
#### Should move towards LTM pattern matching
#### Reason that people learn how to sight sing is to INCREASE the learning of the implict corpus
#### Circular process here
#### Is this what it means to then think IN music
#### Really it's to just know the patterns maybe like model where Justin London suggests we get to know patterns and expect themn
#### Would also make sense in terms of Leonard Meyer 1956
#### Use WMC in music theory, cognition, education studies

--------------------------------


## Computational Cognitive Model Model (If time permits) [Whole article in itself]

COMPUTATIONAL MODEL


So thinking all about this, first reviewing the literature and then trying to run experiments to figure it all out, got kind of frustrated because even with a mixed effects model, all it is saying is that as note density goes UP so does difficult, and it interacts with tonality. Def setting myself up for a “I could have told you that” moment from my music theory colleagues. Also figured since I am doing a dissertation called modeling melodic dictation, would be good to actually make an explanatory model of it.

Hopefully by this point, have a good idea of the factors that might contribute, my task here was to then try and operationalize everything and make a computer do it.

Actually really inspired by this Lewandowsky book where they had a quip about how wit the Baddeley model it really has like 150 permutations of it and some famous example of someone saying that men have more one night stands than women, but in a heterosexual population that is mathematically impossible. So it only seemed like it made sense to see if I could write one out.

So this is how I think it would work, currently in the process of coding it.

So essentially it’s a bayesian inspired model that compares a target melody, with a corpus of melodies or the prior, that are meant to represent all “known” musical material in someone’s musical understanding. I got the idea from reading about Cowan’s Embedded process model which does not posit the working memory structure as very distinct from that of everything in long term representation, but rather uses the central executive idea as the limited window of attention that can then be focused on things either in LTM or near categorical features of what is in the purveiw of attention.

Will first go through it on a high level, then make another pass and talk about how everything is getting operationalised. So imagine that we have a prior corpus of all the melodies that are covered for sight singing and dictaiton in the first semester of aural skills (in this case, melodies X--XX). If we let the corpus M represent all prior knowledge, where not only is the melody represented as a string of intervals, but we also get ever n gram permutation of of the melody based on the idea that the more frequent the exposure of an n-gram, the more likely it would be to be understood. 

This is the first big parameter M.

We could imagine and visualize this as a grid of distributions of all possible n-grams and the strings that make them up seen here GGPLOT2. 
Now although any string of notes is possible, in order to mirror the limited capacity of WMC, there needs to be some sort of threshold that is set that would mirror the limits of the window of WMC.

This threshold is the next big parameter, T. 

Currently conceptualizing this as being reflective of maximum of information content threshold as calculated by IDyOM.

Idea here is that as you hear more notes, more IC, the more your fixed capacity bin of melody is going to be filled up. By making it based on the IC of a melody string represented in the corpus, patterns that are more frequent are going to easier to then dictate, which aligns with intuitions on melodic dictation.

Also need to define a the explicit, implicit threshold. Explicitly known n-grams should have some sort of fixed amount that if it succedes that, you would know it. Like karpinski “knowing” phase, would also assume that every interval class would also be “known” and if not, that is OK too, will lead to people not being able to transcribe it. 

We’ve now defined both the individual knowledge parameters and teh thrshold T of working memory capacity. These consist of WHAT DO YOU CALL THIS. 

With these individual parameters established, we can calculate difficulty of the melody by running what I am going to be determining as the ‘transcribe’ function, which is separate from the individual parameters. 

We then can introduce the target melody , t  which represents the string to be dictated.

Idea here is that this melody is compared to the corpus, M, and you get the IC of each of the n-grams from the prior corpus.

Given the theshold, take the biggest n-gram that fits below the threshold to be put in the buffer. If you had a huge threshold and could take IC of whole melody or you KNEW it, would be reflective here. Could also introduce a function here that just gueses at about the threshold. Once this is filled up, let’s say 4 notes, the notes are then set to the transcription buffer. 

At this point index the n-gram is cross checked against the prior knowledge and look for matches of it that happen above a certain “known” threshold. If it is known, it gets “notated’ and you re-enter the melody.  If not, in this case the 4 gram would get recursively truncated until found a string, even 2 gram where it is known. If it gets to a 2 gram and it’s not explicitly known, basically because you don’t know that interval, what Karpinksi rails againt as atomistic hearing. Also good in that people with bigger chunking will do better.

If there is not an exact match, move to split fuzzy searches (future versions) 

Re-entry function is defined by finding the next part of the melody where either next most common n-gram occurs or where it left off last. This is then to reflect the two different ways of approaching tacking. Seing as some people start at front, others start at end. Could be reflective of primacy and recencey effects of memory, OR maybe it’s due to fact that more prototypical n-grams in habit starts and ends of melodies….?

This process would then re-occur a few times over until either can’t go anymore (intervals are not known) or till completion.

Afterwards have an idea of the path that it took, counts of those, use those as proxies of difficulty. 

I think this covers everything in the decision making process and each step of the process can be automated eventually (post doc anyone?!). More importantly, it reflects ecological phenomonologcial experience of taking melodic dictaiton. 



--------------------------------------------------------- 

Aligns with intuitions that 
People who know more melodies will be better (singer phenomena) 
People will try for bigger chunks first, then go to smaller ones
Atomistic transcription can happen but is inefficient 
People with higher chunking ability will do better
IC is helpful proxy for this in line with IDyOM literature 
Relative pitch is where it’s at for things like this

Having now gone through the model verbally and notating the parameters that need to be set and how things are then represented, I will now define the model formally. 

## MoDiMe 

* Formal Model here



## Example

Having now both described the model and defined it formally, I now can run through a brief example of how the model might work in practice by showing a melody to be dictated, a the corpus of melodies that it represents, and three iterations of model that exhaust each branch of the model.

### Example Corpus

* 12 Melodies on on page and their representation 
* Also show rhythmic and pitch abstractions
* Show their density histograms as a corpus 


* Target Melody

* Define WMC low for example
* Linear re-entry 

* First run through take n- gram chunk, no explicit pattern match, drop 1 gram, yes match, move on
* Next no exact pattern match on linear re-entry, query pitch (yes, proto) query rhythm (yes,rhythm), both transcribed
* Third iteration, full pattern match
* Not complete, but at this point have X amount of melody transcribed with this history.
* Tedious to get the exact thing, go to actual example

* CRAZY THAT YOU COULD USE THIS TO WORK BACKWARDS HOLY SHIT BAYES YOU ARE SWEET!! 

### Actual Data

* Let's now say that we have set of XYZ melodies as targets, the whole semester 1's melodies the prior knowledge, what does the model then say.
* This basically mirrors week 1 of semester 2 of freshman year
* Plot of correlation of model predicts, correlation of actual scores
* Can re-do this now with only pitch scores (or run it with just pitch!)
* Can re-do this now with only rhythm score 

And here is some commentary 


## Future Directions

Now with a formal model of melodic dictation, what is now possible?

* Importantly have formal model of what might be phenomeonlogilly way people do melodic dictation.


* Talk explicitly about representation
* Talk about what happens when
* Link to sight singing
* Testable hypotheses?
* What's the deal with AP 
* Problems with how to define prior
* Future Directions
* How this all relates to helping teach 
* aka explain the model, thus explain learning, perception, process, representation, if one rep performs better than another, atomistic singing 

-------------------------------

## Advantages Of A Computational Model (Optional)

It is important to note that this model is not perfect in that it deterministically mirrors the exact cognitive process someone would use to take melodic dictation.
It instead quantifies each step of the process and provides a detailed account each step in a fully realized and implementable model.
The benefits of creating a computational model of melodic dictation are numerous.
First, as stated above, a computational model acts as an explanatory model that details the inner working of a process.
Second, by quantifying each step of the process, a computational model forces the researcher to specify key theoretical issues that need to be addressed and operationalized.
In this case, there is the issue of representation of musical material, definitions of working memory, as well as what does it mean for something to have been learned. 
Thirdly, in creating a computational model you provide theories with a concrete version that can be more easier falsified.
^[The quintessential example of the importance of this comes from an anecdote in evolutionary psychology claiming that in heterosexual populations, men engage in more casual sexual encounters than women due to their promiscuous proclivities.
This theory has appeared in textbooks, but as noted by AUTHOR, while might be good as a verbal model in that it plays to our preconceived notions about male sexuality, implemented as a computational model, is mathematically impossible.]
Fourthly, creating a computational model rules defines the universe of a possible outcomes that a theory posits.
As shown in work done on the seemingly simple Baddeley and Hitch model of working memory, AUTHOR demonstrated that with XXX parameter values, the YEAR model of working memory actually allows for 15X different permutations of the model.
With such vast possibilities, it becomes much harder to speak about _the_ model as a single entity.
Finally, and most importantly from a pedagogical framework, by detailing the exact steps of the process, a computational model provides a theory of melodic dictation in which each element of the process is able to be discussed and gives exact framework for which it can be critiqued.
Creating a computational model is the next step in furthering research on aural skills pedagogy.

 


Much work has been done on this history of musical representation WIGGINS ET AL 1993 has been exhaustive, even though it has only been on for about 30 years but regardless of what exact representation is chosen, it generally can be conceived of as some sort of symbol string Clifford et al., 2005.
How a string is represented carries with it important assumptions about a listener's perception and also a teacher's pedagogy.
Take for example the many ways in which someone could represent the ever popular Twinkle, Twinkle Little Star as in Figure 1. 

* TWINNLE IMAGE HERE in C and F#

Representing the string of symbols as in Figure 2A using THIS NOTATION SYSTEM would suggest that listeners perception and cognition of the notes mirrors that of fixed _do_ solfege.
The only symbols in the string, or tokens, represented carry with it the name of the note along with its absolute range.
When compared to string 2B-- the melody transposed a tritone up-- a computer would find no symbol overlap between the two strings and might declare the two as completely dissimilar.
Additionally, neither string contains with it any rhythmic information.
Reproduced Figure 2C and Figure 2D, we see the same two strings now with additionally temporal information.

* FIGURE 2 DIFFERENT TYPES OF REPRESENTATION
* C4 C4 G4 G4 A4 A4 G4
* F#4 F#4 C#5 C#5 D#6 D#6 C#6

As stated above, this mapping assumes fixed pitch representation of musical information.
This is at odds to intuition in that most humans would categorize the information in strings 2A and 2B as containing the same musical content.

In order to capture that aspect of human perception and cognition, one might want to represent this string of musical information to the computer as a list of directional intervals (Math citation) which brings with it interval information not present (although maybe implicitly imposed!) in the previous examples.

* C U +P5 U +M2 U -M2

This representation then captures what might be considered phenomena closer to human perception, but again is devoid of any sort of scale degree functionality or qualia that might be involved in listener's perception.
Including this assumption the string and duration values as it might be a Humdrum spine might be represented as:

* DO DO ^SOL SOL ^LA LA >SOL (with rhythm)

Each string represents what might be considered the same perceptual phenomena, but when quantified, one system needs to be explicitly chosen as how information is represented will change model output.
For example if it is assumed that an individual's prior musical information can only be encoded as the first _fixed do_ example, then each of the 12 possible ascending perfect fifths would have to be learned as separate phenomena to be represented in as prior knowledge in this model.
This point also brings up the importance of noting that many of these systems could be at play here and just because one representation might not make sense in one implementation of a model does not make it null and void for other applications in pedagogical settings.

Given what was stated above, there are many was in which to digitally represent musical information.
That said, one advantage in using the pre-exisiting IDyOM model as the backbone for this model is that it is capable of representing many types of musical representation simultaneously.
Not only is pitch information represented, but using different WINDOWS, it then becomes possible to encode these multitude of parameter simultaneously (LIST THEM HERE?) each as their own token, which then can be used to model information content. 

