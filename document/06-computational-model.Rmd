# Computational Model 

## What Is A Model And Why do you need a model

In his 2007 article _Models of Music Similarity_ [@wigginsModelsMusicalSimilarity2007] Geraint Wiggins distinguishes between _descriptive_ and _explanatory_ models in describing the modeling of human behavior. 
Descriptive models assert what will happen in response to an event.
For example, as noted in the previous chapter, as the note density of a melody increases and the tonalness of a melody decreses, a melody will become harder to dictate.
While the increase in note density is assumed to drive the decrease in dictation scores, merely stating that there is an established relationship between one variable and the other says nothing about the inner workings of this process.
An explanatory model on the other hand not only describes what will happen, but additionally notes why and how this process occurs.
For example, much of the work musical expectation demontrates that as exposure to a musical style for an individual increases, so does that persons's ability to to predict specific events with a given musical texture.
Not only does it predict more accurate responses, but many of these models like HURON, MARGULIS, PEARCE derrive their underlying theory about this behavior from the brain's ability to implicitly track statistical regularities in musical perception SAFFRAN.
The _how_ derrives from the brain's ability to track statistical regularities in musical information in the environment and the _why_ derrives from evolutionary demands in that organisms that are able to make more predictions about their environment are more likely to survive and pass on their genes HURON 2006. 

Wiggins notes that although there can be both explanatory and descriptive theories, depending on the level of abstraction, a theory may be explanatory at one level, yet descriptive at another.
Using the mind-brain dichotomy, he notes a theory of implict expectations like PEARCE could be explanatory at the level of behavior as noted above, but says nothing about what is happening at the neuronal level.
Both descriptive and explanatory theories are needed in that descriptive theories are used to test explanatory theories, and by strining together different layers of abstraction, we can arrive at a better understanding of how the world works.

Returning to melodic dictation, under Wiggins' framework, the Karpinki model of melodic dictation [@karpinskiAuralSkillsAcquisition2000; @karpinskiModelMusicPerception1990] would qualify as a decriptive model.
The model says what happens over the time course of a melodic dictation-- specificying four discrete stages discussed in earlire chapters-- but does not explicitly state _how_ or _why_ this process happens. 
In order to have a more complete understanding of melodic dictation, an explanatory model is needed. 

In this chapter I introduce an explanatory model of melodic dictation. 
The model is inspired by work from both computational musicology drawing on the work of Marcus Pearce's IDyOM CITATION, as well as cognitive psychology and uses a uniform memory mechanism inspired by Cowan's Embedded Process Model of working memory to explain the process.
In addition to quantifying each step, the model incorporates flexible parameters that could be tweaked in order to accomodate for individual differences, while still relying on a domain general process.
By relying on cognitive mechanisms based in statistical observations, rather than a rule based system for music analysis (GTTM, NARMOUR, TEMPERLY) this model brings with it the same arguments made by Pearce in his 2018 article that allow for the heterogenity of musical experience amongst music listeners.

## Model Introduction 

The model consists of three main modules, each with its own set of parameters:

1. Prior knowledge
2. Selective Attention
3. Transcription

Inspired by Bayesian computational modeling, the _Prior Knowledge_ module reflects the previous knowledge an individual brings to the melodic dictation.
The selective memory-- somewhat akin to Karpinksi's extractive listening^[I use different terms to avoid confusion when comparing models later]-- segments incoming musical information by using the window of attention as conceptualized as the limits of working memory capacity as a sensory bottleneck to constrit the size of musical chunk that an individual were to trascribe.
Once musical material is in the focus of attention, the Transcription function recusrively pattern matches against the prior knowledge's corpus of information in order to find a match of explicitly known musical information.
This process refects, but does not actually mirror the exact process used in melodic dictation, yet seems to be phenomenologically similar to the decision making process used when attempting notate novel melodies.
Based on both the prior knowledge and individual differences of the indiviual, the model will scale in ability, with the general retrevial mechanisms in place.
The exact details of the assumptions, parameters, and complete formula of the model are discussed below.

* Name it here if you want 

It is important to note that this model is not perfect in that it deterministically mirrors the exact cognitive process someone would use to take melodic dictation.
It instead quantifies each step of the process and provides a detailed account each step of the process in a fully realized and implmentable process.
The benefits of creating a computational model of melodic dictation are numerous.
First, as stated above, a computational model acts as an explanatory model that details the inner working of a process.
Second, by quantifying each step of the process, a computational model forces the researcher to specify key theoretical issues that need to be discussed and operationalized.
In this case, we have to deal with the representation of musical material, defintions of working memory, as well as what does it mean for something to have been learned. 
Thirdly, in creating a computational model you provide theories with a concrete version that can be more easier faslfied.
The quintessential example of the importance of this comes from an anecodote in evolutionary psychology claiming that in heterosexual populations, men engage in more casual sexual encounters than women due to their promiscous proclivities.
This theory has appeared in textbooks, but as noted by AUTHOR, while might be good as a verbal model in that it plays to our preconcived notions about male sexuality, implemented as a computational model, is mathematically impossilble.
Fourthly, creating a computational model rules defines the universe of a possible outcomes that your theory posits.
As shown in work done on the seemingly simple Baddely and Hitch model of working memory, AUTHOR demonstrated that with XXX parameter values, the YEAR model of working memory actually allows for 15X different permuatations of the model.
With such vast possibilites, it becomes much harder to speak about_the_ model as a single entity.
Finally, and most importantly from a pedagogical framework, by detailing the exact steps of the process, it provides a theory of melodic dictation in which each element of the process is able to be discussed and gives exact framework for which it can be criqtiqued.
Creating a computational model is the next step in furthering research on aural skills pedagogy.

## The Model 

### Model Representational Assumptions

In order to write a computer program that mirrors the melodic dictation process, how the mind "percieves" and "thinks" about musical information must be defined _a priori_.
Before delving into questions of representation, this model assumes that the musical surface JACKENDOFF as represented by the notes via Western musical notation are salient and can be percieved as distnict perceptual phenomena.
Although there is work that suggests that different cultures and levels of experience might not categorize melodic information universally MCDERMOTT, other work suggests that things like experiencing pitches as discrete, categorical phenomena is categorized as a statistical human universal SAVAGE.
For the purposes of this model in that it models something specific to a very WEIRD CITATION demographic, this issue is not as relevant. 

Knowing that it is melodic information or melodic data that needs to be represented, the question then becomes what are the best ways in which to represent it.
This issue becomes increasingly complex when considering literature suggesting that the human mind represents musical information in a variety of different forms from PEARCE 2018 CITE THAT YOU HIGHLIGHTED.
While I will attend to this issue below using the current state of the art, a brief review of how information is represnted is warrented. 

Much work has been done on this history of musical representation WIGGINS ET AL 1993 has been exhastive, even though it has only been on for about 30 years but regardless of what exact representation is chosen, it generally can be concived of as some sort of symbol string Clifford et al., 2005.
How a string is represented carries with it important assumptions about a listern's percepetion and also a teacher's pedagogy.
Take for example the many ways in which someone could represent the ever popular Twinkle, Twinkle Little Star as in Figure 1. 

* TWINNLE IMAGE HERE in C and F#

Representing the string of symbolys as in Figure 2A using THIS NOTATION SYSTEM would suggest that listeners perception and cognition of the notes mirrors that of fixed _do_ solfedge.
The only symbolys in the string, or tokens, represented carry with it the name of the note along with it's absolute range.
When compared to string 2B-- the melody transposed a tritone up-- a computer would find no symbol overlap between the two strings and might declare the two as completely dissimilar.
Additionaly, neither string contains with it any sort of rhythmic information.
Reproduced Figure 2C and Figure 2D, we see the same two strings now with additionally temporal information.

* FIGURE 2 DIFFERENT TYPES OF REPRESENTATION
* C4 C4 G4 G4 A4 A4 G4
* F#4 F#4 C#5 C#5 D#6 D#6 C#6

As stated above, this mapping assumes fixed pitch representation of musical information.
This is at odds to intuition in that most humans would categorize the informaiton in strings 2A and 2B as containing the same musical content.

In order to capture that of human perception and cognition, one might want to represent this string of musical informationt to the computer as a list of directional intevals (Math citation) which brings with it interval information not present (although maybe impliclty imposed!) in the previous examples.

* C U +P5 U +M2 U -M2

This then captures what might be considered phenomena closer to human perception, but again is devoid of any sort of scale degree functionality or qualia that might be involved in listener's perception.
Including this assumption the string and durational values as it might be a Humdrum spine might be represented as:

* DO DO ^SOL SOL ^LA LA >SOL (with rhythm)

Each string represnts what might be considered the same perceptual pheneomena, but when quantified, one system needs to be explicitly chosen as how information is represented will change model output.
For example if it is assumed that an individual's prior musical information can only be encoded as the first _fixed do_ example, then each of the 12 possible ascending major fifths would have to be learned as separate phenomena to be represnted in as prior knowledge in this model.
This point also brings up the importance of noting that many of these systems could be at play here and just because one representation might not make sense in one implemntation of a model does not make it null and void for other applications in pedagogical settings.

For the purposeses of this model and further examples I will use the absolute interval representation of pitches here, with further directions in this research addressing how it would be possible to implment things like the scale degree and _fixed do_ model.
Comparisons of this model output will also contribute to conversations regarding pedagogy in that if one form of data represenation mirrors human behavior than the others, it would provide more than anecodotal evidence in support of the pedagogy of one system over another. 
How the model represents musical information is the first important parameter value that needs be chosen before running the model and this establishes the prior information.

Given what was stated above, there are many was in which to digitially represent musical information.
That said, one advantage in using the pre-exisiting IDyOM model as the backbone for this model is that it is capable of representing many types of musical representation simultaneusly.
Not only is pitch information represented, but using different WINDOWS, it then becomes possible to encode these multitude of parameteter simultanoulsy (LIST THEM HERE?) each as their own token, which then can be used to model information content. 

## Modeling Information Content 

Having established that the models first established parameter to be decided is how strings of melodies are represented, the next important decision that has to be made is how decisions of segmentation for the second stage of attentive memory are decided.
Although there has been a large amount of work on different ways to segment the musical surface using rule based methods JACKENDOFF NARMOUR 2 TEMPERLY MARGULIS, which rely on matching a music theorist's intution with a set of descriptive rules somewhat like the boundary formation rules in _GTTM_.
While this continues a long tradition of research trying to determine formal boundaries (History of All Music Theory), as noted by Pearce (2018) rule based models are not able to handle music outside the school of theory that generated the music.
Additionally, since melodic dictation is an active memory process, rather than a semi-passive process of listening this model needs to be able to quantify musical information that is both dependent on prior musical experience and allows a moveable boundary for selective attention so that musical information that is memory can be actively maintained, whilst carring out another cognitive process, that of notating the melody. 

In order to create this metric, I rely on IDyOMs use of information content IC (Shannon) which quantifies the IC of melodies based on corpus of materials.
Once hypothsis that IDyOM relies on is that musical information is learned via implicit statistical learning, which Pearce refers to as the Statistical Learning Hypothesis or SLH.
The SLH states that 

> musical enculturation is a process of implicit statistical learning in which listeners progressively acquire internal models of the statistical and structural regularities present in the musical styles to which they are exposed, over short (e.g., an individual piece of music) and long time scales (e.g., an entire lifetime of listening). p.2 

Additionally IDyOM relies on the probabilistic prediction hypothesis or PPH which states that 

> while listening to new music, an enculturated listener applies models learned via the SLH to generate probabilistic predictions that enable them to organize and process their mental representations of the music and generate culturally appropriate responses. p.2 

Taken together and quantified using Shanaon information content, it then becomes possible to have a quantifiable measure that reliabily predicts the amount of percieved expectedness in a musical melody that can change pending on the musical corpus that the model is trained on.
These measures of information content then can be summated together as a melody progresses over time and be used as a measure for the amount of limited musical material that is capable of being in the window of attention at one time. 

For example, when trained against a corpus of melodies, THIS MELODY has IC (unitless thing) that is on top of notes.
The advantage of operationalizing how an individual hears a melody like this is that melodies with lower information content, derrived from an understanding of having more predictiable patterns from the corpus, will allow for larger chunks to be put inside of the attentive memory buffer. 

* FIGURE HERE

Note that the notes above the melody here are dependent on what is current in the prior knowledge.
A corpus of prior knowledge with less melodies would lead to higher information content measures for each set of notes, while a prior knowledge that has extensive tracking of the patterns would lead to lower IC. 

### Setting Limits with Transcribe

With each note then quantified with a measure of information content, it then becomes possible to set a limit on the maximum amount of information that the individual would be able to hold in memory as defined by a an attentional threshold.
A higher threshold would allow for more musical material to be put in the attentional buffer, and a lower threshold would restrict the amount of information held in an attentional buffer.
By putting a theshold on this value, this serves as something akin to a perceptual bottleneck basesd on the assumption that there is a capacity limit to that of working memory.
Modulating this boundary will help provide insights into the degree to which melodic material can be retained between high and low working memory span individuals.

In practice, notes would enter the attentional buffer until the information content from the melody is equal to the memory threshold.
At this point, the notes that are in the attentional buffer are sequestered and will be actively maintained in the attentional buffer.
In theory, the maximum of the attenional buffer should not be reached since the indivudal performing the dictation would still need mental resources and attention to actively manipulate the information in the attentional buffer for the process of notating.

### Pattern Matching
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
With subset of notes of the melody represented in the attentional buffer, whether or not the melody becomes notated depends on whether or not the melody or string in the buffer can be matched with a string that is explictly known in the corpus.
* Do I introduce the explict implict thing earlier?
Mirroring a search pattern akin to Cowan's Embeded Process model, the individual would search across their long term memory for anything close to or resembling the string pattern in the attentional buffer.

* Image of wallpaper search here? 

Using this logic, longer pattern strings n-grams would be less likley to be recalled exactly.
If a pattern is not found in the initial search, one token of the n-gram would be dropped off the string and the search would happen again.
This recursive process would happen until an explict long term memory match is made.
Like humans taking melodic dictation, the computer would have the best luck finding patterns that fall within the largest density of a corpus of intervals distribution.
Additionally, like students performing a dictation, if a student does not explicitly know an interval, or a 2-gram, the dictation would not be able to be completed. 
If this happens, both the model and student would have to move on to the next segement.

Upon the sucessful explict match of a string, that section of the melody would be consiered to be sucessfully dictated.
The model here would register that one iteration of the funciton has been run and the chunk transcribed would then be recorded.
After recording this history, the process would happen again starting at either the next note from where the model left off, the note in the entire string with the lowest information content, or n-gram left in the melody with that is most represented in the corpus.
This parameter is defined before the model is run and the question of dictation re-entry certainly warrents further research and investigation. 

This type of pattern search also dependent on the way that the prior knowledge is represented.
In the example here, both interval and rhythm are represented with each token in the string.
Since there is probably a very low likelyhood of finding an exact match for every n-gram with both pitch and rhythm, this patten search can happen again with both rhythms and pitch informaiton queried separatly.
If not exact pitch-temporal matches are found and the search is run again on either the pitch or rhythmic information separatly, this would be computationally akin to Karpinki's proto-notation that he suggests students use in learning how to take melodic dictaiton (KARPINKSI CHAPTER 3).
With this feature of the model incorporated, it would be possible to also generate evidence in favor or disfavor of attempting to dictate elements of the melody separatly such as doing rhythm first, then pitch information or vice versa.
Either way, having this as a second level default of the model seems to match with the intutions strategies that someone dictating a melody might use.


### Loop Back IN

Upon the sucessful pattern match of a string, the attentional buffer and transcribe-search function would need too then be run again.
As noted above, re-entry in the melody could be a highly subjective point of discussion.
The model could either re-enter at the last note where the function sucessfully left off, the note in the melody with the lowest information content, or the n-gram most salient in the corpus.
Entering at the last note not transcribed is logical from a computational standpoint, but this linear approach seems to be at odds with anecdotal experience.
Entering at the note with the lowest information content seems to provide a intuitive point of re-entry in that it would then be easier to transcribe.
Entering at the most represented n-gram seems to match the most with intutition in that people would want to tackle the easier tasks first, but this rests on the assumption that humans are able to reliably detect the sections of a melody that are easist to transcribe based on implicitly learned statistcal patterns.
For example, some people might instead choose to go to the end of a melody after sucessful transcription of the start of the melody.
This might be because this part of hte melody is most active in memory due to a recency effect, or it could be that that cadential gestures are more common in being represnted in the prior knowledge.

### Completion 

Given the recursive nature of this process, the model if all 2-grams are explicitly represented in the prior knowledge should be transcribed.
If only represented using such a small chunk, the model will have to loop over the melody many times, thus indicating that the transcriber had a high degree of difficulty dictating the melody.
If there is a gap in explict knowledge in the prior knowledge, only patches of the melody will be recorded and the melody will not be recorded in its entirety.
An easier transcritpion will result in less iterations of the model with larger chunks.
Though the current instatiation of the model does not incorporate how multiple hearingings might change how a melody is dictated, one could constrain the process to only allow a certain number of iterations to reflect this.
Of course as a new melody is learned it is slowly being introduced into long term memory and could be completly be capable of being represented in long term memory without being explicitly notated at the end of a dictation with time running out and thus not possible to be completed.
This of course then would be imposing some sort of experimental constraint on the process and since this is meant to be a cognitive computational model of melodic dictation, this complicates things.
Future reserch could be done to optimize the choices that the model makes in order to satisfy whatever constraits are imposed and could be an interersting avenue of future resarch, but are beyond the initial goals of the model.

The model then outputs each n-gram transcribed and can be counted as a series with less attempts mapping to an easier transcription. 
I believe that this lines with many intutitions about the process of melodic dictaiton.
It first creates a linear mapping of attempts to dictate with difficulty of the melody.
It relies on a distinciton between explict and implicit statistical knowledge.
It is based on the embeded processs model from working memory and attention, so is part of a larger generative model, giving it a bit more creditiblity that htis _could_ be how melodic dictation works. 

Aligns with intuitions that 
People who know more melodies will be better (singer phenomena) 
People will try for bigger chunks first, then go to smaller ones
Atomistic transcription can happen but is inefficient 
People with higher chunking ability will do better
IC is helpful proxy for this in line with IDyOM literature 
Relative pitch is where itâ€™s at for things like this

Having now gone through the model verbally and notating the parameters that need to be set and how things are then represnted, I will now define the model formally. 

## MoDiMe 

* Formal Model here

## Example

Having now both described the model and defined it formally, I now can run through a brief example of how the model might work in practice by showing a melody to be dictated, a the corpus of melodies that it represents, and three iterations of model that exhaust each branch of the model.

### Example Corpus

* 12 Melodies on on page and their representation 
* Also show rhtymic and pitch abstractions
* Show their density histograms as a corpus 


* Target Melody

* Define WMC low for example
* Linear re-entry 

* First run through take n- gram chunk, no explict pattern match, drop 1 gram, yes match, move on
* Next no exact pattern match on linear re-entry, query pitch (yes, proto) query rhythm (yes,rhythm), both transcribed
* Third iteration, full pattern match
* Not complete, but at this point have X amount of melody transcribed with this history.
* Tedious to get the exact thing, go to actual example

* CRAZY THAT YOU COULD USE THIS TO WORK BACKWARDS HOLY SHIT BAYES YOU ARE SWEET!! 

### Actual Data

* Let's now say that we have set of XYZ melodies as targets, the whole semester 1's melodies the prior knowledge, what does the model then say.
* This basically mirrors week 1 of semseter 2 of freshman year
* Plot of correlation of model predicts, correlation of actual scores
* Can re-do this now with only pitch scores (or run it with just pitch!)
* Can re-do this now iwht only rhythm socre 

And here is some commentary 


## Future Directions

Now with a formal model of melodic dictation, what is now possible?

* Imporantly have formal model of what might be phenomeonlogilly way people do melodic dictation.


* Talk explicitly about representation
* Talk about what happens when
* Link to sight singing
* Testable hypotheses?
* What's the deal with AP 
* Problems with how to define prior
* Future Directions
* How this all relates to helping teach 
* aka explain the model, thus explain learning, perceptoin, process, representation, if one rep performs better than another, atomistic singing 

-------------------------------



 


