# Computational Model 

## What Is A Model And Why do you need a model

In his 2007 article _Models of music similarity_ [@wigginsModelsMusicalSimilarity2007] Geraint Wiggins makes a distinction between _descriptive_ and _explanatory_ models when describing models that describe human behavior. 
Descriptive models assert what will happen in response to an event.
For example, as noted in ________________.
While the _______ is assumed to drive the rise in _______, merely stating that there is an established relationship between one variable and the other says nothing about the inner workings of this process.
An explanatory model on the other hand not only describes what will happen, but additionally notes why and how this process occurs.
For example, much of the work musical expectation demontrates that as exposure to a musical style for an individual increases, so does that persons's ability to to predict specific events with a given musical texture.
Not only does it predict more accurate responses, but many of these models like HURON, MARGULIS, PEARCE derrive their underlying theory about this behavior from the brain's ability to implicitly track statistical regularities in musical perception SAFFRAN.
The _how_ derrives from the brain's ability to track statistical regularities musical information in the environment and the _why_ derrives from evolutionary demands in that organisms that are able to make more predictions about their environment are more likely to survive and pass on their genes HURON 2006. 
Wiggins notes that although there can be both explanatory and descriptive theories, depending on the level of abstraction, a theory may be explanatory at one level, yet descriptive at another.
Using the mind-brain dichotomy, he notes that something like a theory of implict expectations like PEARCE could be explanatory at the level of behavior as noted above, but says nothing about what is happening at the neuronal level.
Both descriptive and explanatory theories are needed in that descriptive theories are used to test explanatory theories, and by strinng together different layers of abstraction, we can arrive at a better understanding of how the world works.

Returning to melodic dictation, under Wiggins' framework, the Karpinki model of melodic dictation [@karpinskiAuralSkillsAcquisition2000; @karpinskiModelMusicPerception1990] would qualify as a decriptive model.
The model says what happens over the time course of a melodic dictation-- specificying four discrete stages discussed in earlire chapters-- but does not explicitly state _how_ or _why_ this process happens. 
In order to have a more complete understanding of melodic dictation, an explanatory model is needed. 

In this chapter I introduce an explanatory model of melodic dictation. 
The model is inspired by work from both computational musicology, as well as cognitive psychology and uses a uniform memory mechanism inspired by Cowan's Embedded Process Model of working memory to explain the process.
In addition to quantifying each step, the model has points in it where parameters could be tweaked in order to accomodate for individual differences, while still relying on a domain general process. 
The model consists of three main parts, each with its own set of parameters that 

1. Prior knowledge
2. Selective Attention
3. Transcription

Inspired by Baysian computational modeling, the _Prior Knowledge_ part reflects the previous knowledge an individual brings to the melodic dictation.
The selective memory-- somewhat akin to Karpinksi's extractive listening^[I use different terms to avoid confusion when comparing models later]-- segments incoming musical information by using the windown of attention/limits of working memory capacity as a sensory bottleneck to constrit the size of musical chunk that an individual were to trascribe.
Once the focus of attention, the Transcription function recusrively pattern matches against the prior knowledge's corpus of information in order to find a match of explicitly known musical information.
This process refects, but does not actually mirror the exact process used in melodic dictation, yet seems to be phenomenologically similar to the decision making process used when attempting notate novel melodies.
Based on both the prior knowledge and individual differences of the indiviual, the model will scale in ability, with the general retrevial mechanisms in place.
The exact details of the assumptions, free parameters, and complete formula of the model are discussed below.

* Name it here if you want 

It is important to note that this model is not perfect in that it deterministically mirrors the exact cognitive process someone would use to take melodic dictation.
It instead quantifies each step of the process and provides a detailed account each step of the process in a fully realized and implmentable process.
The benefits of creating a computational model of melodic dictation are numerous.
First, as stated above, a computational model acts as an explanatory model that details the inner working of a process.
Second, by quantifying each step of the process, a computational model forces the researcher to specify key things that could be blown past.
In this case, we have to deal with the representation of musical material, defintions of working memory, as well as what does it mean for something to have been learned. 
Thirdly, in creating a computational model you provide theories with a concrete version that can be more easier faslfied.
The quintessential example of the importance of this comes from an anecodote in evolutionary psychology claiming that in heterosexual populations, men engage in more casual sexual encounters than women due to their promiscous proclivities.
This theory has appeared in textbooks, but as noted by AUTHOR, while might be good as a verbal model in that it plays to our preconcived notions about male sexuality, implemented as a computational model, is mathematically impossilble.
Fourthly, creating a computational model rules defines the universe of a possible outcomes that your theory posits.
As shown in work done on the seemingly simple Baddely and Hitch model of working memory, AUTHOR demonstrated that with XXX parameter values, the YEAR model of working memory actually allows for 15X different permuatations of the model.
With such vast possibilites, it becomes much harder to speak about_the_ model as a single entity.
Finally, and most importantly from a pedagogical framework, by detailing the exact steps of the process, it provides a theory of melodic dictation in which each element of the process is able to be discussed and gives exact framework for which it can be criqtiqued.
Creating a computational model is the next step in furthering research on aural skills pedagogy.

## The Model 

### Model Representational Assumptions

In order to write a computer program that mirrors the melodic dictation process, how the mind "percieves" and "thinks" about musical information must be defined _a priori_.
Before delving into questions of representation, this model assumes that the musical surface JACKENDOFF as represented by the notes via Western musical notation are salient and can be percieved as distnict perceptual phenomena.
Although there is work that suggests that different cultures and levels of experience might not categorize melodic information universally MCDERMOTT, other work suggests that things like experiencing pitches as discrete, categorical phenomena is categorized as a statistical human universal SAVAGE.
For the purposes of this model in that it models something specific to a very WEIRD CITATION demographic, I think it is safe.

Knowing that it is melodic information or melodic data that needs to be represented, the question then becomes what are the best ways in which to represent it.
Much work has been done on this history of musical representation WIGGINS ET AL 1993 has been exhastive, even though it has only been on for about 30 years but regardless of what exact representation is chosen, it generally can be concived of a symbol string Clifford et al., 2005.
How a string is represented carries with it important assumptions about a listern's percepetion and also a teacher's pedagogy.
Take for example the many ways in which someone could represent the ever popular Twinkle, Twinkle Little Star as in Figure 1. 

* TWINNLE IMAGE HERE in C and F#

Representing the string of symbolys as in Figure 2A using THIS NOTATION SYSTEM would suggest that listeners perception and cognition of the notes mirrors that of fixed _do_ solfedge.
The only symbolys in the string, or tokens, represented carry with it the name of the note along with it's absolute range.
When compared to string 2B-- the melody transposed a tritone up-- a computer would find no symbol overlap between the two strings and might declare the two as completely dissimilar.
Additionaly, neither string contains with it any sort of rhythmic information.
Reproduced Figure 2C and Figure 2D, we see the same two strings now with additionally temporal information.

* FIGURE 2 DIFFERENT TYPES OF REPRESENTATION
* C4 C4 G4 G4 A4 A4 G4
* F#4 F#4 C#5 C#5 D#6 D#6 C#6

As stated above, this mapping assumes fixed pitch representation of musical information.
This is at odds to intuition in that most humans would categorize the informaiton in strings 2A and 2B as containing the same musical content.

In order to capture that of human perception and cognition, one might want to represent this string of musical informationt to the computer as a list of directional intevals (Math citation) which brings with it interval information not present (although maybe impliclty imposed!) in the previous examples.

* C U +P5 U +M2 U -M2

This then captures what might be considered phenomena closer to human perception, but again is devoid of any sort of scale degree functionality or qualia that might be involved in listener's perception.
Including this assumption the string and durational values as it might be a Humdrum spine might be represented as:

* DO DO ^SOL SOL ^LA LA >SOL (with rhythm)

Each string represnts what might be considered the same perceptual pheneomena, but when quantified, one system needs to be explicitly chosen as how information is represented will change model output.
For example if it is assumed that an individual's prior musical information can only be encoded as the first _fixed do_ example, then each of the 12 possible ascending major fifths would have to be learned as separate phenomena to be represnted in as prior knowledge in this model.
This point also brings up the importance of noting that many of these systems could be at play here and just because one representation might not make sense in one implemntation of a model does not make it null and void for other applications in pedagogical settings.

For the purposeses of this model and further examples I will use the absolute interval representation of pitches here, with further directions in this research addressing how it would be possible to implment things like the scale degree and _fixed do_ model.
Comparisons of this model output will also contribute to conversations regarding pedagogy in that if one form of data represenation mirrors human behavior than the others, it would provide more than anecodotal evidence in support of the pedagogy of one system over another. 
How the model represents musical information is the first important parameter value that needs be chosen before running the model and this establishes the prior information. 

## Modeling Information Content 

Having established that the models first established parameter to be decided is how strings of melodies are represented, the next important decision that has to be made is how decisions of segmentation for the second stage of attentive memory are decided.
Although there has been a large amount of work on different ways to segment the musical surface JACKENDOFF DANIEL OTHERS THAT MOZART THING, rely on matching a music theorist's intution with a set of descriptive rules somewhat like the boundary formation rules in _GTTM_.
While this continues a long tradition of research trying to determin formal boundaries (History of All Music Theory), since melodic dictation is an active memory process, rather than a semi-passive process of listening (tho MARIUS HAS SAID OTHERWISE/EMBODIMENT) this model needs to be able to quantify musical information that is both dependent on prior musical experience and allows a moveable boundary for selective attention so that musical information that is memory can be actively maintained, whilst carring out another cognitive process, that of notating the melody. 

In order to create this metric, want to use IDyOM which uses IC (Shannon) and quantifies the IC of melodies based on a corpus of materials.
Great thing about it is that IDyOM is machine learning so that can put in lots of different symbolys and get the IC.
THIS IS THE WAY IT WORKS ON HIGH LEVEL WITH N-GRAMS.

Given that each grouping of n-grams will have its own IC, can then use that and cumIC function to determine.
Now all you have to do is set a theshold of memory (allow for indivudal differences) and then to also allow only a certain amount into attentional memory. 

Here parameter to consider is the threhold of WM which then allows for differnces at the indiviudal level. 

For example, when trained against a corpus of melodies, THIS MELODY has IC (unitless thing) that is on top of notes.
Good thing about this is that if you knwo a lot of melodies, cna have bigger chunks at various levels.

* FIGURE HERE

### Setting Limits with Transcribe

### Notating

### Loop Back IN


## Contents of Model and Things Needed to be addressed

* The actual transcription 
* Assumption that if explicit, then NOTATE 
* What happens at impass (split and proto)

## MoDiMe 

* Formal Model here

## Model Performance on Chapter 5 data

## What is now possilbe

* Talk explicitly about representation
* Talk about what happens when
* Link to sight singing
* Testable hypotheses?
* Problems with how to define prior
* Future Directions
* How this all relates to helping teach 
* aka explain the model, thus explain learning, perceptoin, process, representation, if one rep performs better than another, atomistic singing 

-------------------------------

## Computational Cognitive Model Model (If time permits) [Whole article in itself]

COMPUTATIONAL MODEL


So thinking all about this, first reviewing the literature and then trying to run experiments to figure it all out, got kind of frustrated because even with a mixed effects model, all it is saying is that as note density goes UP so does difficult, and it interacts with tonality. Def setting myself up for a “I could have told you that” moment from my music theory colleagues. Also figured since I am doing a dissertation called modeling melodic dictation, would be good to actually make an explanatory model of it.

Hopefully by this point, have a good idea of the factors that might contribute, my task here was to then try and operationalize everything and make a computer do it.

Actually really inspired by this Lewandowsky book where they had a quip about how wit the Baddeley model it really has like 150 permutations of it and some famous example of someone saying that men have more one night stands than women, but in a heterosexual population that is mathematically impossible. So it only seemed like it made sense to see if I could write one out.

So this is how I think it would work, currently in the process of coding it.

So essentially it’s a bayesian inspired model that compares a target melody, with a corpus of melodies or the prior, that are meant to represent all “known” musical material in someone’s musical understanding. I got the idea from reading about Cowan’s Embedded process model which does not posit the working memory structure as very distinct from that of everything in long term representation, but rather uses the central executive idea as the limited window of attention that can then be focused on things either in LTM or near categorical features of what is in the purveiw of attention.

Will first go through it on a high level, then make another pass and talk about how everything is getting operationalised. So imagine that we have a prior corpus of all the melodies that are covered for sight singing and dictaiton in the first semester of aural skills (in this case, melodies X--XX). If we let the corpus M represent all prior knowledge, where not only is the melody represented as a string of intervals, but we also get ever n gram permutation of of the melody based on the idea that the more frequent the exposure of an n-gram, the more likely it would be to be understood. 

This is the first big parameter M.

We could imagine and visualize this as a grid of distributions of all possible n-grams and the strings that make them up seen here GGPLOT2. 
Now although any string of notes is possible, in order to mirror the limited capacity of WMC, there needs to be some sort of threshold that is set that would mirror the limits of the window of WMC.

This threshold is the next big parameter, T. 

Currently conceptualizing this as being reflective of maximum of information content threshold as calculated by IDyOM.

Idea here is that as you hear more notes, more IC, the more your fixed capacity bin of melody is going to be filled up. By making it based on the IC of a melody string represented in the corpus, patterns that are more frequent are going to easier to then dictate, which aligns with intuitions on melodic dictation.

Also need to define a the explicit, implicit threshold. Explicitly known n-grams should have some sort of fixed amount that if it succedes that, you would know it. Like karpinski “knowing” phase, would also assume that every interval class would also be “known” and if not, that is OK too, will lead to people not being able to transcribe it. 

We’ve now defined both the individual knowledge parameters and teh thrshold T of working memory capacity. These consist of WHAT DO YOU CALL THIS. 

With these individual parameters established, we can calculate difficulty of the melody by running what I am going to be determining as the ‘transcribe’ function, which is separate from the individual parameters. 

We then can introduce the target melody , t  which represents the string to be dictated.

Idea here is that this melody is compared to the corpus, M, and you get the IC of each of the n-grams from the prior corpus.

Given the theshold, take the biggest n-gram that fits below the threshold to be put in the buffer. If you had a huge threshold and could take IC of whole melody or you KNEW it, would be reflective here. Could also introduce a function here that just gueses at about the threshold. Once this is filled up, let’s say 4 notes, the notes are then set to the transcription buffer. 

At this point index the n-gram is cross checked against the prior knowledge and look for matches of it that happen above a certain “known” threshold. If it is known, it gets “notated’ and you re-enter the melody.  If not, in this case the 4 gram would get recursively truncated until found a string, even 2 gram where it is known. If it gets to a 2 gram and it’s not explicitly known, basically because you don’t know that interval, what Karpinksi rails againt as atomistic hearing. Also good in that people with bigger chunking will do better.

If there is not an exact match, move to split fuzzy searches (future versions) 

Re-entry function is defined by finding the next part of the melody where either next most common n-gram occurs or where it left off last. This is then to reflect the two different ways of approaching tacking. Seing as some people start at front, others start at end. Could be reflective of primacy and recencey effects of memory, OR maybe it’s due to fact that more prototypical n-grams in habit starts and ends of melodies….?

This process would then re-occur a few times over until either can’t go anymore (intervals are not known) or till completion.

Afterwards have an idea of the path that it took, counts of those, use those as proxies of difficulty. 

I think this covers everything in the decision making process and each step of the process can be automated eventually (post doc anyone?!). More importantly, it reflects ecological phenomonologcial experience of taking melodic dictaiton. 

Aligns with intuitions that 
People who know more melodies will be better (singer phenomena) 
People will try for bigger chunks first, then go to smaller ones
Atomistic transcription can happen but is inefficient 
People with higher chunking ability will do better
IC is helpful proxy for this in line with IDyOM literature 
Relative pitch is where it’s at for things like this
What’s deal with AP? 



### Why?
##### Better than verbal models
##### Sometimes even mathematically infesable proposed theory
##### Beyond Karpinski in that it doesn't just schematize, says exactly when each thing is happening when
##### Lends itself to better discussions that don't just rely on personal anecdotes
##### Can tweak the parameters 
##### Can collect different types of data (corpus or experimental) and use the model
##### This model suggests that atomism approach is actual just subprocess of larger pattern 
#### Theoretical Justification
##### Marries literature on LTM and prior knowledge, information theory, WMC, computation, representation
##### Also can be implemented in computer
##### represntation of rhythm too?
##### inspired by people like margulis 2005, albrecht and shanahan key finding, want something to contribute
##### Really Made me think
#### The Model (note many parameters can be changed in R package)
#### Prior
###### Corpus of music represented in form of n-grams
###### IDyOM extracts all possible n-gram permutations as learned corpus
##### Music notation fed into processing window where incoming n-gram is matched based on WMC window OR IT maximum 
###### Information builds until approaches critical threhold 
###### Upon maximum, model puts n-gram into focus of attention (Cowan 1988) and note why this is better than Baddely Hitch
###### Recursive transcribe function looks for LTM matches 
####### Option 1: Pattern Matched and Pattern Transcribed, success?
####### Option 2: Pattern not matched in full, truncated and use match option again (should be higher probability of match with corpus)
####### Option 3: Pattern not matched downsize again until at interval level and relying on 2-gram (atomism)
####### On sucess of option, reopen gate at nearest long implicit n-gram LTM Match (start or end problem)
###### Put time contraints on search features 
###### Transcribe process resets with trace image of melody after each dictation 
###### Transcribe process ends when all notes accounted for 
#### Model Output
##### Based on leanrning, times needed to hear it
##### Completion percentage
##### Rank order of easier to transcribe parts based on learning
#### Model Compared to Data
##### With Experimental Data 
### Future Suggestions for Aural Skills Pedagoges and Research
#### Use model as teaching stepping off point
#### Should move towards LTM pattern matching
#### Reason that people learn how to sight sing is to INCREASE the learning of the implict corpus
#### Circular process here
#### Is this what it means to then think IN music
#### Really it's to just know the patterns maybe like model where Justin London suggests we get to know patterns and expect themn
#### Would also make sense in terms of Leonard Meyer 1956
#### Use WMC in music theory, cognition, education studies
 


