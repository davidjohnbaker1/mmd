\documentclass[12pt,]{book}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[left=1in, right=1in, top=1in, bottom=1in]{geometry}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{natbib}
\bibliographystyle{apalike}
\usepackage{longtable,booktabs}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\providecommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}

  \title{}
    \pretitle{\vspace{\droptitle}}
  \posttitle{}
    \author{}
    \preauthor{}\postauthor{}
      \predate{\centering\large\emph}
  \postdate{\par}
    \date{2019-06-06}

\usepackage{booktabs}
\usepackage{amsthm}
\usepackage{setspace}
\usepackage{xpatch}
\usepackage{sectsty}
\usepackage{graphicx}
\usepackage{etoolbox}
\usepackage{titlesec}
\usepackage{tocloft}    % tocloft for table of contents style
\makeatletter

%-------------------------------------------------------------------------
% Makes all titles size 14
%-----------------------------------------------------------------------

\titleformat*{\section}{\large\bfseries}
\titleformat*{\subsection}{\large\bfseries}
\titleformat*{\subsubsection}{\large\bfseries}
\titleformat*{\paragraph}{\large\bfseries}
\titleformat*{\subparagraph}{\large\bfseries}


\chaptertitlefont{\large}
%-------------------------------------------------------------------------
% Makes List of Figures and Tables size 14
%-----------------------------------------------------------------------
\setlength{\cftbeforetoctitleskip}{-1cm}
\setlength{\cftbeforeloftitleskip}{-1cm}
\setlength{\cftbeforelottitleskip}{-1cm}

\setlength{\cftaftertoctitleskip}{-0.1cm} % .25 is ok
\setlength{\cftafterloftitleskip}{-0.1cm}
\setlength{\cftafterlottitleskip}{-0.1cm}

\renewcommand{\cfttoctitlefont}{\large\bfseries}
\renewcommand{\cftloftitlefont}{\large\bfseries}
\renewcommand{\cftlottitlefont}{\large\bfseries}


%\renewcommand\cftloftitlefont{\large}
%\renewcommand\cftlottitlefont{\large}

%------------------------------------------------------------------------
%% ---- Adds ...s to everything
%------------------------------------------------------------------------

\renewcommand*\l@chapter[2]{%
   \ifnum \c@tocdepth >\m@ne
     \addpenalty{-\@highpenalty}%
     \vskip 1.0em \@plus\p@
     \setlength\@tempdima{1.5em}%
     \begingroup
       \parindent \z@ \rightskip \@pnumwidth
       \parfillskip -\@pnumwidth
       \leavevmode \bfseries
       \advance\leftskip\@tempdima
       \hskip -\leftskip
       #1\nobreak\normalfont\leaders\hbox{$\m@th
         \mkern \@dotsep mu\hbox{.}\mkern \@dotsep
         mu$}\hfill\nobreak\hb@xt@\@pnumwidth{\hss #2}\par
       \penalty\@highpenalty
     \endgroup
   \fi}

%% Does something
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}

\newlength{\chaptertopskip}
\newlength{\chapterbottomskip}
\setlength{\chaptertopskip}{-1.5cm} % 1cm close
\setlength{\chapterbottomskip}{5pt}

% Level 1 Headings
\def\@makechapterhead#1{%
  \vspace*{\chaptertopskip}%
  {\parindent \z@ \raggedright \large %normalfont
    \ifnum \c@secnumdepth >\m@ne
      \if@mainmatter
        \large\bfseries \@chapapp\space \thechapter\
   %     \par\nobreak % here
   %    \vskip 20\p@ % here
      \fi
    \fi
    \interlinepenalty\@M
    \large \bfseries #1\par\nobreak
    \vskip \chapterbottomskip
  }}

% Level 2 Headings
\def\@makesectionhead#1{%
  \vspace*{\chaptertopskip}%
  {\parindent \z@ \raggedright \large %normalfont
    \ifnum \c@secnumdepth >\m@ne
      \if@mainmatter
        \large\bfseries \@chapapp\space \thechapter\
%        \par\nobreak
%        \vskip 20\p@
      \fi
    \fi
    \interlinepenalty\@M
    \large \bfseries #1\par\nobreak
    \vskip \chapterbottomskip
  }}


% Works by
% \xpatchcmd{\foo}{<search>}{<replace>}{<true>}{<false>}

%% Supposed to remove Space at top of Chapters

% https://tex.stackexchange.com/questions/309920/how-to-move-the-chapter-title-upwards-on-page
%\xpatchcmd{\@makechapterhead}{\vspace*{50\p@}}{}{}{}% Removes space above \chapter head
%\xpatchcmd{\@makeschapterhead}{\vspace*{50\p@}}{}{}{}% Removes space above \chapter* head

% Between Chapter and Chapter Title
%\xpatchcmd{\@makechapterhead}{\vspace*{20\p@}}{}{}{}% Removes space above \chapter head
%\xpatchcmd{\@makeschapterhead}{\vspace*{20\p@}}{}{}{}% Removes space above \chapter* head

% Between Chapter Title and Text
%\xpatchcmd{\@makechapterhead}{\vspace*{40\p@}}{}{}{}% Removes space above \chapter head
%\xpatchcmd{\@makeschapterhead}{\vspace*{40\p@}}{}{}{}% Removes space above \chapter* head



%\renewcommand{\cftsecfont}{}% Remove \bfseries from section titles in ToC
%\renewcommand{\cftsecpagefont}{}% Remove \bfseries from section titles' page in ToC

%\renewcommand{\@makechapterhead}[1]{%
%  {\noindent\raggedright\normalfont% Alignment and font reset
%   \huge\bfseries \@chapapp\space\thechapter~~#1\par\nobreak}% Formatting
%  \vspace{\baselineskip}% ...just a little space
%}


%\renewcommand{\@makeschapterhead}[1]{%
%  {\noindent\raggedright\normalfont% Alignment and font reset
%   \huge\bfseries \@chapapp\space\thechapter~~#1\par\nobreak}% Formatting
%  \vspace{\baselineskip}% ...just a little space
%}

\makeatother % remove for lsu
\raggedbottom % remove for LSu
\frontmatter

\renewcommand\bibname{List of References}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage[table]{xcolor}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}

\begin{document}

%\newpage
\thispagestyle{empty}
\begin{center}
MODELING MELODIC DICTATION\\
\end{center}

\begin{center}
A Dissertation

Submitted to the Graduate Faculty of the\\
Louisiana State University and\\
Agricultural and Mechanical College\\
in partial fulfillment of the\\
requirements for the degree of\\
Doctor of Philosophy\\

in

The School of Music
\end{center}

\begin{center}
by\\
David John Baker\\
B.M., Baldwin Wallace University, 2012\\
MSc., Goldsmiths, University of London, 2014\\
August 2019\\
\end{center}


\setlength{\abovedisplayskip}{-5pt}
\setlength{\abovedisplayshortskip}{-5pt}

%\cleardoublepage\newpage\thispagestyle{empty}\null
%\cleardoublepage\newpage\thispagestyle{empty}\null

% Copyright
\newpage

\thispagestyle{plain}

Â© 2019 copyright

David John Baker

All rights reserved

%\cleardoublepage\newpage %%% KILL
%\newpage

\let\cleardoublepage\clearpage
% Dedication
%% REMOVE DOUBLE COMMENTS FOR PRINTING
%\pagestyle{empty}
%\chapter{Dedication}

%This dissertation is dedicated to my parents.
%Thank you for valuing education.
%Thank you for fostering my growth in every way possible.
%As a result, you've opened my eyes to the world.

\let\cleardoublepage\clearpage
%\setlength{\abovedisplayskip}{-5pt}
%\setlength{\abovedisplayshortskip}{-5pt}

\let\bforigdefault\bfdefault
\addtocontents{toc}{\let\string\bfdefault\string\mddefault}

% Acknowledgements
\pagestyle{plain}
\chapter{Acknowledgements}

This dissertation would not have been possible without the support of a large community people over the past four years.
First and foremost, I would like to acknowledge and thank my dissertation committee.
Daniel Shanahan, my main supervisor throughout my doctoral work, who provided countless office conversations and cups of coffee at Highland-- but more importantly-- encouraged me to pursue my research interests at the intersection of music theory and music perception; Emily Elliott, for your unmatched support and guidance; and Robert Peck, for being living proof that research with music and numbers can always go together.

Secondly, I want to thank the entire Louisiana State University community for making me feel welcomed and supported.
The people make the place and everyone I met during my time in Baton Rouge existed as a constant source of knowledge and inspiration throughout my time as a student.
This includes the Music Cognition and Computation Lab at LSU, the weekly Tin Roof Trivia group, and professors like Jeffrey Perry and Jason Harman who opened the door for me every time I knocked.

Thirdly, I want to especially thank those who either went out of their way to help me learn the more technical aspects of research who often go unacknowledged.
Craig Sapp, for dealing with me as I learned about Humdrum, Yihui Xie for writing the bookdown software I used to write this dissertation, the \#rstats community, specifically Hadley Wickham, for making R so accessible, and Daniel MÃ¼llensiefen for starting me on my journey into programming.
Without a commitment to free and open source software, this research and dissertation would not have been possible.

Lastly, I want to thank all of my friends and family.
Thank you to Will, Rory, Kevin, Amanda, Ryan, Chris, Peter, Jono, Kris, Sarah, Phil (also for the melodies!), Owen, Jacob, Michael, Adam, Mark, Sasha, Lauryn, Connor, Elizabeth, Tanush, Tony, Andrew, Anthony, Savanna, Jared, Matt, Andrew, Erin, Lizbeth, Greg, my parents John and Mary, and especially Evelyn for all your support and love.
\let\cleardoublepage\clearpage
\newpage

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\let\bforigdefault\bfdefault
\addtocontents{toc}{\let\string\bfdefault\string\mddefault}

\addcontentsline{toc}{chapter}{\listtablename}
\addcontentsline{toc}{chapter}{\listfigurename}

\newpage
\begin{doublespacing}
\listoftables
\end{doublespacing}
\newpage
\begin{doublespacing}
\listoffigures
\end{doublespacing}

\cleardoublepage\newpage
\chapter{Abstract}

Melodic dictation is a cognitively demanding process that requires students to hear a melody, then without any access to an external reference, transcribe the melody within a limited time frame.
Despite its ubiquity in curricula within School of Music settings, exactly how an individual learns a melody is not well understood.
This dissertation aims to fill the gap in the literature between aural skills practitioners and music psychologists in order to reach conclusions that can be applied systematically in pedagogical contexts.
In order to do this, I synthesize literature from music theory, music psychology, and music education in order to demonstrate how tools from both cognitive psychology as well as computational musicology can be used to help inform pedagogical practices.

In the second chapter, I discuss factors that might play a role in a student's ability to take melodic dictation and put forward a taxonomy of factors that are assumed to contribute to an individual's ability to take melodic dictation.
The third chapter of the dissertation investigates individual factors that are theorized to contribute to melodic dictation using a cross-sectional experimental design.
The chapter corroborates claims on the importance of understanding individual differences in working memory capacity in research on melodic dictation.
The fourth chapter discusses how aural skills pedagogy can incorporate methodologies from computational musicology in order to inform teaching practice.
In my fifth chapter, I introduce the MeloSol corpus, a new collection of 783 digitized melodies encoded in the **kern format.
In the sixth chapter, I synthesize the previous research in a melodic dictation experiment to show how using robust statistical methods can be used model melodic dictation.
Finally, in my seventh chapter, I introduce a computational, cognitive model of melodic dictation with the goal of helping explain how students improve at melodic dictation.
I demonstrate how modeling the cognitive decision process during melodic dictation helps provide a precise framework for pedagogues to understand student's inner cognition during melodic dictation and can help inform teaching practice.
\mainmatter 

\addtocontents{toc}{\let\string\bfdefault\string\bfdefault}

\hypertarget{significance-of-the-study}{%
\chapter{Significance of the Study}\label{significance-of-the-study}}

\hypertarget{rationale}{%
\section{Rationale}\label{rationale}}

All students pursuing a Bachelor's degree in Music from universities accredited by the National Association of Schools of Music must learn to take melodic dictation \citep[Â§VIII.6.B.2.A]{nasmNationalAssociationSchools2019}.
Melodic dictation is a cognitively demanding process that requires students to hear a melody, then without any access to an external reference, transcribe the melody within a limited time frame.
As of 2019, there are 643 Schools of Music belonging to National Association of Schools of Music, meaning that thousands of students every year will be expected to learn this challenging task as part of their aural skills education.
The implicit logic is that as one improves in their ability to take melodic dictation, this practice of critical and active listening develops one's ability to ``think in music'' \citep{bestMusicCurriculaFuture1992, karpinskiAuralSkillsAcquisition2000} and thus become a more competent musician.

Despite its ubiquity in curricula within School of Music settings, research that explains how people learn to take melodic dictation is limited at best.
The fields of music theory and cognitive psychology are best positioned to make progress on this question, but often the skills required to be well versed in these subjects do not overlap in formal training, findings related to this question are published in separate journals, and thus research that joins these two fields is scarce.
This problem is not new and there have been repeated attempts to bridge the gap between practitioners of aural skills and researchers in cognitive psychology \citep{butlerBridgesUnbuiltComparing1993, davidbutlerWhyGulfMusic1997a, klonoskiImprovingDictationAuralSkills2006, klonoskiPerceptualLearningHierarchy2000, pembrookSendHelpAural1990, karpinskiAuralSkillsAcquisition2000}.
Literature from music theory has established conceptual frameworks regarding aural skills \citep{karpinskiAuralSkillsAcquisition2000}, cognitive psychology literature has explored factors that might contribute to melodic perception \citep{dowlingExpectancyAttentionMelody1990, dowlingScaleContourTwo1978, dowlingTonalStrengthMelody1991, halpernMemoryMelodies2010}, and applied literature from the field of music education \citep{buonviriEffectsTwoListening2017, buonviriMelodicDictationInstruction2015, paneyEffectDirectingAttention2016} has investigated how people learn melodies in more ecological settings.

However, despite these isolated areas of research, we as music researchers do not have a clear understanding of exactly what contributes to the process of how individuals learn melodies \citep{halpernMemoryMelodies2010}.
This is peculiar since ``how does one learn a melody?'' seems to be one of the fundamental questions to the fields of music theory, music psychology, as well as music education.
This chasm in the literature also raises a disconcerting question in music pedagogy:

\begin{quote}
If we as pedagogues do not have an in-depth understanding of how people learn melodies, how can we fairly assess what students can be expected to accomplish in the classroom and then fairly grade them on their attempts?
\end{quote}

This dissertation aims to fill the gap in the literature between aural skills practitioners and music psychologists in order to reach conclusions that can be applied systematically in pedagogical contexts.
In order to do this, I synthesize literature from music theory, music psychology, and music education in order to demonstrate how tools from both cognitive psychology as well as computational musicology can be used to help inform pedagogical practices.

\hypertarget{chapter-overview}{%
\section{Chapter Overview}\label{chapter-overview}}

In the second chapter, I begin by extending the work of Gary Karpinski \citep{karpinskiAuralSkillsAcquisition2000, karpinskiModelMusicPerception1990} in order to introduce the process of melodic dictation and discuss factors that might play a role in a student's ability to take melodic dictation.
The chapter introduces both a theoretical background and rationale for using methods from both computational musicology and cognitive psychology in order to answer questions about how individuals learn melodies.
In order to organize the disparate literatures, I put forward a taxonomy of factors that are assumed to contribute to an individual's ability to take melodic dictation and discuss each in turn.
This chapter outlines the factors hypothesized to contribute to an individual's ability to learn melodies, incorporating both individual and musical parameters.
I conclude the chapter with a discussion of some philosophical and theoretical problems when attempting to measure issues concerning melodic dictation and argue for the advantages of answering this problem using a polymorphic view of musicianship \citep{levitinWhatDoesIt2012, peretzNatureMusicBiological2006, bakerExaminingMusicalSophistication2018a}.

The third chapter of the dissertation investigates individual factors that are theorized to contribute to melodic dictation.
I argue that because the first two steps of Karpinski's model of melodic dictation do not require any musical training, teasing apart the individual factors that contribute to melodic dictation can be done using a memory for melodies paradigm.
I interpret the results of an experiment to highlight the importance of working memory processes in melodic dictation.
The chapter corroborates claims by \citet{berzWorkingMemoryMusic1995} on the importance of understanding individual differences in working memory capacity and establishes the rationale for including differences in cognitive ability as a variable of interest in future research on melodic dictation.

The fourth chapter of the dissertation discusses how aural skills pedagogy could incorporate methodologies from computational musicology in order to inform teaching practice.
The chapter begins by establishing the degree to which aural skills pedagogues agree on the difficulty of melodies for melodic dictation using a survey representing 40 aural skills pedagogues.
I then show how different sets of tools from computational musicology can approximate the intuitions of aural skills pedagogues using the survey data as a ground truth.
The chapter concludes by putting forward a novel theory of musical memory-- The Frequency Facilitation Hypothesis-- which combines theoretical work from cognitive psychology and computational musicology.
I show how this hypothesis can be applied in pedagogical settings to create a more linear path to success in the aural skills classroom for students.

In my fifth chapter, I introduce a novel corpus of 783 digitized melodies encoded in the \texttt{**kern}
format \citep{huronHumdrumToolkitReference1994}.
This chapter provides a rationale for the introduction of a new corpus in the broader context of computational musicology, contains a small corpus analysis in order to contextualize the new \emph{MeloSol} corpus, and ends with a brief commentary regarding ontological and epistemological assumptions in music corpus research.
This dataset serves as a resource for future researchers in music, psychology, and the digital humanities.

In the sixth chapter, I synthesize the previous research in a melodic dictation experiment.
Stimuli for the experiments are selected based on the symbolic features of the melodies discussed in earlier chapters and are manipulated as the independent variables.
I then model responses from the experiments using both individual factors and musical features using mixed-effects modeling in order to predict how well an individual will perform in a melodic dictation.
In discussing the results, I also note important caveats in scoring melodic dictation and highlight how changes in scoring can lead to changes in the final modeling.
Results from this chapter will be discussed with reference to how findings are applicable to pedagogues in aural skills settings.

Finally, in my seventh chapter, I introduce a computational, cognitive model of melodic dictation with the goal of helping explain how students improve at melodic dictation.
The model is based in research from both cognitive psychology \citep{cowanMagicalMysteryFour2010} and computational musicology \citep{pearceStatisticalLearningProbabilistic2018a} and incorporates relevant theoretical aspects such as working memory \citep{chenetteReframingAuralSkills2019, vanhandelRoleWorkingMemory2011} as well as the structure of the melody itself.
In this chapter, I demonstrate how modeling the cognitive decision process during melodic dictation helps provide a precise framework for pedagogues to understand student's inner cognition during melodic dictation and can help inform teaching practice.

\hypertarget{intro}{%
\chapter{Context, Literature, and Rationale}\label{intro}}

\hypertarget{melodic-dictation}{%
\section{Melodic Dictation}\label{melodic-dictation}}

Melodic dictation is the process in which an individual hears a melody, retains it in memory, and then uses their knowledge of Western musical notation to recreate the mental image of the melody on paper in a limited time frame.
For many, becoming proficient at this task is at the core of developing one's aural skills \citep{karpinskiModelMusicPerception1990}.
For over a century, music pedagogues have valued melodic dictation,\footnote{In his highly influential book \emph{Aural Skills Acquisition: The Development of Listening, Reading, and Performing Skills in College-Level Musicians}, \citet{karpinskiAuralSkillsAcquisition2000} documents this sentiment in music pedagogy circles by highlighting poetic adages from Romantic composer Robert Schumann in the mid-19th century through 21st century music educator Charles Elliott, thus providing examples of the belief that improving one's aural skills, or \emph{ear}, is a highly sought after skill.} which is evident from the fact that most aural skills texts with content devoted to honing one's listening skills have sections on melodic dictation \citep{karpinskiAuralSkillsAcquisition2000}.

Yet despite this tradition and ubiquity, the rationales as to \emph{why} it is important for students to learn this ability often comes from some sort of appeal to tradition or underwhelming anecdotal evidence.
The argument tends to go that time spent learning to take melodic dictation results in increases in near transfer abilities after an individual acquires a certain degree of proficiency learning to take melodic dictation.
Rationales given for why students should learn melodic dictation has even been described by Karpinski as being based on ``comparatively vague aphorisms about mental relationships and intelligent listening'' \citep[p.~192]{karpinskiModelMusicPerception1990}, thus leaving the evidence for the argument for learning to take melodic dictation not well supported.

Some researchers have taken a more skeptical stance and asserted that the rationale for why we teach melodic dictation deserves more critique.
For example, Klonoski, in writing about aural skills education aptly questions: ``What specific deficiency is revealed with an incorrect response in melodic dictation settings?'' \citep{klonoskiImprovingDictationAuralSkills2006}.
Earlier researchers like Potter, in their own publications, have noted that many musicians do not actually keep up with their melodic dictation abilities after their formal education ends \citep{potterIdentifyingSucessfulDictation1990}, but presumably go on to have successful and fulfilling musical lives.
Additionally, suggesting that people who are unable to hear and then dictate music, thus presumably unable to think \emph{in} music \citep{karpinskiAuralSkillsAcquisition2000}, seems to forget many musical cultures that do not depend on written notation.

Despite this skepticism towards the topic, melodic dictation remains at the forefront of many aural skills curricula.
The act of becoming better at this skill may or may not lead to large increases in far transfer of ability, but when used as a pedagogical tool, the practice of learning to take melodic dictation intersects with current learning goals relevant to the core of undergraduate music training.
While there has not been extensive research on melodic dictation research in recent years--- in fact, \citet{paneyEffectDirectingAttention2016} notes that since 2000, only four studies were published that directly examined melodic dictation--- this skill set sits on the border between literature on music learning, melodic perception, memory, and music theory pedagogy.
Understanding and modeling the processes underlying melodic dictation remains as an untapped watershed of knowledge for the field of music theory, music education, and music perception, and is deserving of more attention.

In this chapter, I examine literature both directly and indirectly related to melodic dictation by first reviewing the prominent four-step model put forth by Karpinski in order to establish and describe melodic dictation.
After describing his model, I then critique this model and put forward a new taxonomy of parameters that presumably would contribute to an individual's ability to take melodic dictation.
Using this taxonomy, I then review relevant literature and assert that the next steps forward in understanding the underlying processes of melodic dictation by examining melodic dictation both experimentally and computationally.
This research aims to continue to fill the long called for bridging between the fields of aural skills pedagogy and music cognition \citep{davidbutlerWhyGulfMusic1997a, karpinskiAuralSkillsAcquisition2000, klonoskiPerceptualLearningHierarchy2000}.

\hypertarget{describing-melodic-dictation}{%
\subsection{Describing Melodic Dictation}\label{describing-melodic-dictation}}

The foundational pedagogical work on melodic dictation comes from the work of Karpinski.
Summarized most recently in his \emph{Aural Skills Acquisition} \citep{karpinskiAuralSkillsAcquisition2000}--- though first presented in an earlier article \citep{karpinskiModelMusicPerception1990}--- Karpinski proposes a four-step model of melodic dictation.

The four steps of Karpinski's model include

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Hearing
\item
  Short Term Melodic Memory
\item
  Musical Understanding
\item
  Notation
\end{enumerate}

and occur as a looping process, which I have reproduced and depicted in Figure \ref{fig:flowchart}\footnote{Image reproduced with permission. Image taken from Aural Skills Acquisition (2000), p.~102, Figure 1.2}.
Previous attempts to distill melodic dictation into a series of discrete steps have ranged from Michael Roger's assertion of only needing two steps, to Ronald Thomas, who claimed as many as 15 steps, to similar models proposed by Colin Wright that model inner hearing as a five step model \citep{wrightInvestigatingAuralCase2016, karpinskiAuralSkillsAcquisition2000}.
Karpinski's model is discussed extensively in both the original article \citep{karpinskiModelMusicPerception1990} and throughout the third chapter in his book \citep{karpinskiAuralSkillsAcquisition2000}.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/karpinski31} 

}

\caption{Karpinski Model of Melodic Dictation}\label{fig:flowchart}
\end{figure}

Karpinski's hearing stage involves the initial perceptions of the sound at the psychoacoustical level and the listener's attention to the incoming musical information.
If the listener is not actively engaging in the task because of extrinsic factors such as ``boredom, lack of discipline, test anxiety, attention deficit disorder, or any number of other causes'' \citep[p.65]{karpinskiAuralSkillsAcquisition2000}, then any further processes in the model will be detrimentally affected.
Karpinski notes that these types of interferences are normally ``beyond the traditional jurisdiction of aural skills instruction'' \citep[p.65]{karpinskiAuralSkillsAcquisition2000}, but I will later argue that the concept of willful attention, when re-conceptualized as working memory, may actually play a larger role in the melodic dictation process as it is modeled here.

The short-term melodic memory stage in this process references the point in a melodic dictation where musical material is held in active memory.
From Figure \ref{fig:flowchart} and Karpinski's writing on the model, this stage is not explicitly declared as any sort of active process akin to a phonological loop \citep{baddeleyEpisodicBufferNew2000} where active rehearsal might occur, but describes where in the sequential order melodic information is represented.
Though Karpinski does not posit any sort of active process in the short term melodic memory stage, he does suggest there are two separate memory encoding mechanisms, one for contour, and one for pitch.
He arrives at these two mechanisms by using both empirical qualitative interview evidence, as well as noting literature from music perception that supports this claim for contour \citep{dowlingScaleContourTwo1978, dewittRecognitionNovelMelodies1986} and literature suggesting that memory for melodic material is dependent on enculturation \citep{ouraMemoryMelodiesSubjects1988, handelListeningIntroductionPerception1989, dowlingExpectancyAttentionMelody1990}.
Since its publication in 2000, this area of research has expanded with other researchers also demonstrating the effects of musical enculturation via exposure \citep{eerolaExpectancySamiYoiks2009, stevensMusicPerceptionCognition2012, pearceAuditoryExpectationInformation2012, pearceStatisticalLearningProbabilistic2018a}.

In describing the short term melodic memory stage, Karpinski also details two processes that he believes to be necessary for this part of melodic dictation: extractive listening and chunking.
Noting that there is a capacity limit to the perception of musical material by citing Miller \citeyearpar{millerMagicalNumberSeven1956}, Karpinski explains how each strategy might be incorporated.
Extractive listening is the process in which someone dictating the melody will selectively remember only a small part of the melody in order to lessen the load on memory.
Chunking is the process in which smaller musical elements can be fused together in order to expand how much information can be actively held and manipulated in memory.
The concept of chunking is very helpful as a pedagogical tool, but as detailed below, is complicated to formalize.

After musical material is extracted and then represented in memory, the next step in the process is musical understanding.
At this point in the dictation, the individual taking the dictation needs to mentalize the extracted musical material that is represented in memory and then use their music theoretic knowledge in order to comprehend any sort of hierarchical relationships between notes, common rhythmic groupings, or any sorts of tonal functions.
This is the point in the process where solmization of either or both pitch and rhythm, and musical material might be understood in terms of relative pitch.
While Karpinski reserves his discussion of solmization for the musical understanding phase, it is worth questioning if it is possible to disassociate relative pitch relations that would be understood in this phase from the qualia of the tones themselves \citep{arthurPerceptualStudyScaledegree2018}.
For Karpinski, the quicker what is represented in musical memory can be understood, the quicker it can then be translated at the final step of notation.

Notation, the final step of the dictation loop, requires that the individual taking the notation have sufficient knowledge of Western musical notation so that they are able to translate their musical understanding into written notation.
This last step is ripe for errors and has proved problematic for researchers attempting to study dictation \citep{taylorStrategiesMemoryShort1983, klonoskiImprovingDictationAuralSkills2006}.
It is also worth highlighting that it is difficult to notate musical material if the individual who is dictating does not have the requisite musical category and knowledge for the sounds that are actively represented in memory.
Lack of this knowledge will limit an individual's ability to translate what is in their short term melodic memory into notation, even if accurately represented in memory.

Nearer the conclusion of the chapter, Karpinski notes that other factors like tempo, the length and number of playings, and the duration between playings also play a role in determining how an individual will perform on a melodic dictation.
While this framework can help illuminate this cognitive process and help pedagogues understand how to best help their students, presumably there are many more factors that contribute to this process.
The model as it stands is not detailed enough for explanatory purposes and lacks in two areas that would need to be expanded if this model were to be explored experimentally and computationally.

First, having a single model for melodic dictation assumes that all individuals are likely to engage in this sequential ordering of events.
This could in fact be the case,\footnote{And in his Figure 3.1 he does caption it as an \emph{idealized} dictation process} but there is research from music perception \citep{goldmanImprovisationExperiencePredicts2018} and other areas of memory psychology such as work on expert chess players \citep{laneChessKnowledgePredicts2018} that suggests that as individuals gain more expertise in a specific domain, their processing and categorization of information changes.
Additionally, different individuals will most likely have different experiences dictating melodies based on their own past listening experience, an area that Karpinski refers to when citing literature on musical enculturation based on statistical exposure.
The model does not have any flexibility in terms of individual differences.

Second, the model presumes the same sequence of events for every melody.
As a general heuristic for communicating the process, this model serves as an excellent didactic tool.
When this model is applied to more diverse repertoire, this same set of strategies performed in this order might prove to be inefficient.
For example, on page 103 of his text, Karpinski suggests that two listenings should be adequate for a listener with few to no chunking skills to be able to dictate a melody of twelve to twenty notes.
This process might generalize to many tonal melodies, but presumably different strategies in recognition would be involved in dictating the two melodies of equal length shown in Figures \ref{fig:shortmelody1} and \ref{fig:shortmelody2}.
If asked to dictate \ref{fig:shortmelody1}, long term memory processes might begin to play a role much sooner during this task.
If asked to dictate \ref{fig:shortmelody2}, establishing a tonal center to act as a perceptual scaffolding for relative pitch relationships might prove to be more difficult.
Presumably different people with different levels of abilities will perform differently on different melodies.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/musicalexamples/MMD_Figure2-1} 

}

\caption{Tonal Melody}\label{fig:shortmelody1}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/musicalexamples/MMD_Figure3-1} 

}

\caption{Atonal Melody}\label{fig:shortmelody2}
\end{figure}

This agnosticism for both variability for melodic and individual differences serves as a stepping off point for this study.
In order to have a more complete understanding of melodic dictation, a model should be able to accommodate the exhaustive differences at both the individual and musical levels.
Additionally, the model should be able to be operationalized so that it can be explored in both experimental and computational settings.
Explicitly stating variables thought to contribute the underlying processes of melodic dictation will give aural skills pedagogues a more exact framework for discussing melodic dictation.
In turn, this will enable a more complete understanding of melodic perception and subsequently allow for better teaching practices in aural skills classrooms.

\hypertarget{taxonomizing}{%
\subsection{Taxonomizing}\label{taxonomizing}}

At this point, it is worth stepping back and noting that the sheer number of variables at play here is cumbersome and haphazard.
In order to better understand and organize factors thought to contribute to this process, it would be advantageous to taxonomize the multitude of features thought to contribute to melodic dictation.
In doing this, it will allow for a clearer picture of what factors might contribute and what literatures to explore in order to learn more about them.

The taxonomy that I propose appears in Figure \ref{fig:taxonomy} and bifurcates the factors that might affect an individual's ability to take melodic dictation.
This division creates both individual parameters and musical parameters.
These categories are recursively partitioned into cognitive and environmental parameters, as well as structural and experimental factors respectively.
Below I expand on what these categories entail, then explore each in-depth.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/taxonomy} 

}

\caption{Taxonomy of Factors Contributing to Aural Skills}\label{fig:taxonomy}
\end{figure}

The individual parameters are split broadly into cognitive factors and environmental factors.
Factors in the cognitive domain are assumed to be relatively consistent over time.
Factors in the environmental domain are subject to change via training and exposure.
These categories are not deterministic, nor exclusive, and almost inevitably interact with one another.

For example, it would be possible to imagine an individual with higher cognitive ability, the opportunity to have a high degree of training early on in their musical career, and personality traits that lead them to enjoy engaging with a task like melodic dictation.
This individual's melodic dictation abilities might be markedly different then someone with lower cognitive abilities, no opportunity for individualized training, and may not have a general inclination to even take music lessons.
This variability at the individual level might then lead to differences in their ability to take melodic dictation.

Complementing the individual differences, there would also be differences at the musical level which in turn divides into two categories.
On one hand exists the structural aspects of the melody itself.
These are aspects of the melody that would remain invariant when written down on musical notation that can only capture pitch changes over time.
Parameters in this category would include features generated by the interval structure of the pitches over time that allow the melody to be perceived as categorically distinct from other melodies.
These structural features are then complemented by the experimental features which are emergent properties of the structural relation of the pitches over time based on performance practice choices.
Examples of these parameters would include, key, tempo, note density, tonalness, timbral qualities, and the amount of times a melody is played during a melodic dictation.
Again, this division is not an exhaustive, categorical divide.
One could imagine exceptions to these rules where a major mode melody played on the piano is transformed to the minor mode, ornamented, and then played with extensive rubato by a trumpet.
This new melody might be experienced as a phenomenologically distinct, yet similar experience.
This division of structural and experimental features is similar to Leonard Meyer's primary and secondary musical features \citep{meyerEmotionMeaningMusic1956}.

Given all of these parameters that could contribute to the melodic dictation process, the remainder of this chapter will explore literature using this taxonomy as a guide.
The chapter concludes with a reflection on operationalizing each of these factors and problems that can arise in modeling.
These are important to note since from an empirical standpoint, both the task as well as the process of melodic dictation as depicted by Karpinski resemble a process that could be operationalized as both an experiment, as well as a computational model.

\hypertarget{individual-factors}{%
\section{Individual Factors}\label{individual-factors}}

\hypertarget{cognitive}{%
\subsection{Cognitive}\label{cognitive}}

Research from cognitive psychology suggests that individuals differ in their perceptual and cognitive abilities in ways that are both stable throughout a lifetime and are not easily influenced by short term training.
When investigated on a large scale, these abilities--- such as general intelligence or working memory capacity--- predict a wealth of human behavior on a large scale ranging from longevity, annual income, ability to deal with stressful life events, and even the onset of Alzheimer's disease \citep{ritchieIntelligenceAllThat2015, unsworthAutomatedVersionOperation2005}.
Given the strength and generality of these predictors, it is worth investigating the extent that these abilities might contribute when investigating any modeling of melodic dictation, since melodic dictation depends on perceptual abilities.
It is important to understand the degree to which these cognitive factors might influence aural skills abilities in order to ensure that the types of assessments that are given in music schools validly measure abilities that individuals have the ability to improve.
If it is the case that much of the variance in a student's aural skills academic performance can be attributed to something the student has little control over, this would call for a serious upheaval of the current model of aural skills teaching and assessment.

Recently there has been interest in work exploring how cognitive factors are related to abilities in music school.
This interest is probably best explained by the fact that educators are aware of the fact that cognitive abilities are powerful predictors and need to be understood since they inevitably will play a role in pedagogical settings.
Before diving into a discussion regarding differences in cognitive ability, I should note that ideas regarding differences in cognitive ability have been negatively received and for good reasons.
Research in individual differences in cognitive ability can and has been taken advantage to further specious ideologies, but often arguments that assert meaningful differences in cognitive abilities between groups are founded on statistical misunderstandings and have been addressed in other literature \citep{gouldMismeasureMan1996}.
These differences cannot be brushed aside because it is very difficult to maintain a scientific commitment to the theory of evolution \citep{darwinOriginSpecies1859} and not expect variation in all aspects of human behavior, with cognition falling under that umbrella.

\hypertarget{general-intelligence}{%
\subsubsection{General Intelligence}\label{general-intelligence}}

Attempting to measure and quantify aspects of cognition date back over a century.
Even before concepts of intelligence were posited by Charles Spearman via his conception of \emph{g} \citep{spearmanGeneralIntelligenceObjectively1904}, scientists were interested in establishing links between latent constructs they presumed to exist in the real world--- yet were impossible to measure directly like intelligence--- and physical manifestations that could be measured such as body morphology \citep{gouldMismeasureMan1996}.
While scholars like Gould have documented and critiqued much of the history of early psychometrics\footnote{Gould puts forward a complete, yet very charged reading of the early history of cognitive testing and his writings on the subject have been accused of falling prey to the same logic he rails against \citep{warneStephenJayGould2019}}, central to this study are two important schools of thought on intelligence testing commonly discussed in the current literature.

The first ideology originates from Cyril Burt and Charles Spearman who, in developing the statistical tool of factor analysis, posited that a construct of general intelligence exists as a part of human cognition and can be quantified.
Burt and Spearman claimed that a general intelligence factor existed in human cognition from evidence they put forward developing a battery of cognitive tests whose performance on one subtest could often reliably predict performance on another.
This phenomena of multiple related tests predicting each other's performance is a manifestation referred to as the positive manifold.
Spearman and Burt asserted that an individual's ability to solve problems without contextual background information could be understood as general intelligence or \emph{g} \citep{spearmanGeneralIntelligenceObjectively1904}.

Broadly speaking, the second ideology here stems from work by Alfred Binet, who instead of conceptualizing intelligence as a monolithic whole, partitioned intelligence into what today has become understood to be defined as differences in general crystallized intelligence or ( \emph{Gc} ) and general fluid intelligence ( \emph{Gf} ).
General crystallized intelligence is the ability to solve problems given prior contextual information;
General fluid intelligence is the ability to solve problems in novel contexts \citep{cattellAbilitiesTheirGrowth1971, jhornTheoryFluidCrystalized1994} .
Comparing \emph{Gf} and \emph{Gc} to \emph{g}, the cognitive psychology literature has noted that \emph{g} often shares a statistically equivalent relationship to an idea conceptualized as general fluid intelligence \citep{matzkeIssuePowerIdentification2010}.
These conceptions of intelligence and cognitive ability also differ from more current theories that synthesize these previous areas of research \citep{kovacsProcessOverlapTheory2016} using models that do not require taking an ontological stance of entity realism \citep{borsboomTheoreticalStatusLatent2003}.

Even though both of these constructs are powerful predictors on a large scale and do predict variables such as educational success, income, and even life expectancy \citep{ritchieIntelligenceAllThat2015} when confounding variables like socioeconomic status are accounted for, conceptualizing cognitive abilities in terms of only a handful of latent constructs still does not fully explain the diversity of human cognition.
Regardless of their origin, neglecting the predictive power of these variables in pedagogical settings would be a methodological oversight in attempting to explain variance in performance.

\hypertarget{working-memory-capacity}{%
\subsubsection{Working Memory Capacity}\label{working-memory-capacity}}

In addition to concepts of intelligence, be it \emph{Gf} or \emph{Gc}, the working memory capacity literature directly relates to work on melodic dictation.
Working memory is one of the most investigated concepts in the cognitive psychology literature.
According to Nelson Cowan, the term working memory generally refers to

\begin{quote}
the relatively small amount of information that one can hold in mind, attend to, or, technically speaking, maintain in a rapidly accessible state at one time. The term working is meant to indicate that mental work requires the use of such information. \citep[p.~1]{cowanWorkingMemoryCapacity2005}
\end{quote}

The term does not have an exact definition, nor does it have a definitive method of measurement.
While there is no universally recognized first use of the term, researchers began to postulate that there was some sort of system that mediated incoming sensory information from the external world with information in long term storage using modular models of memory in the mid-twentieth century.
Summarized in \citet{cowanWorkingMemoryCapacity2005}, one of the first modal models of memory was proposed by \citet{broadbentPerceptionCommunication1958} and later expanded by \citet{atkinsonHumanMemoryProposed1968}.
As seen in Figure \ref{fig:wmmodels}\footnote{Image reproduced with permission. Image taken from Working Memory (Cowan, 2005), p.~18, Figure 1.2} taken from \citet{cowanWorkingMemoryCapacity2005}, both models here posit incoming information that is then put into some sort of limited capacity store.
These modal models were then expanded on by Baddeley and Hitch \citep{baddeleyWorkingMemory1974} in their 1974 chapter with the name Working Memory, where they proposed a system with an central executive module that was able to carry out active maintenance and rehearsal of information that could be stored in either a phonological store for sounds or a visual sketchpad for images.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/wm_models} 

}

\caption{Models of Working Memory}\label{fig:wmmodels}
\end{figure}

Later revisions of their model also incorporated an episodic buffer \citep{baddeleyEpisodicBufferNew2000} where the modules were explicitly depicted as being able to interface with long term memory in the rehearsal processes.
The model has even been expanded upon by other researchers throughout its lifetime.
The most relevant to this study is by \citet{berzWorkingMemoryMusic1995}, who postulated the addition of a musical rehearsal loop to the already established phonological loop and visual spatial sketchpad.
While Berz is most likely correct in asserting that the nature of storing and processing musical information is different to that of words or pictures and there has been experimental evidence to suggest this \citep{williamsonMusiciansNonmusiciansShortterm2010} that has been interpreted in favor of multiple loops \citep{wollnerAttentionalFlexibilityMemory2016}, the idea of multiple loops introduces the theoretical problem of determining how and why incoming sensory information is partitioned into their respective loops.
Additionally, models that assert some sort of central executive component to attend to materials held in a sensory buffer also face the infinite regress homunculus problem.
Stated more clearly, if the central executive system is what attends to information in the sensory buffers, what attends to the central executive?

In addressing the problem of explicitly stating which rehearsal loops do and do not exist, Nelson Cowan proposed a separate model \citep{cowanEvolvingConceptionsMemory1988, cowanWorkingMemoryCapacity2005}, dubbed the Embedded Process Model, which does not claim the existence of any domain specific module (e.g.~positing a phonological loop, visual spatial sketchpad) but is rather based on an exhaustive model that did away with the problem of asserting specific buffers for specific types of information.

In Cowan's own words comparing his model from that of Baddeley and Hitch:

\begin{quote}
The aim was to see if the description of the processing structure could be exhaustive, even if not complete, in detail. By analogy, consider two descriptions of a house that has not been explored completely. Perhaps it has only been examined from the outside. Baddeley's (1986) approach to modeling can be compared with hypothesizing that there is a kitchen, a bathroom, two equal-size square bedrooms, and a living room. This is not a bad guess, but it does not rule out the possibility that there actually are extra bedrooms or bathroom, that the bedroom space is apportioned into two rooms very different in size, or that other rooms exist in the house. Cowan's (1988) approach, on the other hand, can be compared with hypothesizing that the house includes food preparation quarters, sleeping quarters, bathroom/toilet quarters, and other living quarters. It is meant to be exhaustive in that nothing was left out, even though it is noncommittal on the details of some of the rooms. (p.42) \citep{cowanWorkingMemoryCapacity2005}
\end{quote}

The system is depicted in the bottom tier of Figure \ref{fig:wmmodels}, and conceptualizes the limited amount of information that is readily available as being in the focus of attention.
In this model, activated sensory and categorical features of the focus of attention are thus readily accessible.
Moving further from the locus of attention is long term memory, whose content can be activated using the Central Executive to access non-immediately available information.
The central executive system in this case acts as a spotlight on what is represented in long term memory, rather than a module used to direct attention to specific sensory information.
This change in definition does not completely escape the homunculus problem, but does change the central executive's role in the memory process.
In contrast to the modular approaches, Cowan's framework does not require researchers to specify exactly how and where incoming information is being stored.
This makes it advantageous for studying complex stimuli such as music and melodies.
Using this definition of working memory would require collapsing the first two steps of the Karpinski model of melodic dictation into one step.
In this case, attention is the window of active memory.

In addition to having multiple frameworks for studying working memory capacity, there is also the problem of limits to the working memory system, often referred to as the working memory capacity.
Most popularized by Miller in his famous \citep{millerMagicalNumberSeven1956} speech turned article, Miller suggests out of jest that the number 7 might be worthy of investigating in terms of how many items can be remembered, which has been used as a point of reference for many researchers since then.
It is worth noting that Miller has gone on record as noting that using 7 (plus or minus 2) was a rhetorical device intended to string together his speech and not a claim to pursue the number seven as the limit for memory \citep{millerHistoryPsychologyAutobiography1989}.
Nevertheless, while the number seven is most likely a red herring, it did inspire a large amount of research on capacity limits.
In the decades since, the number 7 has been reduced to about 4 \citep{cowanMagicalMysteryFour2010} and research around capacity limits has been investigated using a variety of novel tasks, most notably the complex span task \citep{unsworthAutomatedVersionOperation2005, unsworthComplexWorkingMemory2009}.
When complex span tasks are used as a measure of working memory capacity, they tend to be both valid and reliable psychometric tools that are stable across a lifetime \citep{unsworthAutomatedVersionOperation2005}.

\hypertarget{working-memory-and-melodic-dictation}{%
\subsubsection{Working Memory and Melodic Dictation}\label{working-memory-and-melodic-dictation}}

Clearly an individual's ability to take in sensory information, maintain it in memory and actively carry out other tasks are almost identical to tasks of working memory capacity.
Before venturing onward and further discussing the importance of this striking parallel, a few clear distinctions between methods used to study working memory and melodic dictation need to be made explicit.
While these two tasks resemble each other, a few key differences exist that researchers must note.

Tasks investigating working memory capacity differ from melodic dictation tasks in a few key ways.
The first is that musical information is always sequential: a melodic dictation task would never require the student to recall the pitches back in scrambled orders.
Serial order recall is an important characteristic in the scoring and analyzing of working memory tasks \citep{conwayWorkingMemorySpan2005}, but musical tones do not appear in random order and are normally in discernible chunks as discussed by Karpinski \citep{karpinskiAuralSkillsAcquisition2000}.
The use of chunks is pervasive in any literature on working memory and is often used as a heuristic to help explain why information is grouped together.
Of the problems with chunking, most are related to music and are related to music and thus melodic dictation.
Below I review the problems with chunking noted by \citet{cowanWorkingMemoryCapacity2005}, and how each confound would manifest itself in music related research.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \emph{Chunks may have a hierarchical organization.} Tonal music has historically been understood to be hierarchical \citep{krumhanslCognitiveFoundationsMusical2001, meyerEmotionMeaningMusic1956, schenkerFreieSatz1935} with the study of memory for tones being confounded by some pitches being understood by their relation to structurally more stable tones.
\item
  \emph{The working memory load may be reduced as working memory shifts between levels in hierarchy.} If an individual understands a chunk to be something such as a major triad, the load on working memory would be less since that information could be understood as a singular chunk.
\item
  \emph{Chunks may be the endpoints of a continuum of associations.} Given tonal music's sequential and statistical properties, two tones might be able to be loosely associated given a context that would make the tones fall between being identified as two separate tones and one distinct chunk.
\item
  \emph{Chunks may include asymmetrical information.} More tonal possibilities are possible from a stable note like tonic or dominant, whereas in a tonal context, a raised scale degree \#\(\hat{4}\) when understood in a functional context would be taken as having stricter transitional probabilities (\#\(\hat{4} \rightarrow \hat{5}\)).
\item
  \emph{There may be a complex network of associations.} If a set of pitches sounds like a similar set of pitches from long term memory, the incoming information cannot be understood as being separate units of working memory.
\item
  \emph{Chunks may increase in size rapidly over time.} Three tones that are seemingly unrelated when played sequentially. E4, G5, C5 might enter sensory perception as three distinct tones, but then be fused together if understood as one chunk, a first inversion major triad.
\item
  \emph{Information in working memory may benefit from rapid storage in long term memory.} Given the number of patterns that an individual learns and can understand, as soon as sensory information is fused, it could be encoded in long term memory. This is especially true if there is a salient feature in the incoming melodic information such as the immediate recognition of a mode or cadence.
\end{enumerate}

The points by Cowan are important to acknowledge in that it is not possible to directly lift work and paradigms from working memory capacity to work in music perception.
That said, the enormous number of theoretical frameworks put forward by the working memory literature when understood in conjunction with theories in music psychology, such as implicit statistical learning \citep{saffranStatisticalLearningTone1999}, can provide for new, fruitful theories.
Past researchers have noted the strength and predictive abilities from the working memory capacity literature as aiding research in music perception.
In ending his article positing a musical memory loop to be annexed to the Baddley and Hitch modular model of working memory, \citet{berzWorkingMemoryMusic1995} captures the power of this concept in the last sentence of his article and warns:

\begin{quote}
Individual differences portrayed in some music aptitude tests may {[}sic{]} represent not talent or musical intelligence but ability, reflecting differences in working memory capacity. (p.~362)
\end{quote}

Berz's assertion has not been exhaustively tested since first published in 1995, but the subject of music, memory, and cognitive abilities has been the focus of research for both both psychologists and musicologists alike.

\hypertarget{working-memory-capacity-and-music}{%
\subsubsection{Working Memory Capacity and Music}\label{working-memory-capacity-and-music}}

Of the papers in the music science literature that specifically investigate working memory, each uses different measures, but all tend to converge on two general findings.
The first is that there is some enhanced memory capabilities in individuals with musical training.
The second is that working memory capacity, however it is measured, often plays a significant role in musical tasks.
Evidence for the first point appears most convincingly in a recent meta analyses by Talamini and colleagues \citep{talaminiMusiciansHaveBetter2017}, who demonstrated via three separate meta-analyses, that musicians outperform their non-musical counterparts on tasks dealing with long-term memory, short-term memory, as well as working memory.
The authors also noted that the effects were the strongest in working memory tasks where the stimuli were tonal, which again suggests an advantage of exposure and understanding of the hierarchical organization of musical materials.
In this meta-analysis and that from others investigating music and cognitive ability, it is important to be reminded that the direction of causality from these studies still cannot be determined using these theoretical and statistical methodologies.
While it might seem that musical training tends to lead to these increases, it is also possible that higher functioning individuals will self select into musical activities.
Even if there is no selection bias in engaging with musical activity, it also remains a possibility that of the people who do engage with musical activity, the higher functioning individuals will be less likely to quit over a lifetime.

In terms of musical performance abilities, working memory capacity has also been shown to be a significant predictor.
Kopiez and Lee suggested that working memory capacity should contribute to sight reading tasks based on research where they found measures of working memory capacity, as measured by a matrix span task, to be significantly correlated with many of their measures hypothesized to be related to sight reading ability in pianists at lower difficulty grading \citep{kopiezDynamicModelSkills2006, kopiezGeneralModelSkills2008}.

Following up on this work on sight reading, Meinz and Hambrick \citep{meinzDeliberatePracticeNecessary2010} found working memory capacity, as measured by an operation span task, a reading span task, rotation span task, and a matrix span task was able to predict a small amount of variance \(R^2=.07 (0.07)\) above and beyond that of deliberate practice alone \(R^2=.45 (.44)\) in a sight-reading task.
More recently, two studies looking at specific sub-groups of musicians have shown working memory capacity to significantly contribute to models of performance on musical tasks related to novel stimuli.
\citet{wollnerAttentionalFlexibilityMemory2016} found that although they did not observe differences between pianists and conductors in measures of working memory capacity as measured via a set of span tasks, conductors showed superior performance in their attention flexibility.
Continuing this line of research \citet{nicholsScoreOneJazz2018}, used the same battery of working memory tasks and found that jazz musicians excelled over their classically trained counterparts in a task which required them to hear notes and reproduce them on the piano.
The authors also noted that of their working memory battery, based on standard operation span methods \citep{engleWorkingMemoryCapacity2002}, that the auditory dictation condition scored surprisingly low, and further research might consider work on dictation abilities.
Additionally, \citet{colleyWorkingMemoryAuditory2017} found working memory capacity, as measured by a backwards digit span and operation span, to be successful predictors in a tapping task requiring sensory motor prediction abilities.
As mentioned above, each of these tasks where working memory was a significant predictor of performance occurred where the task involved active engagement with novel musical material.

The growing evidence in this field suggests that having larger working memory capacity to be greatest in musically trained people and the effects are the most pronounced dealing with novel, tonal information.
Since all three of these factors are related to melodic dictation, it would seem sensible to continue to include these measures in tasks of musical perception and continue Berz's assertion that research in music perception could inadvertently be picking up on individual differences in working memory abilities.

\hypertarget{intelligence-and-music}{%
\subsubsection{Intelligence and Music}\label{intelligence-and-music}}

As discussed above, the idea of IQ or intelligence has a long and complex history.
When used as a predictor in statistical models, it often serves to predict traits that society values like longevity and general income, so given its ability to predict in more domain general settings, surveying literature where applicable to musical activity warrants attention.
Below I use the term intelligence as a catch all term to avoid the historical context of IQ, and to specify, where available, which measure was used.
Before surveying the literature, it is also worth noting that research on music and intelligence is not as developed as some of the larger studies that look at intelligence, which creates problems for both establishing causal directionality, as well as controlling for other factors like self-theories of ability, socioeconomic status, and personality \citep{mullensiefenInvestigatingImportanceSelftheories2015}.

As reviewed in \citet{schellenbergMusicNonmusicalAbilities2017}, both children and adults who engage in musical activity tend to score higher on general measures of intelligence than their non-musical peers \citep{gibsonEnhancedDivergentThinking2009, hilleAssociationsMusicEducation2011, schellenbergExaminingAssociationMusic2011, schellenbergMusicTrainingEmotion2012} with the duration of training sharing a relationship with the extent of the increases in IQ \citep{corrigallPredictingWhoTakes2015, degeMusicLessonsIntelligence2011, schellenbergLongtermPositiveAssociations2006}.
Though many of these studies are correlational, they have also made attempts to control for confounding variables like socio-economic status and parental involvement in out of school activities \citep{corrigallAssociationsLengthMusic2011, degeMusicLessonsIntelligence2011, schellenbergExaminingAssociationMusic2011, schellenbergMusicTrainingEmotion2012}.
Schellenberg notes the problem of smaller sample sizes in his review \citep{corrigallAssociationsLengthMusic2011, parbery-clarkMusicalExperienceAging2011, straitMusicalTrainingEarly2012} in that studies that are typically smaller do not reach statistical significance.
Schellenberg also references evidence that when professional musicians are matched with non-musicians from the general population, there do not seem to be these associations \citep{schellenbergLongtermPositiveAssociations2006}.
His review suggests the current state of the literature might be interpreted as higher functioning kids tend to gravitate towards music lessons, then subsequently persist with the lessons.
Additionally, Schellenberg remains skeptical of any sort of causal factors regarding increases in IQ \citep{francoisMusicTrainingDevelopment2013, morenoMusicalTrainingInfluences2009} noting methodological problems like how short exposure times were in studies claiming increases in effects, or researchers not holding pre-existing cognitive abilities constant \citep{mehrTwoRandomizedTrials2013}.
Continued work by Swaminithan, Schellenberg, and Khalil continue to support evidence for this selection bias in training resulting in higher cognitive abilities among musicians \citep{swaminathanRevisitingAssociationMusic2017}.
Under my taxonomy, the cognitive traits such as general intelligence and working memory are presumed to be stable over a lifetime.
I now turn to individual traits that are more malleable, those dubbed the environmental factors.

\hypertarget{environmental}{%
\subsection{Environmental}\label{environmental}}

Standing in contrast to factors that individuals do not have much control over such as the size of their working memory capacity or factors related to their general fluid intelligence, most of the factors music pedagogues believe contribute to one's ability to take melodic dictation are related to what I have put forward as environmental factors.
In fact, one of the tacit assumptions of any formal education revolves around the belief that with deliberate and attentive practice, an individual is able to move from novice status to some level of expertise in their chosen domain.
The idea that time invested results in beneficial returns is probably best formalized by work produced by \citet{ericssonRoleDeliberatePractice1993} that suggests that performance at more elite levels results from deliberate practice.

As noted in studies above, such as \citet{meinzDeliberatePracticeNecessary2010}, deliberate practice is able to explain variance in task performance, but other research suggests more variables are at play.
\citet{dettermanMoreComprehensiveTheory1999} propose that three factors, general intelligence, domain specific ability, and practice are the cornerstones of developing expertise in music.
The first of their three factors is not normally believed to be malleable, while the latter two are presumed to be plastic.
This reasoning has been explored by researchers such as \citet{ruthsatzBecomingExpertMusical2008}, who investigated these assertions and provided empirical evidence to support this notion using hierarchical multiple regression modelling and concluded that each of these variables does in fact contribute significantly to the target variable of musical performance.
Other researchers have since commented on these expertise models like \citet{mosingPracticeDoesNot2014} who have asserted that a genetic component, rather than those listed above best explain variance on musical ability.

One major problem interpreting literature like the studies mentioned above is the general lack of agreement on what constitutes musical behaviors.
At a very high level, many of the aforementioned studies take a parochial view of what it means to engage in musical activity, a problem which is only exacerbated by not having uniform psychometric measurements \citep{bakerExaminingMusicalSophistication2018a, talaminiMusiciansHaveBetter2017}.
Interpreting this data then becomes difficult as what it means to be proficient at a musical task is culturally dependent.
Investigating musical talent as if it were a universal is a problem well documented in both the ethnomusicological and music education literature \citep{blackingHowMusicalMan2000, murphyHowFarTests1999}.

\hypertarget{aural-training}{%
\subsubsection{Aural Training}\label{aural-training}}

In addition to individuals differing in their general musical abilities, individuals also differ in their abilities at the level of their aural skills.
The same problems that arise in operationalizing musicianship are apparent in defining aural skills.
Reviewing the literature, I operationalize aural skills to encompass the many skills often taught in music school, not restricting those skills to any particular sets of exercises.
Some researchers like \citet{chenetteReframingAuralSkills2019} have taken stricter definitions attempting to state only skills that engage working memory capacity as those that are truly aural, but this operationalization would limit this review's scope.

Though not as heavily researched in the past few decades \citep{furbyEffectsPeerTutoring2016}, there has been specific research looking at modeling how individuals perform in aural skills examinations.
\citet{harrisonEffectsMusicalAptitude1994} examined the effect of aural skills training on undergraduate students by creating a latent variable model investigating musical aptitude, academic ability, musical expertise, and motivation to study music in a sample of 142 undergraduate students and claimed to be able to explain 73\% of the variance in aural skills abilities using the variables measured.
Work from Colin Wright's dissertation incorporated a mixed methods approach investigating correlations between aural ability and their degree success as well as interviewing university students regarding the importance of aural skills education.
In his work he found a general positive correlation between aural ability and measures of degree success \citep{wrightInvestigatingAuralCase2016}.

While results are still mixed regarding how best to measure and assess this ability, the near ubiquity of aural skills education has resulted in many investigations of how people might improve their ability.
As noted in \citet{furbyEffectsPeerTutoring2016}, researchers in the past have suggested a variety of techniques for improving abilities in melodic dictation by isolating rhythm and melody \citep{bantonRoleVisualAuditory1995, blandSightSingingMelodic1984, rootMethodicalSightSingingLessons1931}, listening attentively to the melody before writing \citep{bantonRoleVisualAuditory1995}, recognizing patterns \citep{bantonRoleVisualAuditory1995, blandSightSingingMelodic1984, rootMethodicalSightSingingLessons1931} and silently vocalizing while dictating \citep{klonoskiImprovingDictationAuralSkills2006}.
Interpreting a clear best path forward from these studies again remains difficult due to the sheer amount of variables at play.

Often described as the other side of the same coin of melodic dictation, sight singing is an area of music pedagogy research that has received some attention, yet probably not the extent deserved given its prevalence in school of music curricula.
Recently, \citet{fournierCognitiveStrategiesSightsinging2017a} cataloged and categorized fourteen different strategies that students used when learning to sight read.
The authors organized their fourteen categories into four larger main categories and suggested that aural skills pedagogues should employ their framework in their aural skills pedagogy in order to better communicate effective sight singing strategies.

Similar to commentaries in literature on melodic dictation, \citet{fournierCognitiveStrategiesSightsinging2017a} also note a line of research that has documented that university students are often unprepared to sight-read single lines of music \citep{asmusMusicTeachingMusic2004, thompsonPitchInternalizationStrategies2003} even though it is, like dictation, thought of as a means for deeper musical understanding \citep{karpinskiAuralSkillsAcquisition2000, rogersTeachingApproachesMusic2004}.
\citet{fournierCognitiveStrategiesSightsinging2017a} also documented that sight-reading has been an active area of research because of the often reported relationship.
Performance on sight reading often predicts links between academic success in sight-singing and predictors such as entrance tests \citep{harrisonValidityMusicalAptitude1987}, academic ability, and musical experience \citep{harrisonEffectsMusicalAptitude1994}.

Taken as a whole, the research tends to suggest that learning to be a fluid and competent sight reader helps musicians hone their skills by bootstrapping other musical skills since those needed for sight-reading touch on many of the skills used in musical performance such as pattern matching and listening for small changes in intonation.
While the above literature suggests there are empirical grounds to consider these individual factors in predicting how well an individual will do in melodic dictation, these factors will invariably interact with the other half of the taxonomy: the musical factors.

\hypertarget{musical-factors}{%
\section{Musical Factors}\label{musical-factors}}

Transitioning to the other half of the taxonomy on Figure \ref{fig:taxonomy}, the other main source of variance in any study investigating melodic dictation is the effect of the melody itself.
I find it safe to assume that not all melodies are equally difficult to dictate and assert that variance in the difficulty of the melody can be partitioned between both structural and experimental aspects of a melody.
As noted above, there is not a strict delineation between these two categories since once could imagine manipulations in experimental parameters in order to result in a phenomenologically different experience of melody.
Questions of transformations of melodies and musical similarity have been addressed in other research \citep{cambouropoulosHowSimilarSimilar2009, wigginsModelsMusicalSimilarity2007} and are beyond the scope of this study.

\hypertarget{the-notes}{%
\subsection{``The Notes''}\label{the-notes}}

The assumption that a musical score is able to provide insights towards meaningful understanding is a core tenant of music theory and analysis.
Throughout the 20th Century, music theorists have almost exclusively relied on musical scores as their central point of reference in their work.
According to Clarke \citep{clarkeWaysListeningEcological2005}, this structuralist approach to music lays at the foundation of many academic discussions, possibly stemming from latent assumptions regarding absolutism in music.
General interest in structure has been a dominant part of the discourse as evidenced from the extensive lines of thought emanating from Heinrich Schenker \citep{schenkerFreieSatz1935, salzerStructuralHearingTonal1982, schachterSchenkerStudies2006, schenkerSchenkerStudies1990} and variations on linking what one might colloquially refer to as ``the notes'' to some sort of musical meaning is the lifeblood of music theory.
While issues surrounding ``the notes'' as they pertain to discourse have been central to large debates within the musicological community \citep{agawuHowWeGot2004, kermanContemplatingMusicChallenges1986}, tethering ``the notes'' to explain phenomenological listening experiences in music received much of its theoretical framework from the work of Leonard Meyer and assertions he put forward in \emph{Emotion and Meaning in Music} \citep{meyerEmotionMeaningMusic1956}.
In his text, Meyer posits that much of a listener's experience in music can be understood by considering a listener's expectations which are generated from the statistical properties of the music.

Research in Meyer's tradition inspired work investigating the perception of melodic structures via the work of Eugene Narmour \citep{narmourAnalysisCognitionBasic1990, narmourAnalysisCognitionMelodic1992}, Glenn Schellenberg \citep{schellenbergSimplifyingImplicationRealizationModel1997}, Elizabeth Hellmuth Margulis \citep{margulisModelMelodicExpectation2005}, and David Huron \citep{huronSweetAnticipation2006}.
Meyer has also been the cited source of inspiration for recent, successful implementations of models of human auditory cognition like that of Marcus Pearce's Information Dynamics of Music \citep{pearceConstructionEvaluationStatistical2005, pearceStatisticalLearningProbabilistic2018a}, which derives from information theoretic models of musical perception put forward by Ian Witten and Darrell Conklin \citep{conklinMultipleViewpointSystems1995}.

Though even prior to Meyer and Schenker, one of the earliest researchers that sought to make an explicit link between ``the notes'' and perception comes from outside the dominant academic musicological discourse.
The first study to examine the link between what might be understood as ``the notes'' and explicit memory was Otto Ortmann in 1933 \citep{ortmannTonalDeterminantsMelodic1933}.
Ortmann used a series of twenty five-note melodies in order to examine the effects of repetition, pitch direction, conjunct-disjunct motion (contour), interval size, order, and chord structure, all of which he deemed to be the determinants of an individual's ability to recall melodic material.
Though Ortmann did not use any statistical methods to model his data, he did assert that each of his determinants contributed to an individual's ability to recall musical material.
This work was extended by \citet{taylorStrategiesMemoryShort1983} who additionally incorporated using musical skill as a predictor and subsequently found evidence that these factors contributed to individual dictation abilities in a sample of 122 undergraduate students.

What Ortmann referred to as determinants are structural aspects of the melody that can then be mapped to some aspect of perception.
While Ortmann used the term determinants, for the rest of this study I instead adopt the term feature which better reflects current terminology used to talk about these aspects of a melody.
Given Ortmann's design of using isorhythmic five tone sequences, his detriments--- or features--- under my taxonomy from Figure \ref{fig:taxonomy} would generally include only structural aspects.
Were Ortmann to have increased the tempo of the tones he presented, change the timbre of their instrumentation, or maybe provide participants more attempts to give their responses, he would have then been adjusting what I am referring to as the experimental features.
In the section below, I first explore literature that set out to understand certain structural aspects of the musical side of my taxonomy, then begin to introduce studies that incorporate more parameters.

As with the above problems listed in attempting to measure latent psychological constructs, similar problems also arise in operationalizing many of the musical constructs in the experiments from above.
Unlike individual features, since musical scores can be digitized, attempting to create more objective measurements for musical features is more straightforward than that of measuring latent psychological variables.
One way to accomplish this is to use symbolic features of the melodies themselves as a variable to be measured.
Unfortunately, much of the work from computational musicology such as David Huron's Humdrum toolbox \citep{huronHumdrumToolkitReference1994} or Michael Cuthbert's music21 \citep{cuthbertMusic21ToolkitComputerAided2010} pre-dates some of the earlier experimental work I will discuss below, but as these computations are more straightforward than considering larger experimental designs, I begin with them here.
While I reserve a longer discussion on the histories of computational musicology for the computational chapter of this dissertation, relevant to this review are the additional ways it is now possible to abstract features from symbolic melodies beyond what was capable in studies such as \citet{ortmannTonalDeterminantsMelodic1933} and \citet{taylorStrategiesMemoryShort1983}.

\hypertarget{abstracted-features}{%
\subsection{Abstracted Features}\label{abstracted-features}}

An abstracted symbolic feature of a melody is an emergent properties of the melody that results from performing a calculation on the melody when digitized into discrete, computer readable tokens.
Abstracted symbolic features of melodies can largely be conceptualized as being static or dynamic.
Static features of melodies are obtained by summarizing some aspect of the melody as if it were to be experienced in suspended animation.
For example, a static feature of a melody might be the melody's range as calculated by the number of half steps from the lowest to the highest note or the number of notes in a melody.
Using static features helps quantify something that might be intuitive about a melody or piece of encoded music.
For example, David Huron's contour class used in a study investigating melodic arches \citep{huronMelodicArchWestern1996} using the Essen Folksong Collection \citep{schaffrathEssenFolkSong1995} can only be understood as a feature of the melody itself once the melody has been sounded and recalled would be a static feature of a melody.
Other examples include a melody's global note density, normalized pairwise variability index \citep{grabeDurationalVariabilitySpeech2002}, and a melody's tonalness as calculated by one of the various key profile algorithms \citep{krumhanslCognitiveFoundationsMusical2001, albrechtUseLargeCorpora2013}.
These measures are useful when describing melodies and are predictive of various behavioral phenomena as detailed below, but at this point it has not been well established to what degree these summary features can be directly and reliably mapped to aspects of human behavior.

The quintessential and most comprehensive toolbox example of this is Daniel MÃ¼llensiefen's Feature ANalysis Technology Accessing STatistics (In a Corpus) or FANTASTIC toolbox \citep{mullensiefenFantasticFeatureANalysis2009}.
FANTASTIC is software that is capable summarizing musical material for monophonic melodies.
In addition to computing 38 features such as contour variation, tonalnesss, note density, note length, and measures inspired by computational linguistics \citep{manningFoundationsStatisticalNatural1999} FANTASTIC is also able to calculate m-types (melodic-rhythmic motives) that are based on the frequency distributions of melodic segments in musical corpora.

Work using the FANTASTIC toolbox has been successful in predicting court case decisions \citep{mullensiefenCourtDecisionsMusic2009}, predicting chart successes of songs on the Beatles' album \emph{Revolver}, \citep{kopiezAufSucheNach2011}, memory for old and new melodies in signal detection experiments \citep{mullensiefenRoleFeaturesContext2014}, memory for ear worms \citep{jakubowskiDissectingEarwormMelodic2017, williamsonEarwormsThreeAngles2012}, memorability of pop music hook \citep{balenCorpusAnalyisTools2015}.
In experimental studies, FANTASTIC has also been used to determine item difficulty \citep{bakerPerceptionLeitmotivesRichard2017, harrisonModellingMelodicDiscrimination2016} and has even been the basis of the development of a computer assisted platform for studying memory for melodies \citep{rainsfordMUSOSMUsicSOftware2018}.

In addition to using summary based features on melodies, it is also possible to model the perception of musical materials by using a dynamic approach that is dependent on the unfolding of musical material.
First explored by Witten and Conklin \citep{conklinMultipleViewpointSystems1995}, and then implemented as a dynamic model of human auditory cognition in his doctoral dissertation, Marcus Pearce's Information Dynamics Of Melody (IDyOM) models musical expectancy using information theoretic concepts \citep{shannonMathematicalTheoryCommunication1948}.
The model takes an unsupervised machine learning approach and calculates the information content for musical events based on multiple pre-specified viewpoints \citep{pearceConstructionEvaluationStatistical2005}.
As a model, IDyOM has has been successful in modeling human responses to expectation, melodic boundary formation, and even measurements of cultural proximity \citep{pearceAuditoryExpectationInformation2012, pearceStatisticalLearningProbabilistic2018a}.
The domain general application of IDyOM has given credence to Meyer's assertion that the enculturation of musical styles stems from statistical exposure to musical genres and is somewhat reflective of the cognitive processes used in musical perception.
IDyOM has also been recently extended to look at expectation in polyphonic work \citep{sauvePredictionPolyphonyModelling2017} and expectations of harmony \citep{harrisonDissociatingSensoryCognitive2018}.

The advantage of using a dynamic approach, as opposed to a static one, is that a dynamic approach theoretically reflects real-time perception of music with the structural characteristics of the music mapping on to real human behavior since expectancy values are calculated for every musical event.
While employing this type of model does allow for calculations to be made for every musical event in question, the assumption also brings into question whether a computer model is able to calculate each musical event and be reflective of human cognition, and does that mean that the human perceptual system is also making on-the-fly probability calculations during perception?
This problem is worthy of mention as it currently exists in literature on implicit statistical learning \citep{perruchetImplicitLearningStatistical2006} and some researchers have put forward similarity based models that have claimed to explain processes attributed to statistical learning, but do not depend on the statistical learning mechanism \citep{jamiesonApplyingExemplarModel2009}.

\hypertarget{ecological-experiments}{%
\subsubsection{Ecological Experiments}\label{ecological-experiments}}

While the field of computational musicology has built models for quantifying these perceptual aspects of melody, work that is generally more aligned with research in music education takes a more ecological approach to inspecting how musical features affect perception.
For example, Long found that length, tonal structure, contour, and individual traits all contribute to performance on melodic dictation examinations and found that structure and tonalness have significant, albeit small predictive powers in modeling \citep{longRelationshipsPitchMemory1977}.
One problem with studies such as \citet{longRelationshipsPitchMemory1977} is that studies like Long's make conspicuous methodological decisions such as eliminating individuals from their sample who met an \emph{a priori} criteria for bad singers.
Not only does this reduce the spectrum of ability levels (assuming that singing ability correlates with dictation ability, a finding which has since been established \citep{norrisRelationshipSightSinging2003}), but is additionally flawed in that it is at odds both with the intuition that an individual's singing ability cannot be taken as a direct representation of their mental image of the melody.
In fact, more recent research might suggest that singing ability might instead relate to motor control ability over the vocal tract rather than pitch imagery abilities \citep{pfordresherPoorPitchSingingAbsence2007}.

Other researchers have also put forward parameters thought to contribute like tempo \citep{hofstetterComputerBaesedRecognitionPerceptual1981}, tonality \citep{dowlingScaleContourTwo1978, longRelationshipsPitchMemory1977, pembrookInterferenceTranscriptionProcess1986, ouraMemoryMelodiesSubjects1988}, interval motion \citep{ortmannTonalDeterminantsMelodic1933, pembrookInterferenceTranscriptionProcess1986}, length of melody \citep{longRelationshipsPitchMemory1977, pembrookInterferenceTranscriptionProcess1986}, number of presentations \citep{hofstetterComputerBaesedRecognitionPerceptual1981, pembrookInterferenceTranscriptionProcess1986},
context of presentation \citep{schellenbergEffectTonalRhythmicContext1985},
the background of the listener \citep{longRelationshipsPitchMemory1977, ouraMemoryMelodiesSubjects1988, schellenbergEffectTonalRhythmicContext1985, taylorStrategiesMemoryShort1983} as well as familiarity with a musical style \citep{schellenbergEffectTonalRhythmicContext1985}.
Again we have a listing of studies that consider both structural and experimental aspects of the taxonomy.

\citet{pembrookInterferenceTranscriptionProcess1986} provides an extensive detailing of a systematic study to melodic dictation where the authors used tonality, melody length, and type of motion as variables in their experiment.
They additionally restricted their experimental melodies to those that were singable.
The authors found all three variables to be significant predictors with tonality explaining 13\% of the variance, length explaining 3\% of the variance and type of motion explaining 1\% of the variance.
The paper also claimed that people on average can hear and remember 10-16 notes with the quarter note set to 90 beats per minute.

Given the lack of consistent methodologies in administration and scoring of these experiments, it becomes difficult to find ways to generalize basic findings like expected effect sizes--- especially when the original materials and data have not been recorded--- but there are often interesting theoretical insights to be gleaned.
For example, \citet{ouraConstructingRepresentationMelody1991a} used a sample of eight people to suggest that when taking melodic dictation, individuals use a system of pattern matching that interfaces with their long term memory in order to complete dictation tasks.
While this paper does not bring with it exhaustive evidence supporting this claim, the idea is explored in detail in final chapter when the idea of pattern matching is used in conjunction with Cowan's Embedded Process model of working memory \citep{cowanEvolvingConceptionsMemory1988}.

More recently, the music education community has also begun to do research around melodic dictation using both qualitative and quantitative methodologies.
\citet{paneyTeachingMelodicDictation2014} interviewed high school teachers on methods they used to teach melodic dictation and among more general findings on teaching methods reported a general awareness and concern among pedagogues regarding the ``psychological barriers inherent in learning aural skills'', as well as a general positive disposition to the use of standardized tests used in melodic dictation.
\citet{gillespieMelodicDictationScoring2001} surveyed over 40 individual aural skill instructors and reported large discrepancies in how aural skills pedagogues graded and gave feedback on students' melodic dictations.
Other work by \citet{pembrookSendHelpAural1990} surveyed various methodologies used by instructors in aural skills settings and reported inconsistencies in grading practice.
Some of these studies considered aural skills as a totality like \citet{norrisRelationshipSightSinging2003} who provided quantitative evidence to suggest most aural skills pedagogue's intuition that there is some sort of relationship between melodic dictation and sight singing.
Looking at the notorious subset of students with absolute pitch (AP), \citet{dooleyAbsolutePitchCorrelates2010} provided empirical evidence that students with AP tend to outperform their non-AP colleagues in tests of dictation.

Continuing to explore the pedagogical literature, Nathan Buonviri and colleagues have also made melodic dictation a central focus of recent work.
Using qualitative methods, \citet{buonviriEffectsMusicNotation2015} interviewed six sophomore music majors in order to find successful strategies that students engaged in when completing melodic dictations and found evidence to suggest that successful students engage in highly concentrated mental choreography when completing melodic dictations.
\citet{buonviriMelodicDictationInstruction2015} found that having students sing a preparatory singing pattern after hearing the target melody, essentially a distraction task, hindered performance on melodic dictation.
\citet{buonviriEffectsMusicNotation2015} found no effects of test presentation format (visual versus aural-visual) using a melodic memory paradigm and more work by \citet{buonviriEffectsTwoListening2017} reported no significant advantage to listening strategies while partaking in a melodic dictation test.

Not specific to computational musicology or that of the music education literature, other research from music perception has also claimed other experimental features might play a role in dictation.
For example, a series of papers by Michael W. Weiss has found a general timbral advantage of voice in memory recall tasks \citep{weissRapidCommunicationPianists2015, weissAbilityProcessMusical2019}, even finding the effect in amusics.
Vocal timbral perception presumably would then have an effect in the recall of music in dictation settings, but evidence supporting other surface features in memory processes has not been as explored \citep{schellenbergMemorySurfaceFeatures2014}.

As documented in this review of the literature on issues that contribute to an individual's ability to take melodic dictation, the problem is complex.
Not only are there difficulties in finding adequate measures of latent psychological constructs assumed to exist and contribute like working memory capacity and musical training, but additionally the amount of musical variables at play that inevitably interact with one another is overwhelming.

Given all the variables that are at play, what then is the best way forward in understanding the processes underlying melodic dictation?
In my opinion, the path forward to understanding relies on adopting a polymorphic view of musical abilities for future modeling.

\hypertarget{polymorphism-of-ability}{%
\section{Polymorphism of Ability}\label{polymorphism-of-ability}}

Given the current state of cognitive psychology and psychometrics, as well as advances in computational musicology, the possibilities for now operationalizing and then modeling aspects of melodic dictation are as advanced as they have ever been.
The research community can now operationalize every factor that is thought to contribute to this process and has literature to support the recording of almost any variable.
This includes concepts from musicianship, to features of a melody, and even unitless measures associated with an individual's working memory capacity.

While this is certainly possible to do, continuing in this manner of picking variables deemed relevant from such an expansive catalog of parameters will only obfuscate further research.
A clearer path forward is needed that reduces the signal to noise ratio in this research.
After reviewing this literature, below I list my recommendations for answering this problem.

One of the most important changes to future studies on melodic dictation need avoid the use of latent variables as predictors in statistical models.
While abstract concepts like intelligence and musical training are helpful concepts for explaining the variance in responses in aural skills settings, using such abstracted variables describe, but do not explain the causal mechanisms underlying this process.

The most illustrative example of this comes from the above study by \citet{harrisonEffectsMusicalAptitude1994} who created a latent variable model of aural skills that was able to predict 74\% of the variance in aural skills performance.
This latent trait that the authors created is helpful in explaining the patterns of covariance in data, but this would be to reify a statistical abstraction and assume a stance of ontological realism as noted before \citep{borsboomTheoreticalStatusLatent2003}.
The idea of statistical reification has been critiqued outside of music \citep{gouldMismeasureMan1996, kovacsProcessOverlapTheory2016} and additionally has served as the basis for an argument within music \citep{bakerExaminingMusicalSophistication2018a}.

The same arguments put forward in this literature are also relevant in research in aural skills.
In order to have a complete, causal model of the processes underlying melodic dictation, it is important to understand melodic dictation as a set of musical abilities that are related to other musical abilities, though may not be unified as a monolithic whole form which individuals draw from in order to execute musical tasks such as melodic dictation.
This idea is not new, even in music psychology, as the past two decades have seen calls for a more polymorphic definition of musical ability \citep{levitinWhatDoesIt2012, peretzModularityMusicProcessing2003} whose modeling will require more concrete ways of defining underlying processes rather than correlating variable together that are helpful at prediction without explaining the process.
Using a polymorphic view of musical abilities coupled with a theoretical framework like Karpinski's will allow for a clearer understanding of the many variables at play during this process.

\hypertarget{conclusions}{%
\section{Conclusions}\label{conclusions}}

In this chapter, I first described melodic dictation using Karpinski's model of melodic dictation.
Using his didactic model as a point of departure, I critique what this model does not consider and then put forward a taxonomy of features meant to encompass what this model lacks.
I suggested there are both individual as well as musical features that need to be understood in order to have a comprehensive understanding of melodic dictation.
Of the two sets of features, individual features can be either cognitive or environmental and musical features can be either structural or experimental.
This taxonomy does not consist of exclusive categories and certainly permits interactions between any and all of the levels.
Using this taxonomy as a guide, I then surveyed relevant literature in order to discuss how research might effectively quantify each parameter of relevance.
Finally, I asserted that in order to provide a more cohesive research program going forward, research on melodic dictation should adopt a polymorphic view of musicianship in line with calls in the past to move away from high level modeling and focus as much as possible on the processes deemed relevant in melodic dictation.
The rest of this dissertation will synthesize these areas and put forth novel research contributing to the modeling and subsequent understanding of melodic dictation.

\let\cleardoublepage\clearpage

\hypertarget{individual-differences}{%
\chapter{Individual Differences}\label{individual-differences}}

\hypertarget{rationale-1}{%
\section{Rationale}\label{rationale-1}}

The first two steps of Gary Karpinski's model of melodic dictation \citep{karpinskiAuralSkillsAcquisition2000, karpinskiModelMusicPerception1990} rely exclusively on the mental representation of melodic information.
Karpinski conceptualizes the first stage of hearing as involving the physical motions of the tympanic membrane, as well as the listener's attention to the musical stimulus.
This stage is distinguished from that of short-term melodic memory, which refers to the amount of melodic information that can be represented in conscious awareness.
Given that neither stage of the first two steps of Karpinski's model requires any sort of musical expertise, every individual with normal hearing and cognition should be able to partake in the first two steps of melodic dictation.
The ability to hear, then remember musical information is where all students of melodic dictation are presumed to begin their aural skills education.
From this baseline, students receive explicit education in music theory and aural skills to develop the ability to link what they hear to what can then be musically understood and consequently notated.

While the majority of beginning students of melodic dictation are assumed to start at the same level of ability, cognitive psychology research suggests that individual differences in cognitive ability exist and must be accounted for from a psychological and pedagogical perspective \citep{cowanWorkingMemoryCapacity2005, ritchieIntelligenceAllThat2015}.
In order to fully capture the diversity of listening abilities among students of melodic dictation, a complete account of melodic dictation must include individual differences in ability.
Understanding how differences at the individual level vary will also help pedagogues know what can be reasonably expected of students with different experiences and abilities.

Attempting to investigate all four parts of melodic dictation from hearing, to short-term melodic memory, to musical understanding, to notation is cumbersome from a theoretical perspective and practically infeasible due to the amount of variables that contribute to this process.
In order to obtain a clearer picture of what mechanisms contribute to this process, these steps must be be investigated in turn.
This chapter investigates the first two steps of the Karpinski model of melodic dictation \citep{karpinskiAuralSkillsAcquisition2000, karpinskiModelMusicPerception1990} with an experiment examining individual factors that contribute to musical memory that do not depend on knowledge of Western musical notation.
Understanding which, if any, individual factors play a role in this process, will inform what can be reasonably expected of individuals when other musical variables are then introduced.

\hypertarget{individual-differences-1}{%
\section{Individual Differences}\label{individual-differences-1}}

\hypertarget{improving-musical-memory}{%
\subsection{Improving Musical Memory}\label{improving-musical-memory}}

Most aural skills pedagogy assumes students begin with approximately the same baseline listening and dictation abilities.
Assuming this baseline allows teachers to cover requisite information systematically and ensures that students are given the the same tools to enable their success in the classroom.
This assumption of similar baseline of abilities is implicit in the Karpinski model of melodic dictation.
The model provides a framework of mental choreography students are encouraged to build upon that is agnostic to individual differences; Karpinski's model assumes that all individuals, regardless of their background, will engage in the same process.
As students gain more knowledge in music theory, they build their musical understanding which in turn enables them to recognize more of the auditory scene they are focusing on.
In addition to learning explicit knowledge that facilitates their musical understanding, Karpinski suggests there are two other skills that students can develop in order to improve their short-term musical memory: extractive listening and chunking.
In Karpinski's own words: ``Only one or both strategies can extend the capacity of short-term musical memory: (1) extractive listening and (2) chunking (p.~71)''.

Karpinski defines extractive listening as ``a combination of focused attention and selective memorization'' (p.~70).
Extractive listening requires students to be able to focus on the material they will be mentally representing and tune out other sources of stimulation that might distract the student.
In order to improve this ability, Karpinski suggests practicing listening to melodies and having students practice directing their attention to pre-determined set sequences of notes.
Students should slowly work towards being able to auralize the melody with other musical information still sounding.
Karpinski claims that honing one's attention via this type of progressive practice will not only improve student's ability to dictate melodies, but will also help them with a host of other musical activity.
Further, Timothy Chenette has since proposed similar types of progressive loading aural exercises by co-opting standard cognitive tasks used in working memory paradigms \citep{chenetteReframingAuralSkills2019} in order to help students improve their ability to focus in the aural skills classroom.

After students master the ability to selectively hear and retain a portion of a melody, the other way in which they can improve their dictation abilities is via chunking.
Chunking is a listener's ability to group smaller units of musical material into a larger group.
The idea of chunking derives from earlier work from Gestalt psychologists and was one of the initial mechanisms proposed by \citet{millerMagicalNumberSeven1956} able to extend the finite window of memory.
The general idea is that if a collection of notes can be identified as its own discrete entity--- such as a descending major triad in first inversion--- the listener will only have to remember that one structure, rather than its component parts.
As discussed in the previous chapter in \protect\hyperlink{working-memory-and-melodic-dictation}{Working Memory and Melodic Dictation}, music's inherently sequential nature affords it many opportunities to find repeated patterns which can be labeled, musically understood, and thus chunked.
While stimuli that are inherently sequential are problematic for psychologists investigating capacity limits of working memory capacity \citep{cowanWorkingMemoryCapacity2005}, students are expected to use chunking to their advantage in order to become more adept listeners.
As students learn to chunk more efficiently, they are able to process more musical information in their short-term musical memory.
With the development of both skills, students are presumed to increase their musical memory and ultimately improve their melodic dictation abilities.
But what evidence supports the assertion that individuals are able to improve on their ability to both learn and remember melodies?

\hypertarget{memory-for-melodies}{%
\subsection{Memory for Melodies}\label{memory-for-melodies}}

Research findings from the memory for melody literature are mixed when considering how people vary in their ability to remember musical material \citep{halpernMemoryMelodies2010}.
For example, no effect of an individual's musical training was found by \citet{mcauleyPlayItAgain2004} in a paradigm where both musically trained and non-musically trained individuals were presented with melodies using a recognition paradigm task with melodies over the course of two days.
In a musical recognition task, \citet{korenmanRoleFamiliarityEpisodic2004} found no effect of musicianship on memory.
Using a recognition paradigm, \citet{munganLevelsofProcessingEffectsRemember2011} found an effect of musical training on melodic memory, but the significant effect reported was not found in correctly identifying melodies, but rather in correctly identifying melodies that they had not heard before.
\citet{mullensiefenRoleFeaturesContext2014} reported no effects of musical training on their recognition paradigm experiment.
They however did not include any expert participants in their sample and the focus of this particular study was to look at structural features of the melody, rather than individual level features.
Additionally, other studies have also found that musical expertise is not a successful predictor of melodic recognition \citep{demorestLostTranslationEnculturation2008, halpernAgingExperienceRecognition1995}.
As with much of the music psychology literature, one of the reasons that these studies may have not found a memory advantage for the more musically trained is that how musical training is measured varies widely from study to study \citep{talaminiMusiciansHaveBetter2017}.
This inability to measure musical exposure additionally complicates controlling for the amount of variability of what might drive the memory effects in the models of musical memory.
When measured continuously using paradigms that require immediate recall and judgment, musical training does often predict memory for musical materials.

Using a stepwise modeling procedure, \citet{harrisonModellingMelodicDiscrimination2016} consistently found evidence that musical training was a significant predictor of ability to perform well on a melodic discrimination task when developing an item response theory based test of melodic memory.
Using regression modeling, Harrison et. al reported to be able to explain a large amount of the variance (\(R^2 = 0.46\)) when reporting response variability in a melodic discrimination task \citep{harrisonApplyingModernPsychometric2017a} when measuring musical training via the Goldsmiths Musical Sophistication Index \citep{mullensiefenMusicalityNonMusiciansIndex2014}.
\citet{bakerPerceptionLeitmotivesRichard2017} found musical training, when measured continuously, was able to be a significant predictor of successful identification of musical material using a exposure-recall paradigm within a set of other predictor variables including participant's familiarity with the music and item level predictors.

Even despite mixed evidence suggesting different effects of musical training on an individual's ability to remember melodies, it is important to note that these studies do not specifically deal with melodic dictation, and thus cannot be used as a perfect comparison for a number of reasons.
The first is that melodic dictation is a much more complicated process that not only involves hearing a melody after a few iterations, but also its notation.
Seeing as students need to notate their melodies, which again is dependent on their knowledge of Western musical notation, melodic dictation is secondly a more cognitively demanding process than the previously mentioned studies on memory for melody which often only require a simple discrimination.

\hypertarget{musicians-cognitive-advantage}{%
\subsection{Musician's Cognitive Advantage}\label{musicians-cognitive-advantage}}

While the above memory for melodies literature is mixed regarding the musician's advantage in memory for melodic material, there is research from cognitive psychology to support the evidence of an advantage of musical training in perceptual tests.
Some researchers suggest that musicians have better cognitive abilities on a more domain general level, which could lead to better performance and explains differences in performance.
Work as reviewed in \citet{schellenbergMusicNonmusicalAbilities2017} investigating the relationship between musical training and general intelligence suggests that both children and adults who engage in musical activity tend to score higher on general measures of intelligence than their non-musical peers \citep{gibsonEnhancedDivergentThinking2009, hilleAssociationsMusicEducation2011, schellenbergExaminingAssociationMusic2011, schellenbergMusicTrainingEmotion2012}.
Importantly, this association between intelligence and musical training comes with a correlation between duration of musical training and the extent of the increases in intelligence \citep{corrigallMusicTrainingCognition2013, corrigallPredictingWhoTakes2015, degeMusicLessonsIntelligence2011, schellenbergLongtermPositiveAssociations2006}.
While many of these studies are correlation, other researchers have further investigated this relationship in experimental settings attempting to control for confounding variables like socio-economic status and parental involvement in out-of-school activities \citep{corrigallMusicTrainingCognition2013, degeMusicLessonsIntelligence2011, schellenbergExaminingAssociationMusic2011, schellenbergLongtermPositiveAssociations2006, schellenbergMusicTrainingEmotion2012}, but findings have been mixed.

Schellenberg \citep{schellenbergMusicNonmusicalAbilities2017} notes that in many of these studies there is a problem of too small of a sample size, in his review \citep{corrigallAssociationsLengthMusic2011, parbery-clarkMusicalExperienceAging2011, straitMusicalTrainingEarly2012} in that studies that are typically smaller might be underpowered to detect any effects.
Also referenced in Schellenberg's review is evidence that when professional musicians are matched with non-musicians from the general population, these associations are non-existent \citep{schellenbergMusicTrainingSpeech2015}.
Interpreting the current literature, Schellenberg puts forward the hypothesis that higher functioning children might self-select into music lessons and tend to stay in lessons longer which leads to the observed differences in intelligence.
Additionally, Schellenberg remains skeptical of any sort of causal factors regarding increases in IQ \citep{francoisMusicTrainingDevelopment2013, morenoMusicalTrainingInfluences2009} noting methodological problems such as short exposure times or researchers not holding pre-existing cognitive abilities constant \citep{mehrTwoRandomizedTrials2013}.

In addition to general intelligence, another cognitive ability where musicians tend to exhibit superior performance is that of memory.
\citet{talaminiMusiciansHaveBetter2017}'s meta-analysis investigating musical training and memory found not only a general advantage for musicians, but noted that musicians tended to perform better on memory tasks especially in cases where stimuli were short and tonal.
This musician advantage could derive from a musician's ability to chunk information more effectively based on past exposure via implicit learning practices \citep{ettlingerImplicitMemoryMusic2011, rohrmeierImplicitLearningAcquisition2012}.
This difference also might reflect the above mentioned self-selection of higher functioning individuals to partake in music, which then explains the differences in memory.

As noted above, much of the research at this point still focuses on higher level relationships and is progressively being improved upon by agreeing on how to measure what is actually driving these effects.
Until more concrete theories emerge that link specific musical traits to music ability, music psychology will not be able to put forward clearer models of causal effects \citep{bakerExaminingMusicalSophistication2018a}.

\hypertarget{relationship-established}{%
\subsection{Relationship Established}\label{relationship-established}}

Regardless of the direction of causality, the evidence discussed suggests that there is a relationship between musical training and cognitive ability.
Clearly cognitive ability is at play in many tasks of perception and production.
Presumably these abilities will interact with other variables of interest such as musical training as theorized by the researchers previously mentioned.
Even in studies outside of music, domain general cognitive abilities have been shown to be predictive above and beyond domain specific expertise.
In reviewing the current literature, \citet{hambrickDomainGeneralModelsExpertise2019} reiterate that while there is some evidence of the time in many domain specific areas like chess, games, and music, the current state of the literature is not definitive enough to explain exactly how this phenomena works on a global level.

Though of all the studies mentioned thus far, one cognitive ability deserving of special attention is that of working memory.
As noted by \citet{berzWorkingMemoryMusic1995}, many tests of memory--- such as the aforementioned tests---require the encoding and active manipulation of musical material.
In his 1995 article, Berz draws important parallels between working memory systems and music tests and postulated new loop specifically for musical information.

For example, \citet{meinzDeliberatePracticeNecessary2010} found working memory to be predictive of performance in a sight reading task above and beyond that of deliberate practice.
Work by Kopiez \citep{kopiezDynamicModelSkills2006, kopiezGeneralModelSkills2008} has additionally linked the importance of working memory to performance on sight reading tasks.
In multiple studies, Andrea Halpern and colleagues have also shown measures of working memory to be linked to performance in musical production tasks \citep{halpernEffectsTimbreTempo2008, nicholsScoreOneJazz2018} and have even interpreted these findings in terms of Berz's memory loop.
Other work by \citet{harrisonModellingMelodicDiscrimination2016} has also made important links to an individual's ability to remember and recall musical information and working memory.
Harrison and colleagues put forward a cognitive model based on research in working memory that predicted which features of a melody--- based on theoretical considerations from working memory--- would be best at predicting behavioral performance.
They proposed that perceptual encoding, memory retention, similarity comparison, and decision-making could be used to contextualize differences in their memory recognition paradigm.
While they did find evidence to support this framework, they did not take any domain general measures of working memory capacity and thus were unable to conclude if domain general processes were able to better explain their data than using individual level musical, domain specific predictors.
Presumably any measure of musical perception ability should be able to explain above and beyond that of baseline cognitive abilities with a more established theoretical framework.

Additionally, \citet{okadaIndividualDifferencesMusical2018} used a latent variable approach where they investigated executive function in a sample of 161 university students.
Using Miyake's conception of executive function \citep{miyakeNatureOrganizationIndividual2012, miyakeUnityDiversityExecutive2000} and mixed effects modeling, Okada and Slevc found an effect of musical training as measured with the Goldsmiths Musical Sophistication Index (Gold-MSI) on the updating component of the executive functioning model, a construct often interpreted as similar to working memory capacity.
Okada and Slevc did not however link performance on their executive functioning tasks to an objective measure of musical performance implemented by the Gold-MSI.

\hypertarget{dictation-without-dictation}{%
\subsection{Dictation Without Dictation}\label{dictation-without-dictation}}

Given the complex network of variables at play, in order to understand how these individual factors affect the first two steps of melodic dictation, a multivariate approach is needed.
In order to investigate the effects of individual factors on baseline, I must first assume that using a melodic discrimination paradigm can be used as a proxy for the first two steps of the Karpinski model of melodic dictation.
I argue that because melodic discrimination paradigms require perceptual encoding, memory retention, and two other cognitive manipulations of similarity comparison and decision making as argued by \citet{harrisonModellingMelodicDiscrimination2016}, this paradigms does in fact resemble the first two steps of the Karpinski model.
Karpinski's hearing and short-term musical memory could just as easily be described as perceptual encoding and memory retention.
Additionally, the requirement to execute a decision while representing musical information in memory--- Harrison and MÃ¼llensiefen's similarity comparison and decision making--- can be mapped on to later stages of Karpinski's model of musical understanding, and subsequently notation.

One of the most complete suites of measuring traits associated with musical behavior that additionally employs both objective and subjective measures of musical sophistication is the Goldsmiths Musical Sophistication Index \citep{mullensiefenMusicalityNonMusiciansIndex2014}.
The Gold-MSI has a self-report questionnaire as well as two tests of objective ability and a timbre identification task.
One of the tests employs a beat detection paradigm, the other is a melodic discrimination paradigm.
Since both measures mirror tasks used in the aural skills classroom and the two are purported to measure different constructs, both will be used in this study.
Since its initial publication, adaptive short forms of the tests have been developed using item response theory \citep{harrisonApplyingModernPsychometric2017a}.
These tests were not available for use at the time of this study's data collection.

Assuming that a melodic discrimination task can stand in for the first two steps of the Karpinski model, I can model the relationships between performance on this musical memory task with individual level variables using structural equation modeling.
By doing this I can examine the extent to which factors contribute to the first two steps of melodic dictation.

\hypertarget{cognitive-measures-of-interest}{%
\subsection{Cognitive Measures of Interest}\label{cognitive-measures-of-interest}}

Having previously established that many tests of musical ability and aptitude may in fact be tests of working memory \citep{berzWorkingMemoryMusic1995}, one factor not yet accounted for in the memory for melodies literature is a domain general measure of working memory.
If working memory is conceptualized using Cowan's model of working memory as the window of attention \citep{cowanMagicalMysteryFour2010}, measuring working memory would need to be operationalized using a task that implements both the retention and manipulation of information in memory.
This is commonly done with complex span tasks \citep{unsworthAutomatedVersionOperation2005}.
Complex span tasks, unlike simple span tasks such as the \emph{n-back} paradigms, require both the retention and manipulation of items in memory and thus better reflect theoretically appropriate model of working memory \citep{cowanWorkingMemoryCapacity2005}.

Additionally, since general intelligence is often predictive of performance on a host of cognitive tasks such as educational success, income, and even life expectancy \citep{ritchieIntelligenceAllThat2015} and has been theoretically related to working memory \citep{kovacsProcessOverlapTheory2016}, this measure should also be accounted for when investigating individual features that contribute to the first two steps of melodic dictation using a standard paradigms of intelligence testing \citep{ravenManualRavenProgressive1994, thurstonePrimaryMentalAbilities1938}.
Finally, in response to claims made by \citet{okadaIndividualDifferencesMusical2018}, having to need to account for specific covariates, this study also will track socioeconomic status and degree of education, variables used in previous music psychology research \citep{corrigallMusicTrainingCognition2013, swaminathanRevisitingAssociationMusic2017}.

\hypertarget{structural-equation-modeling}{%
\subsection{Structural Equation Modeling}\label{structural-equation-modeling}}

Given the complex nature being investigated and the theoretical concepts at play such as working memory, general fluid intelligence, and musical sophistication conceptualized as a latent variable, it follows that the most appropriate method of parsing out the variance in this covariance structure would be to use some form of structural equation modeling \citep{beaujeanLatentVariableModeling2014}.
Structural equation modeling uses latent variables--- theoretical constructs thought to exist, yet are not possible to measure directly--- by taking advantages of algebraic systems originally developed by Sewall Wright \citep{wrightMethodPathCoefficents1934}.
When used under the right conditions, the technique is powerful enough to determine causal mechanisms in closed systems \citep{pearlBookWhyNew2018}, but this is not the case in this analysis.

\hypertarget{hypotheses}{%
\subsection{Hypotheses}\label{hypotheses}}

If I then assume that a same-different melodic memory paradigm is a stable proxy for the first two steps of Karpinski's model of melodic dictation, then data generated from both objective tests of the Goldsmiths' Musical Sophistication Index can serve as proxy for this measure of interest.
In this analyses, I will use a series of structural equation models in order to investigate how various individual factors contribute to an individual's memory for melody.
Following a step-wise procedure, these sets of analyses will provide a way to investigate what individual factors need to be accounted for in future research.

Given a robust instrument for measuring musicality, and two well established cognitive measures as specifically defined below, this analysis seeks to investigate the degree to which these individual level variables are predictive of a task that is proxy to the first two steps of melodic dictation.
If a large proportion of the variance of musical memory can be attributed to training, then variables related to the Goldsmiths Musical Sophistication Index should be most predictive with the highest path coefficients and lead to the best model fit.
If instead cognitive factors do play a role, this should be evident in the path coefficients.

\hypertarget{overview-of-experiment}{%
\section{Overview of Experiment}\label{overview-of-experiment}}

\hypertarget{participants}{%
\subsection{Participants}\label{participants}}

Two hundred fifty-four students enrolled at Louisiana State University completed the study.
Students were mainly recruited in the Department of Psychology and the School of Music.
The criteria for inclusion in the analysis were no self-reported hearing loss, not actively taking medication that would alter cognitive performance, and the removal of any univariate outliers (defined as individuals whose performance on any task was greater than 3 standard deviations from the mean score of that task).
Using these criteria, eight participants were not eligible due to self reporting hearing loss, one participant was removed for age, and six participants were eliminated as univariate outliers due to performance on one or more of the tasks of working memory capacity.
Thus, 239 participants met the criteria for inclusion.
The eligible participants were between the ages of 17 and 43 (M = 19.72, SD = 2.74; 148 females).
Participants volunteered, received course credit, or were paid \$20.

\hypertarget{materials}{%
\subsection{Materials}\label{materials}}

\hypertarget{cognitive-measures}{%
\subsubsection{Cognitive Measures}\label{cognitive-measures}}

All variables used for modeling approximated normal distributions.
Processing errors for each task were positively skewed for the complex span tasks similar to \citet{unsworthComplexWorkingMemory2009}.
Positive and significant correlations were found between recall scores on the three tasks measuring working memory capacity (WMC) and the two measuring general fluid intelligence (Gf).
The WMC recall scores negatively correlated with the reported number of errors in each task, suggesting that rehearsal processes were effectively limited by the processing tasks \citep{unsworthComplexWorkingMemory2009}.

\hypertarget{goldsmiths-musical-sophistication-index-self-report-gold-msi}{%
\subsubsection{Goldsmiths Musical Sophistication Index Self Report (Gold-MSI)}\label{goldsmiths-musical-sophistication-index-self-report-gold-msi}}

Participants completed a 38-item self-report inventory and questions consisted of free response answers or choosing a
selection on a likert scale that ranged from 1-7. \citep{mullensiefenMusicalityNonMusiciansIndex2014}.
The complete survey with all questions used can be found at goo.gl/dqtSaB.

\hypertarget{tone-span-tspan}{%
\subsubsection{Tone Span (TSPAN)}\label{tone-span-tspan}}

Participants completed a two-step math operation and then tried to remember three different tones in an alternating sequence (based upon \citet{unsworthAutomatedVersionOperation2005}).
The three tones were modeled after \citet{liEstimatingWorkingMemory2013} using frequencies outside of the equal tempered system (200Hz, 375Hz, 702Hz).
The same math operation procedure as OSPAN was used.
The tones was presented aurally for 1000ms after each math operation.
During tone recall, participants were presented three different options, H M and L (High, Medium, and Low), each with its own check box.
Tones were recalled in serial order by clicking on each tone's box in the appropriate order.
Tone recall was untimed.
Participants were provided practice trials and similar to OSPAN, the test procedure included three trials of each list length (3-7 tones), totaling 75 letters and 75 math operations.

\hypertarget{operation-span-ospan}{%
\subsubsection{Operation Span (OSPAN)}\label{operation-span-ospan}}

Participants completed a two-step math operation and then tried to remember a letter (F, H, J, K, L, N, P, Q, R, S, T, or
Y) in an alternating sequence \citep{unsworthAutomatedVersionOperation2005}.
The same math operation procedure as TSPAN was used.
The letter was presented visually for 1000ms after each math
operation.
During letter recall, participants saw a 4 x 3 matrix of all possible letters, each with its own check box.
Letters were recalled in serial order by clicking on each letter's box in the appropriate order.
Letter recall was untimed.
Participants were provided practice trials and similar to TSPAN, the test procedure included three trials of each list length (3-7 letters), totalling 75 letters and 75 math operations.

\hypertarget{symmetry-span-sspan}{%
\subsubsection{Symmetry Span (SSPAN)}\label{symmetry-span-sspan}}

Participants completed a two-step symmetry judgment and were prompted to recall a visually-presented red square on a 4 X 4 matrix \citep{unsworthAutomatedVersionOperation2005}.
In the symmetry judgment, participants were shown an 8 x 8 matrix with random squares filled in blank.
Participants had to decide if the black squares were symmetrical about the matrix's vertical axis and then click the screen.
Next, they were shown a ``yes'' and ``no'' box and clicked on the appropriate box.
Participants then saw a 4 X 4 matrix for 650 ms with one red square after each symmetry judgment.
During square recall, participants recalled the location of each red square by clicking on the appropriate cell in serial order.
Participants were provided practice trials to become familiar with the procedure.
The test procedure included three trials of each list length (2-5 red squares), totalling 42 squares and 42 symmetry judgments.

\hypertarget{gold-msi-beat-perception}{%
\subsubsection{Gold-MSI Beat Perception}\label{gold-msi-beat-perception}}

Participants were presented 18 excerpts of instrumental music from rock, jazz, and classical genres \citep{mullensiefenMusicalityNonMusiciansIndex2014}.
Each excerpt was presented for 10 to 16s through headphones and had a tempo ranging from 86 to 165 beats per
minute.
A metronome beep was played over each excerpt either on or off the beat.
Half of the excerpts had a beep on the beat, and the other half had a beep off the beat.
After each excerpt was played, participants answered if the metronome beep was on or off the beat and provided their confidence: ``I am sure,'' ``I am somewhat sure,'' or ``I am guessing.''
The final score was the proportion of correct responses on the beat judgment.

\hypertarget{gold-msi-melodic-memory-test}{%
\subsubsection{Gold-MSI Melodic Memory Test}\label{gold-msi-melodic-memory-test}}

Participants were presented melodies between 10 to 17 notes long through headphones \citep{mullensiefenMusicalityNonMusiciansIndex2014}.
There were 12 trials, half with the same melody and half with different melodies.
During each trial, two versions of a melody were presented.
The second version was transposed to a different key.
In half of the second version melodies, a note was changed a step up or down from its original position in the structure of the melody.
After each trial, participants answered if the two melodies had identical pitch interval structures.

\hypertarget{number-series}{%
\subsubsection{Number Series}\label{number-series}}

Participants were presented with a series of numbers with
an underlying pattern.
After being given two example problems to solve, participants had 4.5 minutes in order to solve 15 different problems.
Each trial had 5 different options as possible answers \citep{thurstonePrimaryMentalAbilities1938}.

\hypertarget{ravens-advanced-progressive-matrices}{%
\subsubsection{Raven's Advanced Progressive Matrices}\label{ravens-advanced-progressive-matrices}}

Participants were presented a 3 x 3 matrix of geometric patterns with one pattern missing \citep{ravenManualRavenProgressive1994}. Up to eight pattern choices were given at the bottom of the screen.
Participants had to click the choice that correctly fit the pattern above.
There were three blocks of 12 problems, totalling 36 problems.
The items increased in difficulty across each block.
A maximum of 5 min was allotted for each block, totalling 15 min.
The final score was the total number of correct responses across the three blocks.

\hypertarget{procedure}{%
\subsection{Procedure}\label{procedure}}

Participants in this experiment completed eight different tasks, lasting about 90 minutes in duration.
The tasks consisted of the Gold-MSI self-report inventory, coupled with the Short Test of Musical Preferences \citep{rentfrowReMiEveryday2003}, and a supplementary demographic questionnaire that included questions about socioeconomic status, aural skills history, hearing loss, and any medication that might affect their ability to perform on cognitive tests.
Following the survey they completed three WMC tasks: a novel Tonal Span, Symmetry span, and Operation span task; a battery of perceptual tests from the Gold-MSI (Melodic Memory, Beat Perception, Sound Similarity) and two tests of general fluid intelligence (Gf): Number Series and Raven's Advanced Progressive Matrices.

Each task was administered in the order listed above on a desktop computer.
Sounds were presented at a comfortable listening level for the tasks that required headphones.
All participants provided informed consent and were debriefed.
Only measures used in modeling are reported below.

\hypertarget{results}{%
\section{Results}\label{results}}

\hypertarget{descriptive-data-screening-correlational}{%
\subsubsection{Descriptive, Data Screening, Correlational}\label{descriptive-data-screening-correlational}}

The goal of the analyses was to examine the relationships among the measures and constructs of WMC, general fluid intelligence, and musical sophistication (operationalized as the General score from the Gold-MSI), in relation to the two objective listening tests on the Gold-MSI.
Before running any sort of modeling, data was inspected to ensure that in addition to outlier issues as mentioned above, the data exhibited normal distributions.

Before running any modeling, I checked our data for assumptions of normality since violations of normality can strongly affect the covariances between items.
While some items in Figure 1 displayed a negative skew, many of the individual level items from the self report scale exhibited high levels of Skew and Kurtosis beyond the generally accepted Â± 2 \citep{fieldDiscoveringStatisticsUsing2012}, but none of the items with the unsatisfactory measures are used in the general factor.

\hypertarget{modeling}{%
\subsubsection{Modeling}\label{modeling}}

\hypertarget{measurement-model}{%
\paragraph{Measurement Model}\label{measurement-model}}

I then fit a measurement model to examine the underlying structure of the variables of interest used to assess the latent constructs (general musical sophistication, WMC, general fluid intelligence) by performing a confirmatory factor analysis (CFA) using the lavaan package \citep{rosseelLavaanPackageStructural2012} using R \citep{teamLanguageEnvironmentStatistical2015}.
Model fits can be found in table \ref{tab:exp2variables}.
For each model, latent factors were constrained to have a mean of 0 and variance of 1 in order to allow the latent covariances to be interpreted as correlations.
Since the objective measures were on different scales, all variables were converted to z scores before running any modeling.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/measurementModel} 

}

\caption{CFA Measurement Model}\label{fig:measurementmodel}
\end{figure}

\begin{table}[t]

\caption{\label{tab:exp2variables}Variables Used In Structural Equation Modeling}
\centering
\begin{tabular}{ll}
\toprule
Abbreviation & Variable\\
\midrule
gen & General Self-Report Musical Sophistication\\
wmc & Working Memory Capacity\\
gf & General Fluid Intelligence\\
zIS & Identify What is Special\\
zHO & Hear Once Sing Back\\
\addlinespace
zSB & Sing Back After 2-3\\
zDS & Donât Sing In Public\\
zSH & Sing In Harmony\\
zJI & Join In\\
zNI & Number of Instruments\\
\addlinespace
zRP & Regular Practice\\
zNCS & Not Consider Self Musician\\
zNcV & Never Complimented\\
zST & Self Tonal\\
zCP & Compare Performances\\
\addlinespace
zAd & Addiction\\
zSI & Search Internet\\
zWr & Writing About Music\\
zFr & Free Time\\
zTP & Tone Span\\
\addlinespace
zMS & Symmetry Span\\
zMO & Operation Span\\
zRA & Ravens\\
zAN & Number Series\\
\bottomrule
\end{tabular}
\end{table}

\hypertarget{structural-equation-models}{%
\subsubsection{Structural Equation Models}\label{structural-equation-models}}

Following the initial measurement model, I then fit a series of structural equation models in order to investigate both the degree to which factor loadings changed when variables were removed from the model as well as the model fits.
I began with a model incorporating our three latent variables (general musical sophistication, WMC, general fluid intelligence) predicting our two objective measures (beat perception and melodic memory scores) and then detailed steps we took in order to improve model fit.
For each model, I calculated four model fits: \(\chi^2\) , comparative fit index (CFI), root mean square error (RMSEA), and Tucker Lewis Index (TLI).
In general, a non-significant \(\chi^2\) indicates good model fit, but is overly sensitive to sample size.
Comparative Fit Index (CFI) values of .95 or higher are considered to be indicative of good model fits as well as Root Mean Square Error (RMSEA) values of .06 or lower, Tucker Lewis Index (TLI) values closer to 1 indicate a better fit \citep{beaujeanLatentVariableModeling2014}.

After running the first model (Model 1), I then examined the residuals between the correlation matrix the model expects and our actual correlation matrix looking for residuals above .1.
While some variables scored near .1, two items dealing with being able to sing (``I can hear a melody once and sing it back after hearing it 2 -- 3 times'' and ``I can hear a melody once and sing it back'') exhibited a high level of correlation amongst the residuals (.41) and were removed for Model 2 and model fit improved significantly (\(\chi^2\) (41)=123.39, p \textless{} . 001).

After removing the poorly fitting items, I then proceeded to examine if removing the general musical sophistication self-report measures would significantly improve model fit for Model 3.
Fit measures for Model 3 can be seen in Table \ref{tab:ModelFits} and removing the self-report items resulted in a significantly better model fit (Ï2 (171)=438.8, p \textless{} . 001).
Following the rule of thumb that at least 3 variables should be used to define any latent-variable \citep{beaujeanLatentVariableModeling2014}.
I modeled WMC as latent variable and Gf as a composite average of the two tasks administered in order to improve model fit.
This model resulted in significant improvement to the model (\(\chi^2\) (4)=14.37, p \textless{} . 001).
Finally I examined the change in test statistics between Model 2 and a model that removed the cognitive measures--- a model akin to one of the original models reported in \citep{mullensiefenMusicalityNonMusiciansIndex2014}--- for Model 5.
Testing between the two models resulted in a significant improvement in model fit (\(\chi^2\) (78)=104.75, p \textless{} . 001).
Figure \ref{fig:model4} displays Model 4, our nested model with the best fit indices.

\begin{table}[t]

\caption{\label{tab:ModelFits}Structural Equation Model Fits}
\centering
\begin{tabular}{lrrlrrr}
\toprule
Models & df & chi & p & CFI & RMSEA & TLI\\
\midrule
CFA & 186 & 533.60 & > .001 & 0.83 & 0.09 & 0.81\\
Model 1 & 222 & 586.30 & > .001 & 0.83 & 0.08 & 0.80\\
Model 2 & 181 & 462.90 & > .001 & 0.86 & 0.08 & 0.83\\
Model 3 & 10 & 24.11 & > .05 & 0.97 & 0.08 & 0.94\\
Model 4 & 6 & 9.74 & > .14 & 0.99 & 0.51 & 0.97\\
Model 5 & 130 & 358.16 & > .001 & 0.83 & 0.10 & 0.80\\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/sem1} 

}

\caption{Model 1 | Full Model, All Variables Included}\label{fig:model1}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/sem2} 

}

\caption{Model 2 | Full Model, Highly Correlated Residual Items}\label{fig:model2}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/sem3} 

}

\caption{Model 3 | Self Report Removed, Only Cognitive Measures}\label{fig:model3}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/sem4} 

}

\caption{Model 4 | Cognitive Measures, Gf as Observed}\label{fig:model4}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/sem5} 

}

\caption{Model 5 | General Self Report Only}\label{fig:model5}
\end{figure}

\hypertarget{discussion}{%
\section{Discussion}\label{discussion}}

\hypertarget{model-fits}{%
\subsection{Model Fits}\label{model-fits}}

\hypertarget{measurement-model-1}{%
\subsubsection{Measurement Model}\label{measurement-model-1}}

After running a confirmatory factor analysis on the variables of interest, the model fit was below the threshold of what is considered a ``good model fit'' as shown in \ref{tab:ModelFits} with references to above model fits.
This finding is to be expected since no clear theoretical model has been put forward that would suggest that the general musical sophistication score, when modeled with two cognitive measures should have a ``good'' model fit.
This model was run to create a baseline measurement.

\hypertarget{structural-equation-model-fitting}{%
\subsubsection{Structural Equation Model Fitting}\label{structural-equation-model-fitting}}

Following a series of nested model fits, I was able to improve model fits on a series of structural equation models that incorporated both measures of working memory capacity and measures of general fluid intelligence.
Before commenting on new models, it is worth noting that Model 5 does not seem to align with the findings from the original 2014 paper by \citep{mullensiefenMusicalityNonMusiciansIndex2014}.
While the correlation between the objective tasks is the same (\(r\) = .16), the factor loadings from this analysis suggest lower values for both Beat Perception (\(r\) = .37 original, \(r\) = .27 this chapter) as well as Melodic Memory (\(r\) = .28 original, \(r\) = .18 this chapter).
Note that two items were removed dealing with melody for memory for this model; when those items were re-run with the data, the factor loadings did not deviate from these numbers.

The first two models I ran resulted in minor improvements to model fit.
While the difference in models was significant (\(\chi^2\) (41)=123.39, p \textless{} . 001), probably due to the number of parameters that were now not constrained, the relative fit indices of the models did not change for practical interpretation.
It was not until the self-report measures were removed from the model, and then manipulated according to latent variable modeling recommendations, that was there a marked increase in the relative fit indices.
Fitting the model with only the cognitive measures, I was able to enter the bounds of acceptable relative fit indices that were noted above.
In order to find evidence that the cognitive models (Models 3 and 4) were indeed a better fit than using the General factor, I additionally ran a comparison between our adjusted measurement model and a model with only the self-report.
While both of the nested models were significantly different, the cognitive models exhibited superior relative fit indices.
Lastly, turning to \ref{fig:model4}, I note that the latent variable of working memory capacity exhibited much larger path coefficient predicting the two objective, perceptual tests than our measure of general fluid intelligence.
I also note that the path coefficients predicting the Beat Perception task (\(r\) =.36) was higher than that of the Melodic Memory task (\(r\) = .21).
These rankings mirror that of the original \citep{mullensiefenMusicalityNonMusiciansIndex2014} paper and merit further examination in order to disentangle what processes are contributing to both tasks.

Given the results here that suggest that measures of cognitive ability play a significant role in tasks of musical perception, my analysis suggests that future research should consider taking measures of cognitive ability into account, so that other variables of interest are able to be shown to contribute above and beyond baseline cognitive measures.

\hypertarget{relating-to-melodic-dictation}{%
\subsection{Relating to Melodic Dictation}\label{relating-to-melodic-dictation}}

This study sought to investigate the extent to which individual factors contributed to an individual's ability to perform the first two steps of melodic dictation.
In order to do this, I assumed that the first two steps of the Karpinski model--- hearing and short term melodic memory--- could be investigated by using a same-different melodic memory paradigm.
Both tasks require the dual activation of representing information in conscious awareness and completing a cognitive task.
Using this paradigm also allowed me to investigate the first two steps of Karpinski's model using both individuals with and without musical training.

Overall, when interpreting the results I found evidence to corroborate claims made by \citet{berzWorkingMemoryMusic1995} positing the importance of working memory in both tests of musical aptitude, and consequently the first two steps of melodic dictation as described by Karpinski.
Relatively, working memory seemed to dominate as the variable with the most explanatory power as derived from both the best overall model fits and highest path coefficients in the latent variable modeling.
This is not a surprising finding given the context, yet it has major implications for future research in music perception.
If a domain general process is able to predict performance on a domain specific task (melodic memory) better than measures of self report and training, future studies in music perception will need to be able to demonstrate how the process they purport to be the driving factor behind their models explains their findings above and beyond working memory capacity.

Also worth discussing is why general fluid intelligence did not fare as well in the models above.
One reason that this might be is because general intelligence tests are designed in two ways differing from that of melodic dictation.
The first is that general fluid intelligence tests administered here do not have any time component to them.
While tasks like Raven's matrices \citep{ravenManualRavenProgressive1994} and the number series \citep{thurstonePrimaryMentalAbilities1938} tests are timed, the information is presented visually to participants.
The second is that general fluid intelligence is designed to measure abilities outside of the context of previously known information \citep{cattellAbilitiesTheirGrowth1971} and questions surrounding music perception depend on principles of statistical learning \citep{huronSweetAnticipation2006, pearceStatisticalLearningProbabilistic2018a, saffranStatisticalLearningTone1999}, stylistic enculturation \citep{demorestLostTranslationEnculturation2008, eerolaExpectancySamiYoiks2009, meyerEmotionMeaningMusic1956}, and music theory.
General fluid intelligence might be helpful at later stages of cognitive processing such as the musical understanding and notation phases of the Karpinski model, but their effect does not seem to be present here.

From a pedagogical standpoint, this is important in that many teachers are aware that students will vary in terms of their working memory ability.
While it might be tempting to pathologize cognitive predictors deemed relevant in these contexts, further work specifically looking at the load on memory is needed before claims are made about incorporating this as a way to diagnose problems with student performance.
One practical consideration for the classroom within the Karpinski framework that could be put forward would be to encourage students to listen for smaller chunks when using extractive listening.
Using a Cowan's model of working memory, students should extract smaller chunks so that they still have cognitive resources available in order to focus on the later stages of the Karpinski model (musical understanding and notation).
As attention is limited, not listening to more than you can hold will free up cognitive resources that might later be used in melodic dictation.
Further students could take up recommendations like that of \citet{chenetteReframingAuralSkills2019} and focus on activities that might help them increase their ability to focus, knowing that this practice will most likely not increase their working memory.

Not only will these findings have relevance in the classroom, but they suggest that future work looking to do more robust modeling of melodic dictation must take into account the window of attention.
In the final chapter, I incorporate this finding into a computational model of melodic dictation and use the finite window of working memory as a perceptual bottleneck to constrain incoming musical information.

In this chapter, I fit a series of structural equation models in order to investigate the degree to which baseline cognitive ability was able to predict performance on a musical perception task.
My findings suggest that measures of working memory capacity are able to account for a large amount of variance beyond that of self report in tasks of musical perception.

\clearpage

\hypertarget{musical-differences}{%
\chapter{Musical Differences}\label{musical-differences}}

\hypertarget{rationale-2}{%
\section{Rationale}\label{rationale-2}}

Music theorists use their experience and intuition to build appropriate curricula for their aural skills pedagogy.
Teaching aural skills typically starts with providing students with simpler exercises, often employing a limited number of notes and rhythms, and then slowly progressing to more difficult repertoire.
This progression from simpler to more difficult exercises is evident in aural skills textbooks.
Of the major aural skills textbooks such as the \citet{ottmanMusicSightSinging2014}, \citet{berkowitzNewApproachSight2011}, \citet{karpinskiManualEarTraining2007}, and \citet{clelandDevelopingMusicianshipAural2010}, each is structured in a way that musical material presented earlier in the book is more manageable than that nearer the end.
In fact, this is true of almost any Ã©tude book: open to a random page in a book of musical studies and the difficulty of the study will likely scale accordingly to its relative position in the textbook.
But it is not a melody's position in a textbook that makes it difficult to perform: this difficulty comes from the structural elements of the music itself.

Intuitively, music theorists have a general understanding of what makes a melody difficult to dictate.
Factors that might contribute to this complexity could range from the number of notes in the melody, to the intricacies of the rhythms involved, to the scale from which the melody derives, to even more intuitively understood factors such as how tonal the melody sounds.
Although given all these factors, there is no definitive combination of features that perfectly predicts the degree to which pedagogues will agree how complex a melody is.
In many ways, questions of melodic complexity are very much like questions of melodic similarity: it depends on both who is asking the question and for what reasons \citep{cambouropoulosHowSimilarSimilar2009}.

Examining the melodies presented in Figures \ref{fig:musicalpuzzleA} and \ref{fig:musicalpuzzleB}, most aural skills pedagogues will be able to successfully intuit which melody is more complex, and presumably, more difficult to dictate.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/musicalpuzzle/MP2/MP_X-1} 

}

\caption{A Musical Puzzle}\label{fig:musicalpuzzleA}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/musicalpuzzle/MP2/MP_Y-1} 

}

\caption{B Musical Puzzle}\label{fig:musicalpuzzleB}
\end{figure}

While I reserve an extended discussion of what features might characterize why one melody is more difficult to dictate than the other for this chapter, I assume that these melodies differ in their ability to be dictated in some fundamental way when performed in a similar fashion.
Additionally, many readers of this dissertation can draw from anecdotal evidence of their own as to how students at various stages of their aural training might fare when asked to dictate both melodies.
For some, Melody \ref{fig:musicalpuzzleB} might be overwhelmingly difficult.

In fact, Melody \ref{fig:musicalpuzzleB} might be overwhelmingly difficult for the vast majority of musicians to dictate.
From a pedagogical standpoint, educators need to be able to know how difficult melodies are to dictate in order to ensure a degree of fairness when assessing a student's performance.
While of course with each student there are inevitably many variables at play in aural skills instruction ranging from personal abilities, to the goals of the instructor in the scope of their course, I find it fair to claim that pedagogues assume that students will be expected to pass pre-established benchmarks throughout their aural skills education.
As students progress, they are expected to be able to dictate more difficult melodies, yet exactly what makes a melody complex and thus difficult to dictate is often left to the expertise and intuition of a pedagogue.
Intuition is an important skill for teachers to cultivate, but when it comes to determining objective measures of judgment, research from decision making science tends to suggest that no matter the expertise, collective and objective knowledge tends to outperform a single person's judgment \citep{kahnemanThinkingFastSlow2012, loggAlgorithmAppreciationPeople2019, meehlClinicalStatisticalPrediction1954}.
Having more clearly defined performance benchmarks also helps remove biases in grading that teachers may or may not be explicitly aware of.
Recent research has suggested that even aural skills pedagogues are open to the idea of looking for more standardization in aural skills assessments \citep{paneyTeachingMelodicDictation2014}.

In this chapter, I survey and examine how tools from computational musicology can be used to help model an aural skills pedagogue's notion of complexity in melodies.
First, I establish that theorists agree on the differences in melodic complexity using results from a survey of 40 aural skills pedagogues.
Second, I explore how both static and dynamic computationally derived abstracted features of melodies can and cannot be used to approximate an aural skills pedagogue's intuition.
Third and finally, I use evidence afforded by research in computational musicology to posit that the distributional patterns in a corpus of music can be strategically employed to create a more linear path to success among students of aural skills.
I demonstrate how combining evidence from the statistical learning hypothesis, the probabilistic prediction hypothesis, and a newly posited distributional frequency hypothesis, it is possible to explain why some musical sequences in a melody are easier to dictate than others.
Using this logic, I then create a new compendium of melodic incipits, sorted by their perceptual complexity, that can be used for teaching applications.

\hypertarget{agreeing-on-complexity}{%
\section{Agreeing on Complexity}\label{agreeing-on-complexity}}

Returning to melodies from Figures \ref{fig:musicalpuzzleA} and \ref{fig:musicalpuzzleB} from above, an aural skills pedagogue most likely has an intuition to which of the two melodies would be easier to dictate.
Melody \ref{fig:musicalpuzzleA} exhibits a predictable melodic syntax and phrase structure, the chromatic notes resolve within the conventions of the Common Practice period, and the melody itself could be scored with tertian harmony.
On the other hand, Melody \ref{fig:musicalpuzzleB}'s syntax does not conform to the conventions of the Common Practice period and does not imply any sort of underlying harmony or predictable phrase rhythm.
The duration of the rhythms appear irregular and the melody implies an uneven phrase structure.
Yet both melodies \ref{fig:musicalpuzzleA} and \ref{fig:musicalpuzzleB} have the exact same set of notes and rhythms.
Despite these content similarities, it would be safe to assume that melody \ref{fig:musicalpuzzleA} is probably much easier to dictate than melody \ref{fig:musicalpuzzleB} assuming both were to be played in a similar fashion.

In fact, aural skills pedagogues tend to agree for the most part on questions of difficulty of dictation.
To demonstrate this, I surveyed 40 aural skills pedagogues who all have taught aural skills at the post-secondary level.
In this survey, participants were asked the questions presented in Table \ref{tab:surveyQuestions1} and Table \ref{tab:surveyQuestions2} using a sample of 20 melodies found in the a commonly used sight-singing textbook \citep{berkowitzNewApproachSight2011}.
I present the details of the survey below.

\hypertarget{methods}{%
\subsection{Methods}\label{methods}}

To select the melodies used in this survey, I randomly sampled 30 melodies from a corpus of melodies (N = 481) from a subset of the Fifth Edition of the Berkowitz \emph{A New Approach to Sight Singing} \citep{berkowitzNewApproachSight2011} in order to ensure a representative sampling of melodies that might be used in a pedagogical setting.
After piloting the randomly sampled melodies on a colleague, I again randomly sampled half of this sub-set and then added in five more melodies that were not in the new set from earlier sections of the book in order to be more representative of materials students might find in the first two semesters of their aural skills pedagogy.
I ran the survey from January 31st of 2019 until March 7th, 2019.
The survey comprised of two sets of questions.

Six questions asked about the teaching background of respondents and these questions can be found in Table \ref{tab:surveyQuestions1}.
These questions were followed by asking participants to make five ratings over the 20 different melodies.
The five questions can be found in \ref{tab:surveyQuestions2}.
To encourage participation, two \$30 cash prizes were offered to two participants.
The survey had questions that were specifically designed to gauge their appropriateness for use in a melodic dictation context.
Participants were recruited exclusively online, and all provided consent to partaking in the data collection as approved by the Louisiana State University Institutional Review Board.

The table below contains the questions used in the demographic questionnaire.
Examples were given following each questions and can be found on the survey link.

\begin{longtable}{l}
\caption{\label{tab:surveyQuestions1}Survey Questions}\\
\toprule
Demographic Questions\\
\midrule
What is your age, in years?\\
What is your educational status?\\
How many years have you been teaching Aural Skills at the University level?\\
Which type of syllable system do you prefer to use?\\
On which instrument have you gained the most amount of professional training?\\
What is the title of the last degree you received?\\
At what institution are you currently teaching?\\
\bottomrule
\end{longtable}

The table below contains the questions regarding the ratings of the melodies.
Participants either responded using ordinal categories or moved a slider that sat atop a 100 point scale.

\begin{table}[t]

\caption{\label{tab:surveyQuestions2}Item Questions}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{l}
\toprule
Item Questions\\
\midrule
During which semester of Aural Skills would you think it is appropriate to give this melody as a melodic dictation?\\
How many times do you think this melody should be played in a melodic dictation\\
considering the difficulty you noted in your previous question?\\
Assume a reasonable tempo choice from 70-100BPM.\\
Please rate how difficult you believe this melody to be for the average second-year  undergraduate student at your institution.\\
The far left should indicate 'Extremely Easy' and the far right should indicate 'Extremely Difficult'.\\
Please rate this melody's adherence to the melodic grammar of the Common Practice  Period.\\
The far left should indicate 'Not Well Formed' and the far right should indicate 'Very Well Formed'.\\
Is this melody familiar to you?\\
\bottomrule
\end{tabular}}
\end{table}

Of the respondents, the average amount of years teaching aural skills was 8.76 years (\(SD = 7.60, R: 21-29\)).
I plotted the breakdown of the respondent's age and educational status below in Figure \ref{fig:surveyageeducation}.
Of the 40 respondents, all reported used some sort of movable system other than 2 who used a fixed system.
The sample represented over 30 different institutions.
Overall, the sample reflects a wide range of experience of teaching aural skills.
The sample contains both younger and older individuals, as well as a range of experience.
In the Figures \ref{fig:berk3} through \ref{fig:berk629} below, I list the 20 melodies sampled.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img/ageedsurveydistribution} 

}

\caption{Demographic Breakdown of Survey Sample}\label{fig:surveyageeducation}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/survey_melodies/Berkowitz3} 

}

\caption{Melody 3   | Rank 1}\label{fig:berk3}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/survey_melodies/Berkowitz9} 

}

\caption{Melody  9  | Rank 2}\label{fig:berk9}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/survey_melodies/Berkowitz26} 

}

\caption{Melody 26 | Rank 3}\label{fig:berk26}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/survey_melodies/Berkowitz59} 

}

\caption{Melody 59 | Rank 4}\label{fig:berk59}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/survey_melodies/Berkowitz70} 

}

\caption{Melody 70 | Rank 5}\label{fig:berk70}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/survey_melodies/Berkowitz74} 

}

\caption{Melody 74 | Rank 6}\label{fig:berk74}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/survey_melodies/Berkowitz75} 

}

\caption{Melody 75 | Rank 7}\label{fig:berk75}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/survey_melodies/Berkowitz88} 

}

\caption{Melody 88 | Rank 8}\label{fig:berk88}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/survey_melodies/Berkowitz156} 

}

\caption{Melody 156 | Rank 9}\label{fig:berk156}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/survey_melodies/Berkowitz282} 

}

\caption{Melody 282 | Rank 10}\label{fig:berk282}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/survey_melodies/Berkowitz294} 

}

\caption{Melody 294 | Rank 11}\label{fig:berk294}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/survey_melodies/Berkowitz312} 

}

\caption{Melody 312 | Rank 12}\label{fig:berk312}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/survey_melodies/Berkowitz334t} 

}

\caption{Melody 334 | Rank 13}\label{fig:berk334t}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/survey_melodies/Berkowitz379} 

}

\caption{Melody 379 | Rank 14}\label{fig:berk379}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/survey_melodies/Berkowitz382t} 

}

\caption{Melody 382 | Rank 15}\label{fig:berk382}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/survey_melodies/Berkowitz417tx} 

}

\caption{Melody 417 | Rank 16}\label{fig:berk417tx}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/survey_melodies/Berkowitz607tx} 

}

\caption{Melody 607 | Rank 17}\label{fig:berk607tx}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/survey_melodies/Berkowitz622} 

}

\caption{Melody 622 | Rank 18}\label{fig:berk622}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/survey_melodies/Berkowitz627} 

}

\caption{Melody 627 | Rank 19}\label{fig:berk627}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/survey_melodies/Berkowitz629} 

}

\caption{Melody 629 | Rank 20}\label{fig:berk629}
\end{figure}

\hypertarget{agreeing-on-difficulty}{%
\subsection{Agreeing on Difficulty}\label{agreeing-on-difficulty}}

In order to assess the degree to which pedagogues agreed on a melody for melodic dictation, I first plotted the mean ratings for each melody across the entire sample along with their standard error of the means in Figure \ref{fig:diffplot}.
The \(x\) axis uses the rank of the melodies, not their index position in the Berkowitz textbook.
I chose to use this rank order metric as the number of a melody in a textbook is presumed to be best conceptualized as an ordinal variable.
For example, it would be correct to assume that Melody 200 is more difficult than melody 2, but not by a factor of 100.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/difficulty_plot} 

}

\caption{Difficulty Ratings from Survey}\label{fig:diffplot}
\end{figure}

From Figure \ref{fig:diffplot}, there is an increasing linear trend from ratings of melodies being less difficult to more difficult across the sample.
Using an intraclass coefficient calculation of agreement using a two-way model (both melodies and raters treated as random effects), the sample reflects an interclass correlation coefficient of .79.
According to \citet{kooGuidelineSelectingReporting2016}, this reflects a good degree of agreement between raters.
This trend across the sample appears in the opposite direction when plotting the mean values to the fourth question in Figure \ref{fig:grammarplot} from the survey reflecting the melody's adherence to the melodic grammar of the Common Practice period.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/grammar_plot} 

}

\caption{Grammar Ratings from Survey}\label{fig:grammarplot}
\end{figure}

While similar trends appear here, yet in the opposite direction as expected, there is a clear breaking of linear trend in the far right potion of the graph that shows melodies that were sampled from the chapter of the corpus that contains atonal melodies.
Using an intraclass coefficient calculation of agreement using a two-way model, with melodies and raters treated as random effects, the sample reflects an interclass coefficient of .65, which according to \citet{kooGuidelineSelectingReporting2016} indicates a moderate degree of agreement among raters.
This lower agreement rating is most likely due to the subjectiveness of this question.
In their free text responses, many participants expressed difficulty in surmising what this meant.

The trends from Figure \ref{fig:diffplot} and Figure \ref{fig:grammarplot} occur in the opposite direction.
As the index or rank of the melody increases, so does the difficulty for the rating as would be expected.
As the index or rank of the melody increases, its adherence to subjective ratings of melodic grammar of the Common Practice period decreases.
Taken together, I ran a correlation on every one of the twenty melodies between a single rater's judged difficulty and its judged adherence to tonal expectations of the common practice era.
The correlations for all 20 melodies are plotted here in Figure \ref{fig:gramcor}.
From this chart, we see this trend is not uniform across all melodies.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/grammar_difficulty_correlation_plot} 

}

\caption{Correlation Between Difficulty and Subjective Ratings of Tonal Grammar}\label{fig:gramcor}
\end{figure}

Overall, the sample exhibited an acceptable degree of inter-rater reliability as measured by the interclass correlation coefficient.
Plotting the respondent's answers across the textbook that melodies were taken from, with the book progressing from less to more difficult, it does appear that aural skills pedagogues tend to agree on how difficult a melody is when used in a dictation setting.

Central to my argument, there appears to a linear trend of difficulty across the sample based on the melodies rank in the sample.
In fact, although I presented the data above as ordinal using rank in the textbook, when I ran a mixed-effects linear regression predicting melody difficulty with both rank order as a variable as well as the actual index number of the melody from the Berkowitz, the index model significantly outperforms the rank order model.
Using the lme4 package \citep{batesFittingLinearMixedEffects2015}, I fit two linear mixed effects models predicting difficulty of melody with subject and item both as random effects in the model, with the only difference in models being a melody rank or melody index.
When comparing models, the index model (BIC = 6706.3) provided a better fit to the data (\(\chi^2\)=5.38, \(p<.05\)) than the rank model (BIC = 6711.7).

Taken together, both anecdotal and empirical evidence for this survey suggest that aural skills pedagogues tend to agree on how difficult a melody is for use in an aural skills setting.
This sense of difficulty or complexity tracks as the book progresses, but to attribute the cause of a melody being difficult as its position in the book would be putting the cart before the horse.
Having now formally established this almost intuitive notion, the remaining portion of this chapter investigates how computationally derived tools can be used to model these commonly held intuitions.
In order to provide a sense of validity to the measure, I carry forward ratings from the survey reported and use the expert answers as the ground truth for the the resulting models.

\hypertarget{modeling-complexity}{%
\section{Modeling Complexity}\label{modeling-complexity}}

The ability to quantify what theorists generally agree to be melodic complexity depends on distilling complexity into its component parts.
Earlier, when comparing melodies \ref{fig:musicalpuzzleA} and \ref{fig:musicalpuzzleB}, some of the features put forward that might contribute to complexity were features such as note density, the melody's rhythm, what scale the melody draws its notes from, and how tonal the melody might be perceived.
Some combination of these component features presumably make up the construct of complexity.

Attempting to use features of a melody to to predict how well a melody is remembered has a long history.
In 1933, Ortmann put forward a set of melodic determinants that he asserted predicted how well a melody was remembered.
These features such as a melody's repetition, pitch-direction, contour (conjunct-disjunct motion), degree, order, and implied harmony (chord structure) were deemed to affect the melody's ability to be remembered \citep{ortmannTonalDeterminantsMelodic1933}.

Since Ortmann, pedagogues such as Taylor and Pembrook have expanded on this research, finding significant effects of musical features such as length, tonality, as well as type of motion as well as an effect of experimental condition \citep{taylorStrategiesMemoryShort1983}.
Following up on Taylor's investigation, \citet{pembrookInterferenceTranscriptionProcess1986} found evidence corroborating Ortmann's initial claims that his four major determinants (repetition, note direction, conjunct-disjunct motion, degree of disjunctivness) had a significant main effects on an individual's ability to take dictation, yet note that these values do not exhaustively explain the findings.
In their discussion, they also note the problems of completely isolating the effects of certain musical features as when you change one parameter, others are also subject to change.
When looking at changes in structural elements of melodies, there is a collinearity issue among features.
Not only does this problem exist within features of melodies, but also among participants.
In reflecting on other factors that might contribute to their results, the authors note

\begin{quote}
Clearly, a complete hierarchy of determinants would constitute a very long
list, because not only would the many melodic structures be included, but also
their interactions with subject and environmental variables. The ones included
in the present study (musical experience, melodic carryover, and response
method) provided evidence that the melodic determinants are not constant;
rather, they vary as a function of the subject and environmental factors, which
in turn can have significant effects on music discrimination and memory. (p.~33)
\end{quote}

The authors later in the article go on to stress that future work should both replicate their findings as well as expand their modeling parameters.
They call for a larger sample, a broader spectrum of musical experiences, and to investigate more musical features.

Since then some researchers have employed using features of the melodies to predict a behavioral measure in experimental settings.
Not using as extensive of a battery as Ortmann, Taylor, or Pembrook, researchers in music psychology such as as \citet{akiva-kabiriMemoryTonalPitches2009}, \citet{dewittRecognitionNovelMelodies1986}, \citet{eerolaExpectancySamiYoiks2009}, \citet{schulzeWorkingMemorySpeech2012} have used the number of notes in a melody as a successful predictor of difficulty in melodic perception and discrimination tasks.
Expanding on just using frequency of note counts, \citet{harrisonModellingMelodicDiscrimination2016} instead of looking at single measures of melodic complexity, addressed the melodic collinearity issue noted by Taylor and Pembrook by using data reductive techniques to derive a single complexity measure found to be predictive in their statistical modeling deriving these measures from the FANASTIC toolbox \citep{mullensiefenFantasticFeatureANalysis2009}.
Following this research, \citet{bakerPerceptionLeitmotivesRichard2017} also incorporated a similar measure of complexity in their model of leitmotiv recognition in which they predicted recall rates in a recognition paradigm.

Each of these examples operationalizes some feature of the melody with a quantitative, numerical proxy that is assumed to be able to be mapped to perception.
Ortmann referred to these as determinants, while others such as MÃ¼llensiefen refer to them as features \citep{mullensiefenFantasticFeatureANalysis2009}.
Since the word feature refers to a `distinctive attribute', I will use this terminology throughout the rest of the chapter, though note that other terms have been used.

\hypertarget{what-are-features}{%
\subsection{What Are Features?}\label{what-are-features}}

A feature can be either a quantitatively or qualitatively observable feature of a melody that is assumed to be perceptually salient to the listener.
Features are often difficult to quantify with the traditional tools of music analysis.
Often, these features come inspired from other domains like computational linguistics.

The nPVI began as a measure of rhythmic variability in language \citep{grabeDurationalVariabilitySpeech2002}.
Shown below, the nPVI quantifies the amount of durational variability in language.
It works by comparing the variability of vowel length compared to syllable length

\[nPVI = 100 * [\sum_{k=1}^{m-1} | \frac{d_k - d_{k+1}}{(d_k + d_{k+1})/2}/(m-1)] \]

where \(M\) is the number of vowels in an utterance and \(d_k\) is th duration of the \(k^{th}\) item and has been used in musical contexts \citep{vanhandelRoleMeterCompositional2010}.

In linguistics, the nPVI has been used to delineate quantitative differences between stress and syllable timed languages.
Recently in the past decade, music science researchers have used the nPVI to attempt to investigate claims about the relationship between speech and music \citep{danieleInterplayLinguisitcHistorical2004, patelStressTimedVsSyllableTimed2003, vanhandelRoleMeterCompositional2010}.
While results are mixed regarding the nPVI's predictive ability and there have been recent calls to limit the measure's use \citep{condit-schultzDeconstructingNPVIMethodological2019}, it does serve as a very good example of a computational derived measure.
Just like summarizing the range of a melody by subtracting the distance between the lowest and highest notes, the nPVI summarizes a phrase and importantly assumes that this measure is representative of the entire phrase the calculation was performed upon.

In computational musicology, features of melodies can generally be classified into two main types: static and dynamic features.
Static features compute a summary measure over the entire melody while dynamic features calculate values for each event onset in a melody.
One of the most complete set of static computational measures as applied to music perception come from Daniel MÃ¼llensiefen's' Feature ANalysis Technology Accessing STatistics (In a Corpus) or FANTASTIC toolbox \citep{mullensiefenFantasticFeatureANalysis2009}.

\begin{quote}
FANTASTIC is a program\ldots{}that analyzes melodies by computing features. The aim is to characterise a melody or a melodic phrase by a set of numerical or categorical values reflecting different aspects of musical structure. This feature representation of melodies can then be applied in Music Information Retrieval algorithms or computational models of melody cognition. (p.~4)
\end{quote}

Drawing from fields both central and peripheral to music science, FANTASTIC computes a collection of 38 features to analyze features of melodies and joined a large and continuing tradition of analyzing music computationally \citetext{\citealp[ ]{lomaxCantometricsApproachAnthropology1977}; \citealp{eerolaExpectancySamiYoiks2009}; \citealp{huronHumdrumToolkitReference1994}; \citealp{lartillotMatlabToolboxMusical2007}; \citealp{lomaxCantometricsApproachAnthropology1977}; \citealp{mcfeeLibrosaAudioMusic2015}; \citealp{steinbeckStrukturUndAhnlichkeit1982}}.
Additionally, FANTASTIC also provides a framework for comparing the features of a melody with a parent corpus from which the melody is assumed to belong similar to a sample-population relationship.

\hypertarget{back-to-the-classroom}{%
\subsection{Back to the Classroom}\label{back-to-the-classroom}}

Returning to the Aural Skills classroom, many of these features can be used to approximate the previously established intuitions of complexity as agreed upon by theorists.
Below in Figure \ref{fig:corfeature}, I plot the the mean difficulty and grammar ratings given by experts for each melody in the experimental sample against each of the output of FANTASTIC's features by correlating the two measures.
Additionally, Table \ref{tab:corfeaturetable} displays the five strongest positive and negatively correlated features of FANTASTIC's output with the ground truth, expert ratings.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/FantasticExpertPlot} 

}

\caption{FANTASTIC Computations Correlated with Expert Ratings}\label{fig:corfeature}
\end{figure}

\begin{longtable}{lll}
\caption{\label{tab:corfeaturetable}Strong Features}\\
\toprule
Feature & Difficulty & Grammar\\
\midrule
i.abs.std & 0.89 & -0.82\\
i.abs.mean & 0.87 & -0.91\\
setp.cont.loc.var & 0.97 & -0.74\\
i.entropy & 0.85 & -0.74\\
p.entropy & 0.84 & -0.72\\
d.median & -0.19 & 0.22\\
d.eq.trans & -0.20 & 0.04\\
mean.Yules.K & -0.43 & 0.40\\
tonalness & -0.48 & 0.44\\
mean.Simpsons.D & -0.57 & 0.50\\
\bottomrule
\end{longtable}

From Figure \ref{fig:corfeature} and Table \ref{tab:corfeaturetable}, there are some features that share a strong relationship with the ground truth of the expert intuitions.
The top five features that correlate most strongly with the expert ground truths are related to the intervallic content of a melody.
The first two features, \texttt{i.abs.std} and \texttt{i.abs.mean} are derived measures using absolute interval distance computations.
The other top three features, \texttt{step.cont.loc.var}, \texttt{i.entropy}, and \texttt{p.entroy} are related to entropy measures.
Of the negatively correlated features, two linguistically derived measures \texttt{mean.Yules.K} and \texttt{mean.Simpsons.D} both correlate with perceived difficulty, as does a measure of \texttt{tonalness} which in FANTASTIC is based on the Krumhansl key profiles \citep{krumhanslCognitiveFoundationsMusical2001}.

One problem in tackling this problem is that although many of these variables correlate strongly with our target variables--- both grammar and difficulty ratings--- one aspect not apparent in this analysis is the correlation between each of the features.
In order to demonstrate this, in Figure \ref{fig:featurecorrelations} I visualize how a sample of features from the FANTASTIC toolbox correlate with one another with mode additionally included to highlight the breakdown of the corpus.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/FANTASTIC_collin} 

}

\caption{Visualization of the Problem of Melodic Feature Collinearity}\label{fig:featurecorrelations}
\end{figure}

Among these variables, we see that there is a very high degree of correlation between many of the variables.
For example, the two features inspired from linguistics--- \texttt{mean.Yules.K} and \texttt{mean.Simpsons.D} --- exhibit an alarming degree of correlation.
We also see in this dataset evidence of the inappropriateness of including some variables such as \texttt{d.median}, a measure relating rhythm.

In \ref{fig:featurecorrelations} we see computational evidence of claims made by \citet{taylorStrategiesMemoryShort1983} when reviewing exactly what features might contribute to the degree of difficulty from a melodic dictation.
Given this collinearlity problem, it becomes very difficult to be able to isolate the effect of one feature of the melody.
One way to begin to understand these relationships would be to be to build statistical models that are able to partition covariance structures such as the general linear model when used in the context of multiple regression.
Another method, as mentioned above, could instead take a more exhaustive, but less explanatory approach forward and follow past research \citep{bakerPerceptionLeitmotivesRichard2017, harrisonModellingMelodicDiscrimination2016}, that uses data reductive techniques such as principal components analysis to obtain more accurate predictive measures of complexity.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/univariate_cow} 

}

\caption{Univariate Feature Models}\label{fig:univariatecow}
\end{figure}

In Figure \ref{fig:univariatecow}, I plot eight features extracted via the FANTASTIC Toolbox.
The figure plots linear models of each feature compared against the expert ratings of difficulty.
I additionally list the Pearson correlation coefficient for each model.
From the plot, it is evident that some features correlate much stronger with the ground truth features than others.
For example, \texttt{pitch.entropy} correlates with the ground truth data \(r = .84\).
Not only that, but the model is not being driven completely by outliers.
While some points fall below the regression line, extreme values are not driving this effect.
A similarly strong relationship is evident with the \texttt{step.cont.local.var} variable.
In line with work by Dowling, this provides further evidence that contour changes have a significant impact on how people hear melodies \citep{dowlingScaleContourTwo1978}.
In exploring these relationships in multivariate context, when I combined the top four variables from \ref{fig:univariatecow} in a linear multiple regression model, the model was able to predict a high degree of variance \(F(4,15) = 30.47, p < .05, R^2 = .89\).
While this model is explicitly exploratory, this dataset will serve as a foundation to build future theories to test.

Relating back to its implication for aural skills pedagogy, the above analysis suggests that features derived from the FANTASTIC toolbox can provide a meaningful step forward in helping standardize the assessment of aural skills pedagogy.
If pedagogues were able to employ tools such as the FANTASTIC toolbox, pedagogues could not only select melodies for their own work that are able to hold certain features constant, but the use of this research could also be used to generate melodies based on the desired difficulty parameter measures in order to design course curricula that would foster a more stable curricular path among students.
Additionally, students could also work at slowly challenging themselves if this were to be incorporated into an online pedagogical learning application or website.

Although this approach has been relatively successful at modeling expert ratings, using FANTASTIC's various linear combinations of these features does have important limitations.
One of the most obvious limitations is that FANTASTIC's measures tacitly assume listeners recall melodies in some sort of perceptual suspended animation.
Illuminating this problem using a more tangible example, again returning to melodies \ref{fig:musicalpuzzleA} and \ref{fig:musicalpuzzleB}, when the full set of FANTASTIC features are computed on both, the both melodies are computational equivalent in their range, pitch entropy, durtational range, durational entropy, length, tonalness, tonal clarity, tonal spike and stepwise contour global variation.
This computation arises from computing a summary measure over the melody and not modeling it in terms of real time perception.
In order to have a more phenomenologically appropriate model that incorporates computationally derived features, it is important to also consider dynamic models of music perception when modeling difficulty.
Following up on another finding from this section, it also is worthy of mention that the variables with the strongest predictive powers tend to be those associated with information content.
In the next section, I explore how using a dynamic approach such as Marcus Pearce's implementation \citep{pearceConstructionEvaluationStatistical2005, pearceStatisticalLearningProbabilistic2018a} of a multiple viewpoints model \citep{conklinMultipleViewpointSystems1995}, might provide more insights into understanding the aural skills classroom.

\hypertarget{dynamic}{%
\subsection{Dynamic}\label{dynamic}}

The Information Dynamic of Music (IDyOM) model of Marcus Pearce is a computational model of auditory cognition \citep{pearceStatisticalLearningProbabilistic2018a}.
IDyOM is based on the assumption put forward by Leonard Meyer that musical style can be understood as a complex network of probabilistic relationships that underlie a musical style and is implicitly understood by a musical community \citep{pearceAuditoryExpectationInformation2012, pearceConstructionEvaluationStatistical2005, pearceStatisticalLearningProbabilistic2018a} that incorporates a multiple viewpoint framework \citep{conklinMultipleViewpointSystems1995}.
Unlike measures from FANTASTIC, which calculate summary statistics based on melodic features, IDyOM works by calculating measures of expectancy of an event based on a predefined set of musical parameters that the model was trained on.
As mentioned in Chapter 2, the IDyOM model relies on two important theoretical assumptions based on two neural mechanisms involved in musical enculturation: the statistical learning hypothesis and probabilistic prediction hypothesis.
According to Pearce, the Statistical Learning Hypothesis (SLH) states that:

\begin{quote}
musical enculturation is a process of implicit statistical learning in which listeners progressively acquire internal models of the statistical and structural regularities present in the musical styles to which they are exposed, over short (e.g., an individual piece of music) and long time scales (e.g., an entire lifetime of listening). (Pearce, 2018)
\end{quote}

The logic here is that the more an individual is exposed to a musical style, the more they will implicitly understand its internal syntax and rules.
The SLH leads the corroborating probabilistic prediction hypothesis which Pearce states as:

\begin{quote}
while listening to new music, an enculturated listener applies models learned via the SLH to generate probabilistic predictions that enable them to organize and process their mental representations of the music and generate culturally appropriate responses. (Pearce, 2018).
\end{quote}

IDyOM works by providing the model with a musical corpus that it assumes is representative of a genre or musical style.
This musical corpus then serves as training data to approximate either a listener's ground truth or musical style.
After establishing this corpus, IDyOM then learns both long term and short term expectations of events using a variable-order Markov model in order to best optimize its predictive abilities in line with theoretical frameworks provided by \citet{conklinMultipleViewpointSystems1995}.
The expectations that IDyOM calculates are based on a probability distribution of the proceeding events, which is then quantified in terms of information content \citep{shannonMathematicalTheoryCommunication1948}.
As detailed in a summary review article on IDyOM by Pearce, IDyOM has been successful at predicting

\begin{quote}
Western listeners' melodic pitch expectations in behavioral,
physiological, and electroencephalography (EEG) studies using a range of experimental designs, including the probe-tone paradigm visually guided probe-tone paradigm a gambling paradigm, continuous expectedness ratings, and an implicit reaction-time task to judgments of timbral change.
\end{quote}

Peace notes some of IDyOM successes in modeling beyond expectation, including successes in modeling emotional experiences in music, recognition memory, perceptual similarity, phrase boundary perception and metrical inference.
Importantly in reviewing IDyOM's capabilities regarding memory for musical pitches, Pearce also claims that

\begin{quote}
A sequence with low IC is predictable and thus does not need to be encoded in full, since the predictable portion can be reconstructed with an appropriate predictive model; the sequence is compressible and can be stored efficiently. Conversely, an unpredictable sequence with high IC is less compressible and requires more memory for storage. Therefore, there are theoretical grounds for using IDyOM as a model of musical memory.
\end{quote}

Peace notes four studies \citep{bartlettRecognitionTransposedMelodies1980, cohenRecognitionTransposedTone1977, cuddyMusicalPatternRecognition1981, halpernAgingExperienceRecognition1995} that show that more complex melodies are more difficult to hold in memory.
This theoretical assertion and select empirical findings have important ramifications for the aural skills classroom.
In a dictation setting, melodies that are more expected should tax memory less, thus making them easier to remember and dictate.
If I assume that more expected melodies are easier to remember, then it follows that the information content measures of expectedness can then be used as a stand in measure of melodic memory.
This notion is not new to music psychology and was discussed by David Huron relating exposure to musical material as following similar laws to the the Hick-Hyman hypothesis \citep{hickRateGainInformation1952, hymanStimulusInformationDeterminant1953}, which Huron paraphrases as ``processing of familiar stimuli is faster than processing of unfamiliar stimuli'' \citep[p.~63]{huronSweetAnticipation2006}.
Now a decade later, this assertion can be further investigated using tools from computational musicology.
Combining the Hick-Hyman hypothesis together with the above statistical learning hypothesis and probabilistic prediction hypothesis, I then put forward a new hypothesis: the frequency facilitation hypothesis.

\hypertarget{frequency-facilitation-hypothesis}{%
\section{Frequency Facilitation Hypothesis}\label{frequency-facilitation-hypothesis}}

The Frequency Facilitation Hypothesis (FFH) makes two important assumptions that rely on both the statistical learning hypothesis and the perceptual facilitation hypothesis.
The first, as stated above, is that humans learn melodies via the statistical learning hypothesis.
In line with Huron's reading of the Hick-Hyman Law, melodic information that listeners are more familiar with will consequently be processed more quickly.
More expected notes will tax memory processing less than unexpected notes.
This assertion would also be predicted by the probabilistic prediction hypothesis.
Thus, given a sequence any set of notes, the frequency facilitation hypothesis posits that the efficiency in which a melody is processed in memory is proportionally related to its degree of expectedness when quantified in information content.
Specifically, measures of expectation derived from computational models of auditory cognition like IDyOM should be able to serve as a proxy for musical information.
This falls within the bounds of Pearce's assertion that using the expectancy measures from a melody could be used as a sort of memory proxy \citep{pearceStatisticalLearningProbabilistic2018a}.

The Frequency Facilitation Hypothesis generates testable predictions that can be investigated to verify its verisimilitude.
Important to aural skills pedagogy, the primary prediction from this hypothesis would be that melodic patterns that occur more frequently in a corpus will be be easier to remember than those occurring less frequently.
These frequency patterns should then directly relate to the amount of information content calculated by IDyOM.
If this relationship does exist, then it can be used to develop strategies that would then create a more linear path to success for students learning to take melodic dictation.
In the final section of this chapter, I investigate this claim by conducting an analysis on a corpus of sight singing melodies to demonstrate this claim.
I then take the findings from this corpus analysis and how it can be applied in the aural skills classroom.

\hypertarget{corpus-analysis}{%
\subsection{Corpus Analysis}\label{corpus-analysis}}

In order to investigate the Frequency Facilitation Hypothesis, I conducted a corpus study using N = 622 melodies from the above using the Fifth Edition of the Berkowitz \emph{A New Approach to Sight Singing} \citep{berkowitzNewApproachSight2011}.
The FFH predicts that more frequently occurring patterns will result in lower information content--- a general by-product of quantifying musical feature tokens with information content in a multiple viewpoints framework--- and that these lower information content measures, when quantified, will be able to predict load on memory.

In order to examine this, I first extracted a series of the most frequently occurring melodic tri-grams from a subset of the \emph{MeloSol} corpus after transposing each melody to C major via the \texttt{solfa} tool in humdrum.
I plot the resulting distributions of the top 1,000 patterns of each fixed order predictions below in \ref{fig:bitriquint} and \ref{fig:bitriquint2}.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/trigrams} 

}

\caption{Distribution of n-grams}\label{fig:bitriquint}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/bitriquint} 

}

\caption{Distribution of Bi- Tri- and Quint- grams}\label{fig:bitriquint2}
\end{figure}

From Figure \ref{fig:bitriquint}, we see that when plotted in terms of their frequency distributions, a small amount of the patterns make up for a very large the distribution of the corpus.
As evident from Figure \ref{fig:bitriquint}, we see that with the addition of more tokens added to the n-grams, this results in a visual representation of why and how statistical predictions become more unreliable with higher order predictions \citep{conklinMultipleViewpointSystems1995}.
Intuitively, melodic patterns from the high frequency distribution of the table would seemingly be easier to remember and then dictate than those from the tails of the distributions.

Following up on this analysis, I then trained an IDyOM model on the same corpus of melodies and was able to calculate the average information content for the first five notes of each melody in the corpus.
In this computation, I explicitly assume that the underlying corpus of data is representative of an individual's personal expectations of musical material.
In Figure \ref{fig:quintdist}, I visualize the cumulative information content of the first five notes of each of melodies that the corpus was trained on.
I chose to additionally split the corpus into quintiles to further highlight the progressive changes in information content when these incipits are sorted based on their cumulative information content.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/five_incipit_distiribution} 

}

\caption{Distribution of Opening Quint grams}\label{fig:quintdist}
\end{figure}

This observation may seem tautological, as this relationship would result from how information content is calculated since more expected patterns have less information content.
The novel assertion here is connecting the cumulative information content to memory load.
For example, in Figure \ref{fig:tridensity}, we see that when split into three sections, even just the opening of the first five notes of each melody increase per group.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/tri_distribution} 

}

\caption{Average Information Content of Opening Quint-grams of Melodies}\label{fig:tridensity}
\end{figure}

To visualize what this might look like in a melodic dictation context, we could imagine randomly sampling melodies from even smaller sections.
If quantified using information content measures, these five grams would then fill up the finite bin of memory faster than five grams that were more unexpected, or had more information content associated with them.
I visualize this difference in Figure \ref{fig:cumplot} where I plot similar lengths of five grams filling up the window of memory at different rates based on their cumulative information content.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/cum_grid_plot} 

}

\caption{Cumulative Information Content in Melodic Incipts}\label{fig:cumplot}
\end{figure}

Lastly, to further investigate this claim of cumulative information content, I calculated various information content measures for each melody used the survey above then using the resulting data to plot it against the measures of expert ratings of difficulty for the classroom.
In Figure \ref{fig:modelcomparison}, the resulting visualization shows measures of information content to be very good predictors of difficulty ratings.
I believe that this provides evidence for using computational measures in designing appropriate curricular measures.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/model_comparsion_gg} 

}

\caption{Model Comparison}\label{fig:modelcomparison}
\end{figure}

\hypertarget{implications}{%
\subsection{Implications}\label{implications}}

If true, the Frequency Facilitation Hypothesis would have direct implications for the aural skills classroom, specifically for melodic dictation.
If measures of information content could be used as a more reliable proxy for load on memory, then a more linear path to learning patterns could result in better strategies for learning to take dictation.
For example, the first practical application of this could be that information content could be a more accurate proxy for the limits of musical memory as opposed to using older measures asserted by the literature that follow in the George Miller 7 \(\pm\) 2 tradition, which attempts to logically substitute items in memory for musical notes.
As discussed in the previous section \protect\hyperlink{working-memory-and-melodic-dictation}{Working Memory and Melodic Dictation}, using measures of information content could provide an ecologically acceptable solution to the problems of chunking within music.
For example, if this measure proved to be useful in pedagogical applications, pedagogues would have a very powerful tool to create curricula that was designed in a much more linear path to help students learn.

One of the major challenges in both teaching and learning aural skills beyond the identification of scale degrees is identifying them in a more ecological, melodic context.
Presenting incipits of melodies could be then used as a very small intermediate step in teaching melodic dictation where students can experience more frequent successes in the aural skills classroom while trying to dictate progressively difficulty snippets.
If they learn the more frequent n-grams first, they will find them easier, but more importantly will begin to recognize these patterns in longer exercises.

Instead of picking melodies for practice one-by-one, pedagogues could instead give students a large compendium of small dictation exercises that were ordered to increase in their melodic information content over the course of instruction.
In this type of application, students would not be learning to increase their melodic information capacity limit per se, but could provide a valuable means to give students multiple, smaller attempts to learn to take dictation, rather than being overwhelmed with longer melodies that are given to study on the premise of more ecological validity.
This could be done from the level of scale degree identification to that of full melodies.
Future work should investigate this experimentally and look to model it using similar methodologies that have been employed in music psychology testing paradigms \citep{harrisonApplyingModernPsychometric2017a, wolfGradesReflectDevelopment2014}.
Finally, if useful, this type of modeling could also be used in future computational models of melodic dictation as explored in the final chapter of this dissertation.

\hypertarget{limitations-of-frequency-facilitation-hypothesis}{%
\subsection{Limitations of Frequency Facilitation Hypothesis}\label{limitations-of-frequency-facilitation-hypothesis}}

This conceptualization of calculating the information content of melodies is not without its limitations.
One of the core assumptions to this approach is that statistical learning does in fact take place.
While this assumption is ubiquitous in much of the music psychology literature, statistical learning as a concept has been critiqued in other related fields and deserves mentioning.
Statistical learning rests on the premise that organisms are able to implicitly learn and track the statistical regularities in their environments.
In the case of auditory learning, there is research to assert this claim from both the field of implicit learning and statistical learning as discussed by \citet{perruchetImplicitLearningStatistical2006}.
For example, extensive evidence as reviewed by \citet{cleeremansComputationalModelsImplicit2008} provides many examples of implicit learning.
One point especially worth highlighting is that people have been shown to learn variable order n-gram patterns \citep{remillardImplicitLearningFirst2001}.

This assertion is importantly contrasted by work such as \citet{jamiesonApplyingExemplarModel2009} who claim that explaining these phenomena as resulting in statistical learning is not necessary.
Rather, \citet{jamiesonApplyingExemplarModel2009} assert that employing memory models like that of Minerva 2 can accurately model behavioral patterns in individual responses without the theoretical framework of statistical learning.
They instead note that similar results can be obtained from individuals making similarity judgments.
This assertion is important to highlight because as noted by \citet{perruchetImplicitLearningStatistical2006}, statistical learning depends on the tacit assumption that people might be performing some sort of real-time calculations on incoming stimuli in real time.
Another important caveat in the corpus analysis above is it was done using fixed order search patterns, whereas the calculations from IDyOM are based on variable order Markov-Models.

\hypertarget{conclusions-1}{%
\section{Conclusions}\label{conclusions-1}}

In this chapter, I demonstrated how tools from computational musicology can be used as an aide in aural skills pedagogy.
After first establishing the extent to which aural skills pedagogues agree on various melody parameters, I then show how two types of computationally derived features can stand in for a pedagogue's intuition.
First, using the FANTASTIC toolbox, I show how static abstracted features can help explain how theorists conceptualize complexity.
This first will help with selection of melodies and also provides insights as to which features of the melodies contribute most to perceived difficulty.
Second, I demonstrated how assumptions derived from the IDyOM framework can serve as a basis for the intuitions of why smaller sequences of notes within melodies are more or less difficult to dictate.
Using the logic that sequences that are easier to process are more expected and thus could plausibly tax memory less than are more difficult to process.
I relate this to the classroom by asserting that students could then take a melodic incipt approach to learning to dictate.
I argued that using this smaller incipit based approach will allow students to not be overwhelmed in their learning by taking a more linear path to dictation, before moving on to more more ecologically valid melodies.

\clearpage

\hypertarget{chapterfour}{%
\chapter{Hello, Corpus}\label{chapterfour}}

\hypertarget{rationale-3}{%
\section{Rationale}\label{rationale-3}}

One of the essential features of any scientific discovery is the ability to reproduce the finding.
Given a new claim about reality, in order to be able to demonstrate that the claim is true, the new phenomenon should remain invariant when reproduced.
If the phenomenon satisfies pre-established criteria for causality, this evidence can be used to corroborate its generating theories.
This type of rationale is often associated with scientific methodologies and needs to be adopted here as many questions in music research are better suited for these methods.
As noted by scholars like Allen Forte, ``In virtually any historic period one finds an interaction between music and science and mathematics'' \citep{forteMusicComputingPresent1967}.
Music was one of the seven liberal arts belonging to the quadrivium along with astronomy, geometry, and arithmetic.
In fact, many disciplinary differences in musical study more likely to result from geopolitical divides as to how scholars conceptualize the study of music based on their location, rather than the content and form of their research \citep{parncuttSystematicMusicologyHistory2007}.
It should then come as no surprise that studies in music will often interface with diverse methodologies.

Returning to a phenomenon's invariance under different conditions, one of the most effective ways to investigate claims about the state of reality is to reproduce previously made claims using new data.
One contribution that a researcher can make towards either bolstering or refuting claims and their resulting theories would be to generate more materials in which to examine previous claims under new conditions.
In order to accomplish this, in this chapter I introduce a new corpus of sight-singing melodies based on the pedagogical text \emph{A New Approach to Sight Singing} \citep{berkowitzNewApproachSight2011}.
The corpus contains 783 monophonic melodies that have been digitally encoded in the kern format \citep{huronHumdrumToolkitReference1994} and contain both melodies specifically composed for use in the Aural Skills classroom and examples of melodies from the Western canon.
Due to the fact that the corpus contains melodic data from a sight-singing anthology first published by Sol Berkowitz, for ease of reference I will refer to this corpus as the \emph{MeloSol} corpus.
After introducing the corpus, I compare the \emph{MeloSol} corpus with the \emph{Essen Folk Song Collection} \citep{schaffrathEssenFolkSong1995} as well as a portion of the \emph{Densmore} collection \citep{shanahanDensmoreCollectionNative2014} in order to highlight variability between these musical corpora.
I end by highlighting important considerations in the underlying representations of what the data represent and what these assumptions entail for future work in computational musicology.

\hypertarget{history}{%
\section{History}\label{history}}

The use of computers to study music has been been ongoing for over the past fifty years.
As reviewed by \citet{hewlettComputingMusicology1991}, early approaches to using computers in music research began in the mid 1960s and due to the high effort and cost of computation, projects pursued by researchers at this time tended to focus on questions that might have global relevance.
The use of computers to study music at this time was not by any means an uncommon area of study and throughout the second half of the 20th century, research in computational musicology grew in relation to the computing abilities afforded by the available technologies \citep{nettheimBibliographyStatisticalApplications1997}.
During this time, not only was there progress made on computing power, but during this time many new digital music encoding frameworks were created.
As discussed by \citet{wigginsFrameworkEvaluationMusic1993}, the design and development of these encoding frameworks has impact on the degree that the systems can be assessed.
According to Wiggins and colleagues, a musical, digital encoding framework can be evaluated on the two orthogonal dimensions of expressive completeness and structural generality.
Considering how a system is developed in order to encode encode musical information then becomes paramount given that the level of granularity of encoding data will determine the types of questions that could eventually be asked in a computational analysis.
For example, data encoded in MIDI or CHARM format is able to store micro-time variations in performance practice, which lends itself to the ability to do performance based analyses on this data.
If this data were instead to have been encoded using a frequency spectrum as would be stored in an MP3 or WAV file, this type of analysis could not be carried out as accurately due to the task of automating the detection of pitch onsets.

On a higher level of abstraction, this problem of how to encode a melody becomes exacerbated when considering meta-research issues such as the the tools-to-theories heuristic put forward by Gigerenzer \citep{gigerenzerToolsTheoriesHeuristic1991}.
Gigerenzer claims that much of both the novelty and authority given to the trajectory of a research path is determined by the tools a group decides is valid and not the generation of new data or theories.
Contextualizing this problem for digital music encoding, again choosing how to represent the data reflects ontological and epistemological assumptions about the data itself.
Not only does committing to an encoding system come with the inevitable elimination of important musical features, but over time the establishing of canonical assumptions about the nature of methods might lead to researchers choosing questions and methods based on the convenience of answering those questions, rather than commitment to the question itself.
This type of problem would only be exacerbated in high pressure, performance based research environments.
Further, the technology used to be able to query or test this data would provide an additional constraint on the analysis.

Currently there is a large amount of variability in types of encoding available as well as tools that can be used for computer based analysis including music21 \citep{cuthbertMusic21ToolkitComputerAided2010} and David Huron's Humdrum \citep{huronHumdrumToolkitReference1994} toolbox.
While there are differences in the advantages between various types of encoding and tools used to analyze this data, parsers such as the MeloSpySuite are constantly being developed to serve as digital music's Rosetta stone, resulting in a current eco-system that allows for moving between encoding formats \citep{frielerIntroducingJazzomatProject2013}.

While many of the encoding formats throughout the past 50 years have fallen out of favor, the kern format of encoding data developed by David Huron has persisted as a choice for many computational musicologists since its initial development in 1994.
The kern format (often stylized as \texttt{**kern}) was developed in tandem with the Humdrum Toolbox for music analysis that according to Humdrum user guide \citep{huronHumdrumToolkitReference1994} is

\begin{quote}
a set of command-line tools that facilitates musical analysis, as well as a generalized syntax for representing sequential streams of data. Because it's a set of command-line tools, it's program-language agnostic. Many have employed Humdrum tools in larger scripts that use PERL, Ruby, Python, Bash, LISP, and C++.
\end{quote}

Humdrum files, unlike that of anything used in MEI are human readable and non-hierarchical, thus mirroring Western notated music's sequential nature.
Because of this, editing kern files using the humdrum tool set and humdrum extras developed by Craig Sapp \citep{sappHumdrumExtras2008} can be done with short, UNIX scripts as opposed to similar analyses in music21.
Since moving between digitally encoded ecosystems is not nearly as difficult and much of encoding is can be left to the jurisdiction of the researcher, I have chosen to encode this data set using the kern format.

\hypertarget{melosol-corpus}{%
\section{MeloSol Corpus}\label{melosol-corpus}}

In this next section, I introduce a new corpus of melodies encoded in the kern format.
The melodies come from the 5th edition of \emph{A New Approach to Sight Singing} written by Sol Berkowitz, Gabriel Fontrier, Leo Kraft, Perry Goldstein, and Edward Smaldone, \citep{berkowitzNewApproachSight2011}.
This corpus includes 783 melodies from the first and last chapters of the book.
The first chapter contains melodies from five different sections and the fifth chapter contains ``Melodies from the Literature'' and is made up of four sections.
Melodies from the first chapter have all been specifically composed for use in sight-singing contexts.
Melodies from the fifth chapter are small excerpts from examples of both excerpts from Western Classical Music canon and traditional folk songs of various countries.
Some excerpts from the literature have been slightly modified for singablilty.

The information for each melody is recorded in the meta-data of the kern file.
In addition to having a key signature in each kern file, I have also added an explicit key to each kern file.
Each section of the book contains melodies that would be considered tonal, except for melodies in the fifth section of the first chapter and intermittent melodies in the fourth section of the fifth chapter which contain atonal melodies.
If a melody is decidedly atonal or modal, this is documented in the metadata.
Atonal melodies are given the explicit key of C major so that they can be analyzed and parsed as if they were part of a \emph{fixed-do} system.
This encoding decision is reflected in the Key Distribution panel of Figure \ref{fig:melosoldescriptivepanel}.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/melosoldescriptpanel} 

}

\caption{Descriptive Statistics of MeloSol Corpus}\label{fig:melosoldescriptivepanel}
\end{figure}

Figure \ref{fig:melosoldescriptivepanel} shows basic descriptive statistics of the \emph{MeloSol} corpus.
The figure highlights general features of the corpus that might be of interest to future researchers.
In sum, the corpus represents 783 unique melodies comprising 49,730 data tokens.
Of these 49,730 tokens, 36,641 are currently kern interpretable using the humdrum toolbox.
The dataset also exists in a MIDI, csv, and xml format for analysis with other tool sets.
All data was manually encoded via MuseScore then converted and cleaned using the humdrum extras toolkit \citep{sappHumdrumExtras2008, wernerMuseScore2019}.

\hypertarget{comparison-of-corpora}{%
\section{Comparison of Corpora}\label{comparison-of-corpora}}

In order to give a brief overview of the corpus and contextualize it in the context of other corpora, in this next section I compare the \emph{MeloSol} corpus with the \emph{Essen Folk Song Collection} \citep{schaffrathEssenFolkSong1995}, as well as the \emph{Densmore} collection \citep{shanahanDensmoreCollectionNative2014} and provide a brief corpus analysis.
All three corpora here contain vocal melodies.
The Berkowitz corpus was specifically designed for pedagogical purposes, whereas the \emph{Essen} and \emph{Densmore} are more ecologically reflective of melodies originating from a diversity of sources.
Given that these corpora consist of vocal melodies, there presumably would be differences between the corpora on a large scale structure.
I then further investigate differences at the group level by investigating melodies of Asian origin from the \emph{Essen} collection and those of Native American from the \emph{Densmore}.
These groupings reflect differences in geographic location and are not taken to be reflective of a cultural aggregate.

Another important reason for comparing these corpora is that the \emph{Essen} is one of the most heavily cited corpora in the field of computational musicology and is often taken as a proxy to represent the underlying expectational structure of Western music.
Much of the research that assumes this makes claims about general level musical features such as the melodic arch \citep{huronMelodicArchWestern1996, shanahanExaminingEffectOral2019} or that the implicitly learned patterns of a musical style can be represented using a corpus of digitized melodies \citep{demorestQuantifyingCulture2015, pearceStatisticalLearningProbabilistic2018a}.
In this context, the underlying assumption in this inference is that a corpus--- in this case a folk song collection--- is a sample of the larger population of Western music.
This assumption tacitly borrows the underlying logic from the Frequentist schools of statistical thought \citep{dienesUnderstandingPsychologyScience2008}; the corpus is taken to be a sample of the population.
This assumption is furthered when analyses are done using the null hypothesis significance testing framework.

If researchers adopt this underlying assumption, it should follow that in order to continually find support for these theories and hypotheses, new evidence should be put forward that uses a similar population, but with different samples.
Doing so would require the creation of new samples from a parent population, very much akin to the \emph{MeloSol} corpus.
Like the \emph{Essen}, the \emph{MeloSol} corpus contains melodies in the Western, tonal tradition constrained by vocal performance.
Alternatively, researchers could adopt different research epistemologies other than a general Frequentist approach, such as using Bayesian or Likelihood methods that do not assume a sample-population relationship, but rather take the data as the model itself.
Regardless of what methodology is chosen, providing more evidence for previous claims depends on, as noted above, finding new evidence for old claims with new data.

\hypertarget{corpus-analysis-1}{%
\subsection{Corpus Analysis}\label{corpus-analysis-1}}

In order to compare the \emph{Essen} collection with the \emph{MeloSol} corpus, I first plot general level descriptive features of all corpora in Figure \ref{fig:compdesc}.
Creating these visualizations demonstrates size differences between the corpora.
As noted in the bottom right panel of \ref{fig:compdesc}, then \emph{Essen} collection is much larger than that of the \emph{MeloSol} or the \emph{Densmore} collection.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/comparative_descritivepanel} 

}

\caption{Descriptive Features of MeloSol Corpus}\label{fig:compdesc}
\end{figure}

From the above panels, it appears that the \emph{MeloSol} corpus has much more variability in the range or tessitura of melodies.
This difference is most likely reflective of the nature of the \emph{MeloSol} melodies which were composed for didactic use, and thus were the product of composition with a notational system.
Interestingly, both sets from the \emph{Essen} Collection tend to have much more defined peaks.
Though a post-hoc interpretation, these peaks might serve as the basis for a study on physical affordances drawing together work on melody transmission \citep{shanahanExaminingEffectOral2019} and the cognitive affordance provided via notation \citep{lerdahlCognitiveConstraintsCompositional1992}.
Melodies between the four data sets also tend to have overlapping density distributions in terms of both length and note density.

Secondly, I then overlay emergent properties from the corpora--- standardized for size--- using density plots.
The underlying logic in the following exploratory analysis would be if the \emph{MeloSol} and European subsets of the \emph{Essen} are samples of large subset of properties found in Western music, there should be some degree of overlap between the emergent elements.
From the panels below, the key comparisons will be to look between the blue and yellow distributions.
Interestingly, looking at the panels plotting Tonalness as well as Tonal Spike, two measures of tonality derived from the FANTASTIC toolbox used to complete this analysis, the underlying distributions tend to follow a similar distribution
Inspecting some of the features further, the Densmore collection shows a marked departure in interval entropy from the other three distributions and also shows more variability in terms of contour variation as calculated by the FANTASTIC stepwise.contour.global.variation metric \citep{mullensiefenFantasticFeatureANalysis2009}.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/corporaemergent} 

}

\caption{Emergent Features}\label{fig:compemergent}
\end{figure}

The addition of the \emph{MeloSol} corpus also provides an opportunity to investigate and replicate other claims made in the musicological literature.
For example, in Figure \ref{fig:comphuron}, I have reproduced the first level analysis David Huron puts forward in \citep{huronMelodicArchWestern1996}.
From the figure, there appears to be similar patterns between the European subset of the \emph{Essen} collection and that of the \emph{MeloSol} corpus.
The most prominent phrase type in both corpora is the convex contour, followed secondly by the descending contour pattern.
Future versions of the \emph{MeloSol} corpus could be used to add phrase marks and examine the extent to which Huron's claims hold in a categorically different, yet grammatically similar corpus.
Lastly, in Figure \ref{fig:krum}, I plot standardized key profiles for the \emph{Essen} and \emph{MeloSol} corpora as presented in this chapter \citep{krumhanslCognitiveFoundationsMusical2001}.
The \emph{Densmore} collection is not included here as it does not come with explicit key data.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/huronrecreation} 

}

\caption{Replication of Analysis 1, Huron 1994}\label{fig:comphuron}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/krumpanel} 

}

\caption{Tonal Hierarchy of Corpora}\label{fig:krum}
\end{figure}

An overall view shows that the three corpora exhibit relatively similar distribution profiles.
As with previous research, the tonic and dominant scale degree occur most frequently.
There appears to be a general lack of scale degree four and seven in the Asian subset of the \emph{Essen} and a large degree of the supertonic.
There is also a large amount of the sixth scale degree here, a topic addressed by \citet{brinkmanLeadingSixthScale2018}, though in the context of European music.
As a corpus, the \emph{MeloSol} corpus shows a high percentage of the leading tone, a musical feature synonymous with Western classical music.
Inspecting the chromatic aggregate, the \emph{MeloSol} corpus also has the highest representation of all scale degree sevens.
Overall, finding similar distributional patterns in scale degrees with a new corpus provides further support of the stability of the existence of tone distribution profiles.

\hypertarget{discussion-1}{%
\subsection{Discussion}\label{discussion-1}}

Surveying how the \emph{MeloSol} corpus compares to that of the \emph{Essen} and \emph{Densmore} Folk song collections, I've demonstrated various ways to investigate what properties of music remain invariant under different analyses.
While the \emph{MeloSol} corpus does not exactly reflect the global level parameters of that of the European subset of the \emph{Essen} Collection, there appears to be evidence that some properties are the same.
Thinking about this problem begs the question if the computational musicology community does assume a sample-population relationship between corpus and population.
If this is true, considering how to reproduce findings in a meaningful way is important for the health of the field.
If true, the community would be able to continue doing analyses such as that of \citet{huronMelodicArchWestern1996}, but then needs to consider such a claim could replicate.

Huron later addressed this issue \citep{huronVirtuousVexatiousAge2013} suggesting that the proliferation of ``Big Data'' will eventually lead computational musicology to an ``ironic'' state where statistical inference tools are no longer applicable because researchers will have access to an entire population.
This assertion again implicitly assumes the Frequentist epistemological framework of samples and population, but Frequentism is not the only framework available to the empirical research community \citep{dienesUnderstandingPsychologyScience2008}.
It is my assertion that these assumptions are yet to be made explicit in computational music analyses.

For example, studies such as \citet{frielerTellingStoryDramaturgy2016} fit a series of models on solos from a jazz corpus.
The solos are taken to represent the larger population of jazz and the authors further subdivide the jazz into its various genre divisions such as post-bop, traditional, and cool.
While jazz might be the population and the genre divisions to be sub-populations, each solo is then further sampled from an individual.
In this case, many of the individuals represented in their corpus are deceased, thus theoretically making the population exhaustive and theoretically accessible.
At this point, practically replicating these results would either depend on finding undiscovered archive recordings or the generation of new material based on estimating parameter values of a style in order to recreate new stimuli following in the example of the music generation literature.
For example, \citet{sturmTakingModelsBack2017} developed an artificial intelligence using deep learning to create folk songs based on 30,000 transcriptions.
As discussed in follow up work \citep{sturmMachineLearningResearch2019}, this research brings with it implications and assumptions about the state of a musical style.
If a population is limited by time, presumably all data will follow the path that Huron predicted and lead us to an ``ironic'' state of population hermeneutics.
If populations are not bound by time, the field of computational musicology needs to consider adopting other frameworks to situate itself.
Further, future work might consider if accessing the population parameters is the end research goal in question.
Questions of style analysis seek to generalize beyond a population so new works might created.
Regardless of the choice, future work from this should make this clear.

\hypertarget{conclusions-2}{%
\section{Conclusions}\label{conclusions-2}}

In this, chapter I presented the \emph{MeloSol} corpus, new database of monophonic singing melodies.
Comparing the corpus to others used in the literature, I demonstrated how the \emph{MeloSol} corpus might be used for future research.
Throughout the chapter, I additionally described how the use of corpora in computational musicology often, though not directly, adopts the assumptions of a sample, population relationship.

\clearpage

\hypertarget{experiment}{%
\chapter{Experiment}\label{experiment}}

\hypertarget{rationale-4}{%
\section{Rationale}\label{rationale-4}}

Using experiments to understand factors that contribute to an individual's ability to remember melodic material are by no means new \citep{ortmannTonalDeterminantsMelodic1933}.
This is not a simple problem as noted in the section \protect\hyperlink{intro}{Context, Literature, and Rationale}, as both individual differences as well as musical features are difficult to quantify and subsequently model.
Capturing variability at both the individual and item level is not only riddled with measurement problems, but this variability problem is exacerbated when realizing many of the statistical ramifications of measuring so many variables in a single experiment.
Many variables leads to many tests, which leads to inflated type I error rates, as well as massive resources needed in order to detect even small effects.

Fortunately, dealing with high levels of variability at both the individual and item level is not a problem exclusive to work on melodic dictation.
Work from field of linguistics has developed more sophisticated methodologies that are able to accommodate the above challenges and provide a more elegant way of handling these types of problems \citep{baayenMixedeffectsModelingCrossed2008}.
In this chapter, I synthesize work from the previous chapters of this dissertation in an experiment investigating melodic dictation.
Unlike work in the past literature, I take advantage of statistical methodologies that are able to better accommodate problems in experimental design using paradigms that accommodate for both individual and item level differences.
By using mixed effects modeling, I put forward a more principled way of modeling data that more ecologically reflects melodic dictation.
I show how it is possible to combine both tests of individual ability and as well as musical features in order to predict performance.
Additionally, I discuss the intricacies associated with scoring and relate these practices back to the classroom.

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

Despite its near ubiquity in Conservatory and School of Music curricula, research surrounding topics concerning aural skills is not well understood.
This is peculiar since almost any individual seeking to earn a degree in music usually must enroll in multiple aural skills classes that cover a wide array of topics from sight-singing melodies, to melodic and harmonic dictation-- all of which are presumed to be fundamental to any musician's formal training.
Skills acquired in these classes are meant to hone the musician's ear and enable them not only to think about music, but to borrow Karpinski's phrase, to ``think in music'' \citetext{\citealp[p.4]{karpinskiAuralSkillsAcquisition2000}; \citealp{bestMusicCurriculaFuture1992}}.
The tacit assumption behind these tasks is that once one learns to think in music, these abilities should transfer to other aspects of the musician's playing in a deep and profound way.
The skills that make up an individual's aural skills encompass many abilities, and are thought to be reflective of some sort of core skill.
This logic is evident in early attempts to model performance in aural skills classes where \citet{harrisonEffectsMusicalAptitude1994} created a latent variable model to predict an individual's success in aural skills classes based on musical aptitude, musical experience, motivation, and academic ability.
While their model was able to predict a large amount of variance (73\%), modeling at this high, conceptual level does not provide any sort of specific insights into the mental processes that are required for completing aural skills related tasks.
This trend can also be seen in more recent research that has explored the relationship between how well entrance exams at the university level are able to predict success later on in the degree program.

\citet{wolfGradesReflectDevelopment2014} noted multiple confounds in their study attempting to assess ability level in university musicians such as inflated grading, which led to ceiling effects, as well as a broad lack of consistency in how schools are assessing the success of their students.
But even if the results at the larger level were to be clearer, this again says nothing about the processes that contribute to tasks like melodic dictation.
Rather than taking a bird's eye view of the subject, this chapter will primarily focus on descriptive factors that might contribute to an individual's ability dictate a melody.

Melodic dictation is one of the central activities in an aural skills class.
The activity normally consists of the instructor of the class playing a monophonic melody a limited number of times and the students must use both their \emph{ear}, as well as their understanding of Western Music theory and notation, in order to transcribe the melody without any sort of external reference.
No definitive method is taught across universities, but many schools of thought exist on the topic and a wealth of resources and materials have been suggested that might help students better complete these tasks \citep{berkowitzNewApproachSight2011, clelandDevelopingMusicianshipAural2010, karpinskiManualEarTraining2007, ottmanMusicSightSinging2014}
The lack of consistency could be attributed to the fact that there are so many variables at play during this process.
Prior to listening, the student needs to have an understanding of Western music notation at least to the level of understanding the melody being played.
This understanding must to be readily accessible, since as new musical information is heard, it is the student's responsibility, in that moment, to essentially follow the Karpinski model and encode the melody in short term memory or pattern-match to long term memory \citep{ouraConstructingRepresentationMelody1991a} so that they can identify what they are hearing and transcribe it moments later into Western notation \citep{karpinskiAuralSkillsAcquisition2000, karpinskiModelMusicPerception1990}.
Regardless, performing some sort of aural skills task requires both long term memory and knowledge for comprehension, as well as the ability to actively manipulate differing degrees of complex musical information in real time while concurrently writing it down.

Given the complexity of this task, as well as the difficulty in quantifying attributes of melodies, it is not surprising that scant research exists for describing these tasks.
Fortunately, a fair amount of research exists in related literature which can generate theories and hypotheses explaining how individuals dictate melodies.
Beginning first with factors that are less malleable from person to person would be individual differences in cognitive ability.
While dictating melodies is something that is learned, a growing body of literature suggests that other factors can explain
unique amounts of variance in performance via differences in cognitive ability.
For example, \citet{meinzDeliberatePracticeNecessary2010} found that measures of working memory capacity (WMC)
were able to explain variance in an individual's ability to sight read above and beyond that of sight reading experience and
musical training.
\citet{colleyWorkingMemoryAuditory2017} recently suggested an individual's WMC could also help explain differences beyond musical training in tasks related to tapping along to expressive timing in music.
These issues become more confounded when considering other recent work by \citet{swaminathanRevisitingAssociationMusic2017} that suggests factors such as musical aptitude, when considered in the modeling process, can better explain individual differences in intelligence between musicians and nonmusicians implying that there are selection biases at play within the musical population.
They claim there is a selection bias that ``smarter'' people tend to gravitate towards studying music, which may explain some of the differences in memory thought to be caused by music study \citep{talaminiMusiciansHaveBetter2017}.
Knowing that these cognitive factors can play a role warrants attention from future researchers on controlling for variables that might contribute to this process, but are not directly intuitive and have not been considered in much of the past
research.
This is especially important given recent critique of models that purport to measure cognitive ability but are not grounded in an explanatory theoretical model \citep{kovacsProcessOverlapTheory2016}.

\hypertarget{memory-for-melodies-1}{%
\subsection{Memory for Melodies}\label{memory-for-melodies-1}}

The ability to understand how individuals encode melodies is at the heart of much of the music perception literature.
Largely stemming from the work of Bregman \citep{bregmanAuditorySceneAnalysis2006}, Deutsch and Feroe, \citep{deutschInternalRepresentationPitch1981}, and Dowling's \citep{bartlettRecognitionTransposedMelodies1980, dowlingExpectancyAttentionMelody1990, dowlingPerceptionInterleavedMelodies1973, dowlingScaleContourTwo1978} work on memory for melodies has suggested that both key and contour information play a central role in the perception and memory of novel melodies.
Memory for melodies tends to be much worse than memory for other stimuli, such as pictures or faces, noting that the average area under the ROC curve tends to be at about .7 in many of the studies they reviewed, with .5 meaning chance and 1 being a perfect performance \citep{halpernMemoryMelodies2010}.
Halpern and Bartlett also note that much of the literature on memory for melodies primarily used same-different experimental paradigms to investigate individuals' melodic perception ability similar to the paradigm used in \citep{halpernEffectsTimbreTempo2008}.

\hypertarget{musical-factors-1}{%
\subsection{Musical Factors}\label{musical-factors-1}}

Not nearly as much is known about how an individual learns melodies, especially in dictation settings.
The last, and possibly most obvious, variable that would contribute to an individual's ability to learn and dictate a melody would be the amount of previous exposure to the melody and the complexity of the melody itself.
A fair amount of research from the music education literature examines melodic dictation in a more ecological setting \citep{buonviriEffectsMusicNotation2015, buonviriEffectsPreparatorySinging2015, buonviriEffectsTwoListening2017, buonviriExplorationUndergraduateMusic2014, buonviriMelodicDictationInstruction2015, unsworthAutomatedVersionOperation2005}, but most take a descriptive approach to modeling the results using between-subject manipulations.
Some rules of thumb regarding how many times a melody should be played in a dictation setting have been proposed by Karpinski \citep[p.99]{karpinskiAuralSkillsAcquisition2000} that account for chunking as well as the idea that more exposure would lead to more complete encoding.
For example, he suggests using the formula \(P = (Ch/L) + 1\) where \(P\) is the number of playings, \(Ch\) is the number of chunks in the dictation (with chunk defined as a single memorable unit), and \(L\) = the limit of a listener's short term memory in terms of chunks, a number between 6 and 10.
This definition requires expert selection of what a chunk is, and does not take into account any of the Experimental factors put forward in the taxonomy presented in this dissertation's \protect\hyperlink{intro}{Context, Literature, and Rationale}.

Recently, tools have been developed in the field of computational musicology to help with operationalizing the complexity of melodies.
Both simple and more complex features have been used to model performance in behavioral tasks.
For example \citet{eerolaPerceivedComplexityWestern2006} found that note density, though not consciously apparent to the participants, predicted judgments of human similarity between melodies not familiar to the participants.
Both \citet{harrisonModellingMelodicDiscrimination2016} and \citet{bakerPerceptionLeitmotivesRichard2017} used measures of melodic complexity created from data reductive techniques to successfully predict difficulty on melodic memory tasks.

Note density would be an ideal candidate to investigate because it is both easily measured and the amount of information that can be currently held in memory as measured by bits of information has a long history in cognitive psychology \citep{cowanWorkingMemoryCapacity2005, millerInformationMemory1956, pearceStatisticalLearningProbabilistic2018a}.
In terms of more complex features, much of this work largely stems from the work of MÃ¼llensiefen and his development of the FANTASTIC Toolbox \citeyearpar{mullensiefenFantasticFeatureANalysis2009}, and a few papers have claimed to be able to predict various behavioral outcomes based on the structural characteristics of melodies.
For example, \citep{kopiezAufSucheNach2011} claimed to have been able to predict how well songs from The Beatles' album \emph{Revolver} did on popularity charts based on structural characteristic of the melodies using a data driven approach.
Expanding on an earlier study, \citet{mullensiefenRoleFeaturesContext2014} found that the degree of distinctiveness of a melody when compared to its parent corpus could be used to predict how participants in an old/new memory paradigm were able to recognize melodies.
These abstracted features also have been used in various corpus studies \citep{jakubowskiDissectingEarwormMelodic2017, janssenPredictingVariationFolk2017, rainsfordDistinctivenessEffectRecognition2019, rainsfordMUSOSMUsicSOftware2018}
that again use data driven approaches in order to explain which of the 38 features that FANTASTIC calculates can predict real-world behavior.

While helpful and somewhat explanatory, the problem with either data reductive or data driven approaches to this modeling is that they take a post-hoc approach with the assumption that listeners are even able to abstract and perceive these features.
Doing this does not allow for any sort of controlled approach.
Without experimentally manipulating the parameters, which is then further confounded when using some sort of data reduction technique.
This is understandable seeing as it is very difficult to manipulate certain qualities of a melody without disturbing
other features \citep{taylorStrategiesMemoryShort1983}.
For example, if you wanted to decrease the ``tonalness'' of a melody by adding in a few more chromatic pitches, you inevitably will increase other measures of pitch and interval entropy.
In order to truly understand if these features are driving changes in behaviour, each needs to be altered in some sort of controlled and systematic way while simultaneously considering differences in training and cognitive ability.

In order to accomplish this, I put forward findings from an experiment modeling performance on melodic dictation tasks using both individual and musical features.
A pilot study was run (N=11) in order to assess musical confounds that might be present in modeling melodic dictation.
Results of that pilot study are not reported here.
Based on the results of this pilot data, a follow up experiment was conducted to better investigate the features in question.

The study sought to answer three main hypotheses:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Are all experimental melodies used equally difficult to
  dictate?
\item
  To what extent do the musical features of Note Density
  and Tonalness play a role in difficulty of dictation?
\item
  Do individual factors at the cognitive level play a role
  in the melodic dictation process above and beyond musical
  factors?
\end{enumerate}

\hypertarget{methods-1}{%
\section{Methods}\label{methods-1}}

\hypertarget{participants-1}{%
\subsection{Participants}\label{participants-1}}

Forty-three students enrolled at Louisiana State University School of Music completed the study.
The inclusion criteria in the analysis included reporting no hearing loss, not actively taking medication that would alter cognitive performance, and individuals whose performance on any task performed greater than three standard deviations from the mean score of that task.
Using these criteria, two participants were dropped for not completing the entire experiment.
Thus, 41 participants met the criteria for inclusion.
The eligible participants were between the ages of 17 and 26 (M = 19.81, SD = 1.93; 15 women).
Participants volunteered, received course credit, or were paid \$10.

\hypertarget{materials-1}{%
\subsection{Materials}\label{materials-1}}

Four melodies for the dictation were selected from a corpus of N=115 melodies derived from the A New Approach to Sight Singing aural skills textbook \citep{berkowitzNewApproachSight2011}.
Melodies were chosen based on their musical features as extracted via the FANTASTIC Toolbox \citep{mullensiefenFantasticFeatureANalysis2009}.
After abstracting the full set of features of the melodies, possible melodies were first narrowed down by limiting the corpus to melodies lasting between 9 and 12 seconds and then indexed to select four melodies that were chosen as part of a 2 x 2 repeated measures design including a high and low tonalness and note density condition.
Melodies, as well as a table of their abstracted features can be seen in \ref{tab:expfeaturetable} and \ref{fig:b34}, \ref{fig:b95},\ref{fig:b112}, and \ref{fig:b9} .
Melodies and other sounds used were encoded using MuseScore 2 using the standard piano timbre and all set to a tempo of quarter = 120 beats per minute and adjusted accordingly based on time signature to ensure they all were same absolute time duration.
The experiment was then coded in jsPsych \citep{deleeuwJsPsychJavaScriptLibrary2015} and played through a browser offline with high quality headphones.

\begin{table}[t]

\caption{\label{tab:expfeaturetable}Melodic Features for Experimental Melodies}
\centering
\begin{tabular}{lrrl}
\toprule
Melody Number & Tonalness & Note Density & Feature Category\\
\midrule
34 & 0.95 & 1.67 & High Tonal, Low Note Density\\
112 & 0.98 & 3.73 & High Tonal, High Note Density\\
9 & 0.71 & 1.75 & Low Tonal,  Low Note Density\\
95 & 0.76 & 3.91 & Low Tonal,  High Note Density\\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/Berkowitz34} 

}

\caption{Melody 34}\label{fig:b34}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/Berkowitz95} 

}

\caption{Melody 95}\label{fig:b95}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/Berkowitz112} 

}

\caption{Melody 112}\label{fig:b112}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/BerkowitzNo9} 

}

\caption{Melody 9}\label{fig:b9}
\end{figure}

\hypertarget{procedure-1}{%
\subsection{Procedure}\label{procedure-1}}

Upon arrival, participants sat down in a lab at their own personal computer.
Multiple individuals were tested simultaneously although individually.
Each participant was given a test packet that contained all information needed for the experiment.
After obtaining written consent, participants navigated a series of instructions explaining the nature of the experiment and were given an opportunity to adjust the volume to a comfortable level.
The first portion of the experiment that participants completed was the melodic dictation.
In order to alleviate any anxiety in performance, participants were explicitly told that ``unlike dictations performed in class, they were not expected to get perfect scores on their dictations''.
Each melody was played five times with 20 seconds between hearings and 120 seconds after the last hearing \citep{paneyEffectDirectingAttention2016}.\\
After the dictation portion of the experiment, participants completed a small survey on their Aural Skills background, as well as the Bucknell Auditory Imagery Scale C \citep{halpernDifferencesAuditoryImagery2015}.
After completing the Aural Skills portion of the experiment, participants completed one block of two different tests of working memory capacity \citep{unsworthAutomatedVersionOperation2005} and Raven's Advanced Progressive Matrices, and a Number Series task as two tests of general fluid intelligence (Gf) \citep{ravenManualRavenProgressive1994, thurstonePrimaryMentalAbilities1938} resulting in four total scores.

Exact details of the design can be found in Chapter 3.
After completing the cognitive battery, participants finished the experiment by compiling the self-report version of the Goldsmiths Musical Sophistication Index \citep{mullensiefenMusicalityNonMusiciansIndex2014}, the Short Test of Musical Preferences \citep{rentfrowReMiEveryday2003}, as well as questions pertaining to the participant's socio-economic status, and any other information needed to control for such as hearing loss and use of medication that might affect performance.

\hypertarget{scoring-melodies}{%
\subsection{Scoring Melodies}\label{scoring-melodies}}

Melodies were scored by counting the amount of notes in the melody and multiplying that number by two.
Half the points were attributed to rhythmic accuracy and the other
half to pitch accuracy.
Points were not deducted for notating the melody in the incorrect octave.
Points for pitch could only be given if the participant correctly notated the rhythm.
For example, in Melody 34 in Figure \ref{fig:b34} there were 40 points possible (20 notes * 2).
If a participant was to have put a quarter note on the second beat of the third measure, and have everything else correct, they would have scored a 19/20.
Only if the correct rhythms of the measures were accurate could pitch points be
awarded.
In cases where there were more serious errors, for example if the second half of the second bar was not notated, points would have been deducted in both the pitch and rhythm sub-scores.
Dictations were scored all melodies independently by two raters and then cross referenced for inter rater reliability (\(\kappa\) = .96) which suggests a high degree of inter-rater reliability \citep{kooGuidelineSelectingReporting2016}.

\hypertarget{results-1}{%
\section{Results}\label{results-1}}

\hypertarget{data-screening}{%
\subsection{Data Screening}\label{data-screening}}

Before conducting any analyses, data was screened for quality.
List-wise deletion was used to remove any participants where not all variables were present.
This process resulted in removing four participants: two did not complete any of the survey materials and two did not have any measures of working memory capacity due to computer error.
After list-wise deletion, thirty-nine participants remained.

\hypertarget{modeling-1}{%
\subsection{Modeling}\label{modeling-1}}

In order to model the data and investigate the hypotheses, I fit a series of linear mixed effects models using the R programming language \citep{R-base} and the \emph{lme4} and \emph{LmerTest} \citep{batesFittingLinearMixedEffects2015, lmerTest} packages to predict scores from the dictation exercise using both individual and musical variables.
Variables are added at each step and tests of significant model improvement are presented in Table \ref{tab:modeltable}.
P-values were obtained by likelihood ratio tests between models.
I report AIC and BIC measures for each model ran.
Models were ran sequentially moving from a null model to a theoretical, statistical model predicted by variables discussed above.

After establishing a null model (Model 1, \texttt{null\_model}), I then added individual level predictors (\texttt{wmc\_model} (Model 2) and \texttt{gf\_model}, and \texttt{cognitive\_model}) carrying forward only measures of working memory capacity.
No cognitive variables resulted in a significant increase in model fit.
Next, I modeled musical level features.
Ideally each musical model that I ran would have allowed musical features to have random slopes since presumably individuals would perform differently on each melody.
This was not possible due to the size of the dataset, though future experiments should incorporate this parameter.
The first musical model (Model 3) treated each melody as a categorical fixed effect.
The second musical model (Model 4) used the categorical features as predictors, which included an interaction effect.
The third musical model (Model 5) used the continuous FANTASTIC measures as fixed effects, which included an interaction effect.
In order to investigate claims put forward in the third chapter of this dissertation, I also ran a model (Model 6) using only interval entropy as calculated by FANTASTIC.
Finally, I ran two models that accounted for both musical and individual level predictors.
Model 7 extends Model 6, with the addition of working memory capacity as fixed effect, allowing for random slopes.
Model 8 extends Model 5, with the addition of working memory capacity as a fixed effect, allowing for random slopes.
I depict the fixed effects for all models in \ref{fig:megrid}.
Tables for each model are presented in Tables \ref{tab:memodeltable1} through \ref{tab:memodeltable8}.
Distributions for average scores of the melodies are found in Figure \ref{fig:eboxplot} and density scores of these items are found in Figure \ref{fig:edistribution}.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/melody_difficulty} 

}

\caption{Item Difficulty Distribution}\label{fig:eboxplot}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/melody_differences} 

}

\caption{Between Melody Differences}\label{fig:edistribution}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/me_grid} 

}

\caption{Fixed Effects of Models}\label{fig:megrid}
\end{figure}

\begin{table}[!h]

\caption{\label{tab:memodeltable1}Model 1 | Null Model}
\centering
\begin{tabu} to \linewidth {>{\raggedright}X>{\raggedleft}X>{\raggedright}X>{\raggedright}X}
\toprule
Predictors & Estimates & CI & p\\
\midrule
(Intercept) & 0.35 & 0.30Â âÂ 0.40 & <0.001\\
Marginal R2 & 0.00 &  & \\
Conditional R2 & 0.06 &  & \\
\bottomrule
\end{tabu}
\end{table}

\begin{table}[!h]

\caption{\label{tab:memodeltable2}Model 2 | WMC Model}
\centering
\begin{tabu} to \linewidth {>{\raggedright}X>{\raggedleft}X>{\raggedright}X>{\raggedright}X}
\toprule
Predictors & Estimates & CI & p\\
\midrule
(Intercept) & 0.35 & 0.30Â âÂ 0.40 & <0.001\\
wmc & -0.04 & -0.10Â âÂ 0.03 & 0.272\\
Marginal R2 & 0.01 &  & \\
Conditional R2 & 0.07 &  & \\
\bottomrule
\end{tabu}
\end{table}

\begin{table}[!h]

\caption{\label{tab:memodeltable3}Model 3 | Melody Model}
\centering
\begin{tabu} to \linewidth {>{\raggedright}X>{\raggedleft}X>{\raggedright}X>{\raggedright}X}
\toprule
Predictors & Estimates & CI & p\\
\midrule
(Intercept) & 0.54 & 0.47Â âÂ 0.61 & <0.001\\
Melody 34 & -0.33 & -0.40Â âÂ -0.26 & <0.001\\
Melody 9 & -0.44 & -0.51Â âÂ -0.37 & <0.001\\
Melody 95 & 0.01 & -0.06Â âÂ 0.09 & 0.746\\
Marginal R2 & 0.47 &  & \\
Conditional R2 & 0.68 &  & \\
\bottomrule
\end{tabu}
\end{table}

\begin{table}[!h]

\caption{\label{tab:memodeltable4}Model 4 | Feature Category Model}
\centering
\begin{tabu} to \linewidth {>{\raggedright}X>{\raggedleft}X>{\raggedright}X>{\raggedright}X}
\toprule
Predictors & Estimates & CI & p\\
\midrule
(Intercept) & 0.35 & 0.30Â âÂ 0.40 & <0.001\\
wmc & -0.04 & -0.10Â âÂ 0.03 & 0.272\\
Marginal R2 & 0.01 &  & \\
Conditional R2 & 0.07 &  & \\
\bottomrule
\end{tabu}
\end{table}

\begin{table}[!h]

\caption{\label{tab:memodeltable5}Model 5 | Feature Continuous Model}
\centering
\begin{tabu} to \linewidth {>{\raggedright}X>{\raggedleft}X>{\raggedright}X>{\raggedright}X}
\toprule
Predictors & Estimates & CI & p\\
\midrule
(Intercept) & -0.90 & -1.44Â âÂ -0.37 & 0.001\\
tonalness & 0.90 & 0.27Â âÂ 1.53 & 0.006\\
note.dens & 0.36 & 0.17Â âÂ 0.55 & <0.001\\
tonalness:note.dens & -0.21 & -0.43Â âÂ 0.01 & 0.059\\
Marginal R2 & 0.47 &  & \\
Conditional R2 & 0.68 &  & \\
\bottomrule
\end{tabu}
\end{table}

\begin{table}[!h]

\caption{\label{tab:memodeltable6}Model 6 | Interval Entropy Model}
\centering
\begin{tabu} to \linewidth {>{\raggedright}X>{\raggedleft}X>{\raggedright}X>{\raggedright}X}
\toprule
Predictors & Estimates & CI & p\\
\midrule
(Intercept) & -0.41 & -0.53Â âÂ -0.28 & <0.001\\
i.entropy & 1.34 & 1.11Â âÂ 1.56 & <0.001\\
Marginal R2 & 0.38 &  & \\
Conditional R2 & 0.60 &  & \\
\bottomrule
\end{tabu}
\end{table}

\begin{table}[!h]

\caption{\label{tab:memodeltable7}Model 7 | Inteval Entropy Experimental Model}
\centering
\begin{tabu} to \linewidth {>{\raggedright}X>{\raggedleft}X>{\raggedright}X>{\raggedright}X}
\toprule
Predictors & Estimates & CI & p\\
\midrule
(Intercept) & -0.41 & -0.54Â âÂ -0.27 & <0.001\\
i.entropy & 1.34 & 1.11Â âÂ 1.56 & <0.001\\
wmc & -0.03 & -0.10Â âÂ 0.04 & 0.391\\
Marginal R2 & 0.38 &  & \\
Conditional R2 & 0.57 &  & \\
\bottomrule
\end{tabu}
\end{table}

\begin{table}[!h]

\caption{\label{tab:memodeltable8}Model 8 | Feature Continuous Experimental Model}
\centering
\begin{tabu} to \linewidth {>{\raggedright}X>{\raggedleft}X>{\raggedright}X>{\raggedright}X}
\toprule
Predictors & Estimates & CI & p\\
\midrule
(Intercept) & -0.90 & -1.43Â âÂ -0.37 & 0.001\\
tonalness & 0.90 & 0.27Â âÂ 1.53 & 0.006\\
note.dens & 0.36 & 0.17Â âÂ 0.55 & <0.001\\
wmc & -0.03 & -0.10Â âÂ 0.04 & 0.391\\
tonalness:note.dens & -0.21 & -0.43Â âÂ 0.01 & 0.059\\
Marginal R2 & 0.47 &  & \\
Conditional R2 & 0.68 &  & \\
\bottomrule
\end{tabu}
\end{table}

\begin{table}[t]

\caption{\label{tab:modeltable}Linear Mixed Effects Model Fits}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{lrrrrrrrr}
\toprule
  & Df & AIC & BIC & logLik & deviance & Chisq & Chi Df & Pr(>Chisq)\\
\midrule
null\_model & 3 & 62.74 & 71.89 & -28.37 & 56.74 & NA & NA & NA\\
wmc\_model & 4 & 63.45 & 75.65 & -27.72 & 55.45 & 1.29 & 1 & 0.26\\
gf\_model & 4 & 64.73 & 76.93 & -28.37 & 56.73 & 0.00 & 0 & 1.00\\
cognitive\_model & 5 & 65.38 & 80.63 & -27.69 & 55.38 & 1.35 & 1 & 0.25\\
melody\_model & 6 & -59.77 & -41.47 & 35.88 & -71.77 & 127.15 & 1 & 0.00\\
feature\_category\_model & 6 & -59.77 & -41.47 & 35.88 & -71.77 & 0.00 & 0 & 1.00\\
feature\_cont\_model & 6 & -59.77 & -41.47 & 35.88 & -71.77 & 0.00 & 0 & 0.00\\
feature\_ientropy\_model & 6 & -28.80 & -10.50 & 20.40 & -40.80 & 0.00 & 0 & 1.00\\
total\_model\_ientropy & 7 & -22.83 & -1.49 & 18.42 & -36.83 & 0.00 & 1 & 1.00\\
total\_model\_experimental & 9 & -56.97 & -29.52 & 37.49 & -74.97 & 38.14 & 2 & 0.00\\
\bottomrule
\end{tabular}}
\end{table}

\hypertarget{discussion-2}{%
\section{Discussion}\label{discussion-2}}

In this chapter, I investigated the extent to which both individual differences and abstracted musical features could be used to model results in melodic dictations.
In order to examine \(H1\), I ran a linear mixed effects model in order discern any differences in melody difficulty.
As noted in Figure \ref{fig:megrid}, both a significant main effect of Tonalness and Note Density was found, as well as a small interaction between the two variables suggesting evidence supporting rejecting \(H2\)'s null hypothesis.
The interaction emerged from differences in melody means in the low density conditions with the melody with higher tonalness actually scoring higher in terms of number of errors.
Subsequent adding of individual level predictors did result in a better fitting model.

While I expected to find an interaction, Melody 34 was hypothesized to be the easiest of the four conditions.
With Melody 9 there was a clear floor effect, which was also to be expected as when I chose the melodies, I had no previous experimental data explicitly looking at melodic dictation to rely on.
Future experiments should use abstracted features from Melody 9 as a baseline in order to avoid floor effects.

The main effect of note density was expected and exhibited a large effect size.
While it would be tempting to attribute this finding exactly to the Note Density feature extracted by FANTASTIC, the high and low density conditions could also be operationalized as having compound versus simple meter.
Given the large effect of note density, I plan on taking more careful steps in the selection of our next melodies in order to control for any effects of meter and keep the effects limited to one meter if at all possible.

Somewhat surprisingly, the analysis incorporating the cognitive measures of covariance did not yield any significant results.
While other researchers and the third Chapter of this dissertation have noted the importance of baseline cognitive ability \citep{schellenbergMusicCognitiveAbilities2013}, the task specificity of doing melodic dictation as I designed the experiment might not be well suited to capture the variability needed for any effects.
Hence, this chapter would not be able to reject \(H3\)'s null hypothesis.
Considering that other researchers have found constructs like working memory capacity and general fluid intelligence to be important factors of tasks of musical perception, a more refined design might be considered in the future to find any sort of effects.
Taken as a whole, these findings suggest that aural skills pedagogues should consider exploring the extent to which computationally extracted features can guide the difficulty expected of melodic dictation exercises.

\hypertarget{conclusions-3}{%
\section{Conclusions}\label{conclusions-3}}

This chapter demonstrated that abstracted musical features such as tonalness and note density can play a role in predicting how well students do in tasks of melodic dictation.
While the experiment failed to yield any significant differences in cognitive ability predicting success at the task, future research plans should not fail to take these effects into consideration.
One important caveat in this modeling is that the models reported here are subject to change given other scoring procedures.
There is a high degree of variability in how melodic dictations are scored \citep{gillespieMelodicDictationScoring2001}, and modeling how different scoring procedures lead to differing results should be considered for future research.
Importantly, if a researcher adopted this paradigm for investigating melodic dictation, what is most important is that there is some sort of external reference a single scorer can compare themselves to.
Without having a pre-defined metric, scoring and thus grading will be subjected to the scorer's explicit or implicit biases.
Given all that has been put forward here, the research thus far still does not explain the underlying processes for melodic dictation.
In this chapter, I have put forward two factors that help describe what contributes to this process using a principled experimental framework.

\clearpage

\hypertarget{computational-model}{%
\chapter{Computational Model}\label{computational-model}}

\hypertarget{levels-of-abstraction}{%
\section{Levels of Abstraction}\label{levels-of-abstraction}}

In his 2007 article \emph{Models of Music Similarity}, Geraint Wiggins distinguishes between descriptive and explanatory models in describing the modeling of human behavior \citep{wigginsModelsMusicalSimilarity2007}.
Descriptive models assert what will happen in response to an event.
For example, as discussed in the previous chapter, as the note density of a melody increases and the tonalness of a melody decreases, a melody becomes more difficult to dictate.
While the increase in note density is assumed to drive the decrease in dictation scores, merely stating that there is an established relationship between one variable and the other says nothing about the inner workings of this process.
An explanatory model on the other hand, not only describes what will happen, but additionally notes why and how this process occurs.
For example, much of the work from musical expectation demonstrates that as an individual's exposure to a musical style increases, so does their ability to predict specific events within a given musical texture \citep{pearceStatisticalLearningProbabilistic2018a}.

Not only does more exposure predict more accurate responses, but many of these models of musical expectation derive their underlying predictive power from the brain's ability to implicitly track statistical regularities in an auditory scene \citep{saffranStatisticalLearningTone1999, margulisRepeatHowMusic2014}.
The \emph{how} derives from the tracking of statistical regularities in musical information and the \emph{why} derives from evolutionary demands; organisms that are able to make more accurate predictions about their environment are more likely to survive and pass on their genes \citep{huronSweetAnticipation2006}.

Wiggins writes that although there can be both explanatory and descriptive theories, depending on the level of abstraction, a theory may be explanatory at one level, yet descriptive at another.
Using the mind-brain dichotomy, he asserts that the example of a theory of musical expectation could be explanatory at the level of behavior as noted above, but says nothing about what is happening at the neural level.
Both descriptive and explanatory theories are needed: descriptive theories are used to test explanatory theories.
By stringing together different layers of abstraction, we can arrive at a better understanding of how the world works.

Returning to melodic dictation, under Wiggins' framework the Karpinski model of melodic dictation \citep{karpinskiAuralSkillsAcquisition2000, karpinskiModelMusicPerception1990} qualifies as a descriptive model.
The model explains what happens over the time course of a melodic dictation--- specifying four discrete stages discussed in earlier chapters--- but does not explicitly state \emph{how} or \emph{why} this process happens.
In order to have a more complete understanding of melodic dictation, an explanatory model is needed.

In this chapter, I introduce an explanatory model of melodic dictation.
The model is inspired by work from both computational musicology and cognitive psychology.
From computational musicology, I draw on the work of Marcus Pearce's IDyOM \citep{pearceConstructionEvaluationStatistical2005} and from cognitive psychology, I draw on Nelson Cowan's Embedded Process model of working memory \citep{cowanEvolvingConceptionsMemory1988, cowanMagicalMysteryFour2010} to explain the perceptual components.
In addition to quantifying each step, the model incorporates flexible parameters that can be adjusted in order to accommodate individual differences, while still relying on a domain general process.
By relying on cognitive mechanisms based in statistical learning rather than a rule based system for music analysis \citep{lerdahlGenerativeTheoryTonal1986, narmourAnalysisCognitionBasic1990, narmourAnalysisCognitionMelodic1992, temperleyCognitionBasicMusical2004}, this model allows for the heterogeneity of musical experience among a diversity of music listeners.

\hypertarget{model-overview}{%
\section{Model Overview}\label{model-overview}}

The model consists of three main modules, each with its own set of parameters:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Prior Knowledge
\item
  Selective Attention
\item
  Transcription and Re-entry
\end{enumerate}

The Prior Knowledge module reflects the previous knowledge an individual brings to the melodic dictation.
The Selective Attention module--- somewhat akin to Karpinski's extractive listening \citep{karpinskiAuralSkillsAcquisition2000, karpinskiModelMusicPerception1990}--- segments incoming musical information by using the window of attention as conceptualized as the limits of working memory capacity as a sensory bottleneck to constrict the size of musical chunk that an individual could transcribe.
Once musical material is in the focus of attention, the Transcription function pattern matches against the Prior Knowledge's corpus of information in order to find a match of explicitly known musical information.
The Transcription function will recursively truncate what musical information is in Selective Attention if no match is found.
In addition to Transcription, there is also a Re-entry function that will restart the entire loop.
This process reflects, but is not intended to mirror, the cognitive process used in melodic dictation.
Rather it attempts to be phenomenologically similar to the decision making process used when attempting to notate novel melodies.
Based on both the prior knowledge and individual differences of the individual, the model will scale in ability, with the general retrieval mechanisms in place.
The exact details of the assumptions, parameters, and complete formula of the model are discussed below.

\hypertarget{verbal-model}{%
\section{Verbal Model}\label{verbal-model}}

Below I describe my model's assumptions, parameters, as well as the steps taken when the model is run.
After detailing the inner workings of each of the assumptions and the modules, described in roughly the order that they occur, I present the model using pseudocode with the terminology described below.
I discuss the issues of assumptions and representations as they arise in describing the model.

\hypertarget{model-representational-assumptions}{%
\subsection{Model Representational Assumptions}\label{model-representational-assumptions}}

In order to write a computer program that mirrors the melodic dictation process, how the mind perceives and represents musical information must be defined \emph{a priori}.
Before delving into questions of representation, this model assumes that the musical surface as represented by the notes via Western musical notation are salient and can be perceived as distinct perceptual phenomena
Although there is work that suggests that different cultures and levels of experience might not categorize musical information universally \citep{mcdermottIndifferenceDissonanceNative2016}, other work suggests that experiencing pitches as discrete, categorical phenomena is categorized as a statistical human universal \citep{savageStatisticalUniversalsReveal2015}.
In either case, the most explanatory models of any account of music perception should incorporate theoretically relevant features \citep{harrisonDissociatingSensoryCognitive2018}.
For the purposes of this model, I assume that individuals do in fact perceive the musical surface similarly to the written score.

Knowing that it is melodic information or melodic data that needs to be represented, the question then becomes what is the best way in which to represent it.
This issue becomes increasingly complex when considering literature suggesting that the human mind represents musical information in a variety of different forms \citep{krumhanslCognitiveFoundationsMusical2001, levitinCurrentAdvancesCognitive2009}.
For the purposes of this model and further examples, I choose to represent musical information using both the pitch (note and scale degree) and timing (rhythm and inter-onset-interval) representation described in \citet{pearceStatisticalLearningProbabilistic2018a}.
Using these two parameters reflects only a subset of the possibilities that could be modeled when using a multiple viewpoint system \citep{conklinMultipleViewpointSystems1995}.
Future research comparing this model's output using different representations will also contribute to conversations regarding pedagogy.
If one form of representation mirrors human behavior better than others, it would provide evidence in support of the pedagogy of one system over another.
How the model represents musical information is the first important parameter value that needs to be chosen before running the model and this establishes the Prior Knowledge.

\hypertarget{contents-of-the-prior-knowledge}{%
\subsection{Contents of the Prior Knowledge}\label{contents-of-the-prior-knowledge}}

The Prior Knowledge consists of a corpus of digitally represented melodies taken to reflect the implicitly understood structural patterns in a musical style that the listener has been exposed to.
The logic of representing an individual's prior knowledge follows the assumptions of both the Statistical Learning Hypothesis (SLH) and the Probabilistic Prediction Hypothesis (PPH), both core theoretical assumptions of the Information Dynamic of Music (IDyOM) model of Marcus Pearce \citep{pearceConstructionEvaluationStatistical2005, pearceStatisticalLearningProbabilistic2018a}.
Using a corpus of melodies to represent an individual's Prior Knowledge relies on the Statistical Learning Hypothesis which states:

\begin{quote}
musical enculturation is a process of implicit statistical learning in which listeners progressively acquire internal models of the statistical and structural regularities present in the musical styles to which they are exposed, over short (e.g., an individual piece of music) and long time scales (e.g., an entire lifetime of listening). p.2 (Pearce, 2018)
\end{quote}

The logic here is that the more an individual is exposed musical material, the more they will implicitly understand it, which leads the corroborating probabilistic prediction hypothesis which states:

\begin{quote}
while listening to new music, an enculturated listener applies models learned via the SLH to generate probabilistic predictions that enable them to organize and process their mental representations of the music and generate culturally appropriate responses. p.2 (Pearce, 2018).
\end{quote}

Taken together and then quantified using Shannon information content \citep{shannonMathematicalTheoryCommunication1948}, it then becomes possible using the IDyOM framework to have a quantifiable measure that reliably predicts the amount of perceived unexpectedness in a musical melody that can change pending on the musical corpus that the model is trained on.
As a model, IDyOM has been successful mirroring human behavior in melodies in various styles \citep{pearceStatisticalLearningProbabilistic2018a}, harmony--- outperforming \citep{harrisonDissociatingSensoryCognitive2018} sensory models of harmony \citep{bigandEmpiricalEvidenceMusical2014}---, and is also being developed to handle polyphonic materials \citep{sauvePredictionPolyphonyModelling2017}.

Stepping beyond the assumptions of IDyOM, the Prior Knowledge also needs to have a implicit/explicitly known parameter which indicates whether or not a pattern of music--- or n-gram pattern--- is explicitly learned.
This threshold can be set relative to the entire distribution of all n-grams in the corpus.

\hypertarget{modeling-information-content}{%
\subsection{Modeling Information Content}\label{modeling-information-content}}

Having established that the models' first parameters to be decided are the representation of strings and the implicit/explicit threshold, the next decision that has to be made is how the model decides segmentation for the second stage of Selective Attention.
Although there has been a large amount of work on different ways to segment the musical surface using rule based methods \citep{lerdahlGenerativeTheoryTonal1986, margulisModelMelodicExpectation2005, narmourAnalysisCognitionBasic1990, narmourAnalysisCognitionMelodic1992}, which rely on matching a music theorist's intuition with a set of descriptive rules somewhat like the boundary formation rules put forward in \emph{A Generative Theory of Tonal Music}, as noted by Pearce \citep{pearceStatisticalLearningProbabilistic2018a}, rule based models often fail when applied to music outside the Western classical tradition.
Additionally, since melodic dictation is an active memory process, rather than a semi-passive process of listening, this model needs to be able to quantify musical information on two conditions.
The first is that it must be dependent on prior musical experience.
The second is that it should allow for a movable boundary for selective attention so that musical information that is in memory can be actively maintained while carrying out another cognitive process, that of notating the melody.

In order to create this metric, I rely on IDyOM's use of information content \citep{shannonMathematicalTheoryCommunication1948}, which quantifies the information content of melodies based on a corpus of materials utilizing a multiple viewpoints framework \citep{conklinMultipleViewpointSystems1995}.

For example, when trained against a corpus of melodies, this excerpt in Figure \ref{fig:schubertF} from the fourth movement of Schubert's \emph{Octet in F Major} (D.803) lists the information content of the excerpt calculated for each note atop the notation.\footnote{The following musical examples is taken from \citet{pearceStatisticalLearningProbabilistic2018a} reflects a model where IDyOM was configured to predict pitch with an attribute linking melodic pitch interval and chromatic scale degree (pitch and scale degree) using both the short-term and long-term models, the latter trained on 903 folk songs and chorales (data sets 1, 2, and 9 from table 4.1 in \citep{schaffrathEssenFolkSong1995} comprising 50,867 notes.}
Appearing in Figure \ref{fig:cumSchubert}, I plot the cumulative information content of the melody, along with both an arbitrary threshold for the limits of working memory capacity, and where the subsequent segmentation boundary for musical material to be put in the Selective Attention buffer would be.
These values chosen show a small example of how the Selective Attention module works.
The advantage of operationalizing how an individual hears a melody like this is that melodies with lower information content, derived from an understanding of having more predictable patterns from the corpus, will allow for larger chunks to be put inside of the selective attention buffer.
Additionally, individuals with higher working memory capacity would be able to take in more musical information.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/SchubertF} 

}

\caption{Cadential Excerpt from Schubert's Octet in F Major}\label{fig:schubertF}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/SchubertPlotNew} 

}

\caption{Cumulative Information in Schubert Octet Excerpt}\label{fig:cumSchubert}
\end{figure}

It is important to highlight that the notes above the melody here are dependent on what is current in the Prior Knowledge module.
A corpus of Prior Knowledge with less musical events would lead to higher information content measures for each set of notes, while a Prior Knowledge that has extensive tracking of the patterns would lead to lower information content \citep{conklinMultipleViewpointSystems1995}.
This increase in predictive accuracy mathematically reflects the intuition that those with more listening experience can process greater chunks of musical information.
I visualize what setting the explicit/implicit threshold on a set of various n-grams would look like in Figure \ref{fig:thresholds}.
While I have arbitrarily set the thresholds here for illustrative purposes, in practice this could be done a number of different ways.
For example, these thresholds could be set based on frequency counts in the corpus, relative distributions, or the parameters might be computationally derived.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/pk_grampanel} 

}

\caption{Setting Prior Knowledge Limits}\label{fig:thresholds}
\end{figure}

\hypertarget{setting-limits-with-transcribe}{%
\subsection{Setting Limits with Transcribe}\label{setting-limits-with-transcribe}}

With each note quantified with a measure of information content, it becomes possible to set a limit on the maximum amount of information that the individual would be able to hold in memory as defined by the Selective Attention module.
A higher threshold would allow for more musical material to be put in the attentional buffer, and a lower threshold would restrict the amount of information held in an attentional buffer.
By putting a threshold on this value, this serves as something akin to a perceptual bottleneck based on the assumption that there is a capacity limit to that of working memory \citep{cowanEvolvingConceptionsMemory1988, cowanMagicalMysteryFour2010}.
Modulating this boundary will help provide insights into the degree to which melodic material can be retained between high and low working memory span individuals.

In practice, notes would enter the attentional buffer until the information content from the melody is equal to the memory threshold.
At this point, the notes that are in the attentional buffer are segmented and will be actively maintained in the Selective Attention buffer.
In theory, the maximum of the attentional buffer should not be reached since the individual performing the dictation would still need mental resources and attention to actively manipulate the information in the attentional buffer for the process of notating.

\hypertarget{pattern-matching}{%
\subsection{Pattern Matching}\label{pattern-matching}}

The subset of notes of the melody represented in the attentional buffer, whether or not the melody becomes notated depends on whether or not the melody or string in the buffer can be matched with a string that is explicitly known in the corpus.
Mirroring a search pattern akin to Cowan's Embedded Process model \citep{cowanEvolvingConceptionsMemory1988, cowanMagicalMysteryFour2010}, the individual would search across their long term memory, or Prior Knowledge, for anything close to or resembling the pattern in the Selective Attention buffer.
Cowan's model differs from other more modular based models of working memory like those of \citet{baddeleyWorkingMemory1974} by positing that working memory should be conceptualized as a small window of conscious attention.
As an individual directs their attention to concepts represented in their long term memory, they can only spotlight a finite amount of information where categorical information regarding what is in the window of attention is not far from retrieval.
Using this logic, longer pattern strings or n-grams would be less likely to be recalled exactly since they occur less frequently in the Prior Knowledge.

If a pattern match that has been moved to Selective Attention is immediately found, the contents of Selective Attention would be considered to be notated.
The model would register that a loop had taken place and document the n-gram match.
Of course, finding an immediate pattern match each time is highly unlikely, and the model needs to be able to compensate if that happens.

If a pattern is not found in the initial search that is explicitly known, one token of the n-gram would be dropped off the string and the search would happen again.
This recursive search would happen until an explicit long term memory match is made.
Like humans taking melodic dictation, the computer would succeed more often finding patterns that fall within the largest density of a corpus of intervals distribution.
Additionally, like students performing a dictation, if a student does not explicitly know an interval, or a 2-gram, the dictation would not be able to be completed.
If this happens, both the model and student would have to move on to the next segment via the Re-entry function.

Eventually there would be a successful explicit match of a string in the Transcription module and that section of the melody would be considered to be dictated.
The model here would register that one iteration of the function has been run and the chunk transcribed would then be recorded.
After recording this history, the process would happen again starting at either the next note from where the model left off, the note in the entire string with the lowest information content, another pre-determined setting.
This parameter is defined before the model is run and the question of dictation re-entry certainly warrants further research and investigation.

This type of pattern search is also dependent on the way that the Prior Knowledge is represented.
In the example here, both pitch and rhythmic information are represented in the string that holds the contents of Selective Attention.
Since there is probably a very low likelihood of finding an exact match for every n-gram with both pitch and rhythm, this pattern search can happen again with both rhythms and pitch information queried separately.
If not found, exact pitch-temporal matches are found and the search is run again on either the pitch or rhythmic information separately.
This would be computationally akin to Karpinski's proto-notation that he suggests students use in learning how to take melodic dictation \citep[p.88]{karpinskiAuralSkillsAcquisition2000}.
This feature of the model would predict that more efficient dictations would happen when pitch and interval information is dictated simultaneously.
Running the model prioritizing the secondary search with either pitch or rhythmic information will provide new insights into practical applications of dictation strategies.
Using this separate search feature as an option of the model seems to match with the intuitions strategies that a student dictating a melody might use.

\hypertarget{dictation-re-entry}{%
\subsection{Dictation Re-Entry}\label{dictation-re-entry}}

Upon the successful pattern match of a string, the Selective Attention and Transcription module would need to then be run again.
This process is done via the Re-entry function.
As noted above, Re-entry in the melody could be a highly subjective point of discussion.
The model could either re-enter at the last note where the function successfully left off, the note in the melody with the lowest information content, the n-gram most salient in the corpus, or theoretically any other way that could be computationally implemented.
Entering at the last note not transcribed is logical from a computational standpoint, but this linear approach seems to be at odds with anecdotal experience.
Entering at the note with the lowest information content seems to provide an intuitive point of re-entry in that it would then be easier to transcribe.
Entering at the most represented n-gram seems to align with intuition in that people would want to tackle the easier tasks first, but this rests on the assumption that humans are able to reliably detect the sections of a melody that are easiest to transcribe based on implicitly learned statistical patterns.
For example, some people might instead choose to go to the end of a melody after successful transcription of the start of the melody.
This might be because this part of the melody is most active in memory due to a recency effect, or it could be that that cadential gestures are more common in being represented in the Prior Knowledge.

\hypertarget{completion}{%
\subsection{Completion}\label{completion}}

Given the recursive nature of this process, if all 1-grams are explicitly represented in the Prior Knowledge then the target melody will be transcribed.
If only represented using such a small chunk, the model will have to loop over the melody many times, thus indicating that the transcriber had a high degree of difficulty dictating the melody.
If there is a gap in explicit knowledge in the prior knowledge, only patches of the melody will be recorded and the melody will not be recorded in its entirety.
An easier transcription will result in less iterations of the model with larger chunks.
Though the current instantiation of the model does not incorporate how multiple hearings might change how a melody is dictated, one could constrain the process to only allow a certain number of iterations to reflect this.
Of course as a new melody is learned, it is slowly being introduced into long term memory and could be completely capable of being represented in long term memory without being explicitly notated at the end of a dictation with time running out and thus not possible to be completed.
This would be imposing some sort of experimental constraint on the process and since this is meant to be a cognitive computational model of melodic dictation this caveat would complicate the model.
Future research could be done to optimize the choices that the model makes in order to satisfy whatever constraints are imposed and could be an interesting avenue of future research, but is beyond the initial goals of the model.

\hypertarget{model-output}{%
\subsection{Model Output}\label{model-output}}

The model then outputs each n-gram transcribed and can be counted as a series with less attempts mapping to an easier transcription.
This aligns with intuitions about the process of melodic dictation.
It first creates a linear mapping of attempts to dictate with difficulty of the melody.
It relies on a distinction between explicit and implicit statistical knowledge.
It is based on the Embedded Process Model from working memory and attention, so is part of a larger generative model, giving more credibility to explaining the underlying processes of melodic dictation.
The output of this model is inspired by work on mental rotation \citep{shepardMentalRotationThreeDimensional1971}, which assumes that time to complete a task is directly proportional to cognitive effort required.
More iterations of the model as reflected in more searches should reflect more mental effort and vice versa.

\hypertarget{formal-model}{%
\section{Formal Model}\label{formal-model}}

I present the computational model in pseudocode as described in Figure \ref{fig:mymodel}.
First, listed are the defined inputs, the functions needed to run the algorithm, and then the sequence the model runs.
To aid distinguishing between functions and objects, I put functions in italics and objects in bold.
Below the model in Figure \ref{fig:walkthru} I provide a brief walk through of one iteration of the model.

\hypertarget{computational-model-1}{%
\subsection{Computational Model}\label{computational-model-1}}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/Model} 

}

\caption{Formal Model}\label{fig:mymodel}
\end{figure}

\hypertarget{example}{%
\subsection{Example}\label{example}}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/Model Graphic} 

}

\caption{Model Example}\label{fig:walkthru}
\end{figure}

Figure \ref{fig:walkthru} depicts one iteration of the model run using the musical example from above using a hypothetical corpus for the pattern matching.
Using the model above, the following inputs were defined \emph{a priori}:

\begin{itemize}
\tightlist
\item
  The \textbf{Prior Knowledge} is a hypothetical corpus of symbolic strings representing all n-grams of melodies
\item
  The \textbf{Threshold} is set to \textbf{five} exact matches in the \textbf{Prior Knowledge}
\item
  The \textbf{WMC} is set at 17
\item
  The \textbf{Target Melody} is the Schubert excerpt from above
\item
  The \textbf{String Position} object is used to track the position in the dictation
\item
  The \textbf{Difficulty} object starts at 0
\item
  The \textbf{Dictation} object is \texttt{NULL} to begin, and each new n-gram successfully transcribed is annexed to it
\end{itemize}

Figure \ref{fig:walkthru} progresses from left to right over the course of time.
The algorithm begins by first running the \texttt{listen()} function on the \textbf{Target Melody}.
First, the model checks that there are notes to transcribe; this being the first loop of the model, this is statement will be \texttt{FALSE} so the next step is taken.
Notes of the \textbf{Target Melody} are read into the \textbf{Selective Attention} buffer until the information content of the melody exceeds that of the working memory threshold.
This is depicted graphically in the leftmost panel of Figure \ref{fig:walkthru}.
Each note unfolding over time fills up the \textbf{Selective Attention} working memory buffer.
When the amount of information reaches the perceptual bottleneck--- as indicated by the dashed line--- the \textbf{Selective Attention} buffer stops receiving information.
At this point the model will mark where in the melody it stopped taking in new information for later.
Here the contents in \textbf{Selective Attention} are moved to the \texttt{transcribe()} function.

With the contents of \textbf{Selective Attention} passed to \texttt{transcribe()}, the model adds one to the counter indicating the first search is about to run.
Moving to the middle panel of Figure \ref{fig:walkthru}, the symbol string of notes in the first column is indexed against the \textbf{Prior Knowledge}.
Only if a five note pattern has appeared more than or equal to five times, as determined by the \textbf{Threshold} input, will the corresponding \texttt{EXPLICIT} column be \texttt{TRUE}.
In this case, this pattern has occurred over the threshold of 5 and thus a successful match is found.

It is at this step that the search resembles that of Cowan's model of working memory as active attention.
The pattern being searched for is compared against a vast amount of information, with cues from the contents of what is in \textbf{Selective Attention} grouping similar patterns together.
At the neural level, this is most likely a much more complex process, but to show this grouping I note that this search is at least organized by the first pitch.
I assume it would be reasonable that patterns starting on G as \(\hat{5}\) might happen together.
Since this string does have a \texttt{TRUE} match with \texttt{EXPLICIT}, the contents of \textbf{Selective Attention} are considered notated.
At this point the model would record the 5-gram, along with the string that it was matched with.
The function would then re-run the \texttt{listen} function via the \texttt{notateReentry()} function at the next point in the melody as tracked by the \textbf{String Position} object.

If there were not to have been an exact match, the model would remove one token from the melody and perform the search again on the knowledge of all 4-grams and add one to the \textbf{Difficulty} counter.
This process would happen recursively until a match is found.
If no match is found in either the complex representation, or that of the two rhythm and pitch corpora, the fifth step of \texttt{transcribe()} would trigger \texttt{notateReentry()} to be run without documenting the n-gram currently being dictated.
This would be akin to a student not being able to identify a difficult interval, thus having to restart the melody at a new position.
Decisions about re-entry warrant further research and discussion, but for the sake of parsimony this model assumes linear continuation.
As notated in \protect\hyperlink{dictation-re-entry}{Dictation Re-Entry}, other modes of re-entry could be incorporated into the model.

This looping process would occur again and again until the entire melody is notated.
With each iteration of each n-gram notated, the difficulty counter would increase in relation to the representation of that string in the corpus.
This provides an algorithmic implementation of the intuition that less common n-grams or intervals (2-grams) are going to lead to higher difficulty in dictation.
Also worth noting is steps 3 and 4 in the \texttt{transcribe()} function are akin to Karpinski's proto-notation.
Further research might consider advantages in the order of searching the \textbf{Prior Knowledge} corpora.

\hypertarget{conclusions-4}{%
\section{Conclusions}\label{conclusions-4}}

In this chapter, I presented an explanatory, computational model of melodic dictation.
The model combines work from computational musicology and work from cognitive psychology.
In addition to being a complete model that explicates every step of the dictation process, the model seems to match phenomenological intuitions as to the process of melodic dictation.
Given the current state of the model, it makes predictions about the dictation process and can eventually be implemented and tested against human behavioral data to provide evidence in support of its verisimilitude.
For example, the model predicts:

\begin{itemize}
\tightlist
\item
  Segments of melodies are likely successfully to be dictated relative to the frequency distribution of their prior knowledge.
\item
  Higher working memory span individuals will be able to dictate larger chunks of melodies, and thus perform better at dictation
\item
  Using an \emph{atomistic} dictation strategy will result in not as effective dictations than attempting to identify larger patterns
\item
  Determining the difficulty of melodies of equal length is predictable from the frequency the melody's cumulative n-gram distribution.
\item
  Some \emph{atonal} melodies will be easier to dictate than tonal melodies if they consist of patterns that are more frequent in a listener's Prior Knowledge.
\item
  Higher exposure to sight-singing results in more explicitly learned patterns, thus the ability to identify larger patterns of music
\end{itemize}

Although many of these hypotheses might seem intuitive to any instructor who has taught aural skills before, work from this dissertation provides a theory as to why each appears to be true.
Future research beyond this dissertation will explore predictions as a result of this work in more detail.
Most importantly from a pedagogical standpoint, the model and underlying theory gives exact language to discuss the underlying processes of melodic dictation, which can serve as a valuable pedagogical and research contribution.

\clearpage

\hypertarget{summary}{%
\chapter{Summary}\label{summary}}

Melodic dictation is a cognitively demanding process that requires individuals to be able to hear a melody then write it down.
Not only is understanding how this process works worthy of study from a pedagogical perspective, but the ability able to hear, understand, then notate a melody is a fascinating psychological feat.
Complex sound waves cause the tympanic membrane to vibrate, these vibrations are transmuted from physical to electrical energy in the brain, then this incoming sensory signal is organized via top-down processing for our inner homunculus to decipher and translate these hierarchically organized sensations into meaningful symbols.
Understanding the intricacies of this process is a complex problem, but complexity is what initially attracts many people to music in the first place.
By dissecting each part of this listening process, it is possible to gain more and more understanding of music.

In this dissertation, I began by examining the Karpinski model of melodic dictation.
While fantastic as a didactic model, the model is not specific enough to be examined with both experimental and computational methodologies.
The model is idealized and was originally developed for pedagogical, rather than explanatory purposes.
It assumes similar processes for different individuals and extends these assumptions to different melodies.

Using these assumptions as a point of departure, I put forward a taxonomy of features that could be used to fill the voids in the Karpinski model were it to be adopted for research purposes.
The taxonomy posits that both individual and musical factors will contribute to an individual's performance on a musical memory task.
Individual factors include cognitive and environmental factors; musical factors include structural and experimental factors.
While I used this taxonomy as a guide in my literature review, the taxonomy can also aid in organizing future research on musical memory.
The subsequent chapters of this dissertation explored the factors of this taxonomy.

In the third chapter, I demonstrated how differences in individual ability can be accounted for in melodic dictation in both musically trained and untrained individuals.
I put forward evidence to corroborate claims by \citet{berzWorkingMemoryMusic1995} and stressed the importance of including cognitive ability as a measure of interest when considering musical memory.
Understanding these factors is important because we as aural skills practitioners and pedagogues need to be able to know if the degree of a student's ability to complete a task is within their ability to change it.
If this skill is malleable, we need to understand how to best cultivate it.
If a skill is not malleable, we need to take caution not to design assessments that measure something beyond the student's control.

In the fourth chapter, I posited that tools from computational musicology can help better understand the musical side of the taxonomy.
I used a survey of 40 aural skills pedagogues to demonstrate the degree of agreement between what makes melodies difficult to dictate.
Using this survey data as a ground truth, I then showed how various computational measures can then be mapped to human judgments of difficulty.
Surveying the application of these computational methods to expert data allowed me to highlight the benefits and pitfalls of using various types of computations.
Measures inspired by information content captured large amounts of the variance of the expert data.
This lead to an assertion of how tools from computational musicology can act as a bridge to formalize ideas put forward by cognitive psychology.
Considering this, I put forward a novel theory of musical memory: the Frequency Facilitation Hypothesis.
I showed how combining both computational measures with theories from cognitive psychology can help better understand the difficulty of the tasks we ask of students.
The chapter concluded by putting forward meaningful ways in which these ideas might be used to create a more linear path to success in the aural skills classroom.

In the fifth chapter, I presented the \emph{MeloSol} corpus.
This corpus is a collection of 783 monophonic vocal melodies taken from a standard sight singing textbook \citep{berkowitzNewApproachSight2011}.
The \emph{MeloSol} corpus is not only a contribution for future research in music perception, music pedagogy, and computational musicology, but in introducing the corpus via a corpus analysis, I was able to highlight many addressed assumptions in computational musicology.

In the sixth chapter, I modeled how melodic dictation could be operationalized using an experiment that takes full advantage of robust statistical modeling techniques.
When cast as an experiment, the amount of variables at play in melodic dictation is cumbersome from both an individual and musical point of view.
By utilizing methods from computational linguistics that have already been developed to ameliorate these problems, I put forward a more stable and robust framework that future experiments investigating melodic dictation can implement.
Using a more consistent, flexible framework will allow for more interpretability across studies and bring more cohesion to future literature examining melodic dictation.

Lastly, I introduced a cognitive, computational model of melodic dictation.
This model allowed me to formalize all the theoretical factors deemed relevant to melodic dictation and incorporate them into a computational model.
Not only does creating a computational model declare all ontological assumptions, but introducing this closed system allows for future systematic investigations to the model's verisimilitude.
The model makes several falsifiable predictions that can be investigated in future research.

There is still plenty of research to be done in terms of bridging the gap between the world of aural skills pedagogy and music psychology, but within this chasm is a large potential for knowledge.
Research at this intersection is important for aural skills pedagogues, as it helps hone our understanding of how to be effective classroom teachers.
Research at this intersection is important for music psychologists, as questions of music perception are able to provide novel insights into cognition that might only be accessible through the complex phenomena which is music.
Understanding the building blocks of aural skills is understanding the building blocks of musical perception.
The two go hand in hand.

\clearpage

\addcontentsline{toc}{chapter}{List of References}

\clearpage

\bibliography{book.bib,packages.bib,pkg-refs.bib}

\backmatter
\chapter{Appendix A: Karpinski Permissions}

\thispagestyle{empty}
\begin{center}
\includegraphics{img/karpinskipermission.png}
\end{center}

\chapter{Appendix B: Cowan Permissions}

\thispagestyle{empty}
\begin{center}
\includegraphics{img/cownpermission.png}
\end{center}

\chapter{Vita}

\doublespacing

David John Baker is a music researcher and educator passionate about questions at the intersection of music theory and music science.
His research looks to understand how the people learn melodies in order to improve pedagogical practices in aural skills education.
Initially trained as a conservatory musician-- studying under members of The Cleveland Orchestra at Baldwin Wallace University-- David has since completed an MSc in Music, Mind and Brain in London, England, and received his PhD in Music Theory at Louisiana State University where he completed his graduate minor in Cognitive and Brain Sciences.
Using his background in the humanities and training in psychological sciences, David builds testable models of how people hear music.
His overlapping quantitative skill set with the world of data science has also led him to both music industry projects and work in the non-profit/charity sector.
By combining knowledge at both the humanities and sciences, David believes that knowledge gained at this intersection will lead to world peace.


\end{document}
