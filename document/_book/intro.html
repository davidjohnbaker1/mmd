<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Chapter 2 Theoretical Background and Rationale | MODELING MELODIC DICTATION</title>
  <meta name="description" content="This dissertation explores both individual and musical features that might contribute to processes involved in melodic dictation.">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Chapter 2 Theoretical Background and Rationale | MODELING MELODIC DICTATION" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This dissertation explores both individual and musical features that might contribute to processes involved in melodic dictation." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Theoretical Background and Rationale | MODELING MELODIC DICTATION" />
  
  <meta name="twitter:description" content="This dissertation explores both individual and musical features that might contribute to processes involved in melodic dictation." />
  

<meta name="author" content="A Dissertation">
<meta name="author" content="Submitted to the Graduate Faculty of the Louisiana State University and Agricultural and Mechanical College in partial fulfillment of the requirements for the degree of Doctor of Philosophy">
<meta name="author" content="in">
<meta name="author" content="The School of Music">
<meta name="author" content="by David John Baker">
<meta name="author" content="B.M., Baldwin Wallace University, 2012">
<meta name="author" content="MSc., Goldsmiths, University of London, 2015">
<meta name="author" content="May 2019">


<meta name="date" content="2019-03-31">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="index.html">
<link rel="next" href="individual-differences.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; position: absolute; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; }
pre.numberSource a.sourceLine:empty
  { position: absolute; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: absolute; left: -5em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Modeling Melodic Dictation</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Significance of the Study</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#rationale"><i class="fa fa-check"></i><b>1.1</b> Rationale</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#chapter-overview"><i class="fa fa-check"></i><b>1.2</b> Chapter Overview</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Theoretical Background and Rationale</a><ul>
<li class="chapter" data-level="2.1" data-path="intro.html"><a href="intro.html#melodic-dictation"><i class="fa fa-check"></i><b>2.1</b> Melodic Dictation</a><ul>
<li class="chapter" data-level="2.1.1" data-path="intro.html"><a href="intro.html#describing-melodic-dictation"><i class="fa fa-check"></i><b>2.1.1</b> Describing Melodic Dictation</a></li>
<li class="chapter" data-level="2.1.2" data-path="intro.html"><a href="intro.html#taxonomizing"><i class="fa fa-check"></i><b>2.1.2</b> Taxonomizing</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="intro.html"><a href="intro.html#individual-factors"><i class="fa fa-check"></i><b>2.2</b> Individual Factors</a><ul>
<li class="chapter" data-level="2.2.1" data-path="intro.html"><a href="intro.html#cognitive"><i class="fa fa-check"></i><b>2.2.1</b> Cognitive</a></li>
<li class="chapter" data-level="2.2.2" data-path="intro.html"><a href="intro.html#environmental"><i class="fa fa-check"></i><b>2.2.2</b> Environmental</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="intro.html"><a href="intro.html#musical-factors"><i class="fa fa-check"></i><b>2.3</b> Musical Factors</a><ul>
<li class="chapter" data-level="2.3.1" data-path="intro.html"><a href="intro.html#discussing-melodic-structure"><i class="fa fa-check"></i><b>2.3.1</b> Discussing Melodic Structure</a></li>
<li class="chapter" data-level="2.3.2" data-path="intro.html"><a href="intro.html#abstracted-features"><i class="fa fa-check"></i><b>2.3.2</b> Abstracted Features</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="intro.html"><a href="intro.html#polymorphism-of-ability"><i class="fa fa-check"></i><b>2.4</b> Polymorphism of Ability</a></li>
<li class="chapter" data-level="2.5" data-path="intro.html"><a href="intro.html#conclusions"><i class="fa fa-check"></i><b>2.5</b> Conclusions</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="individual-differences.html"><a href="individual-differences.html"><i class="fa fa-check"></i><b>3</b> Individual Differences</a><ul>
<li class="chapter" data-level="3.1" data-path="individual-differences.html"><a href="individual-differences.html#rationale-1"><i class="fa fa-check"></i><b>3.1</b> Rationale</a></li>
<li class="chapter" data-level="3.2" data-path="individual-differences.html"><a href="individual-differences.html#individual-differences-1"><i class="fa fa-check"></i><b>3.2</b> Individual Differences</a><ul>
<li class="chapter" data-level="3.2.1" data-path="individual-differences.html"><a href="individual-differences.html#improving-musical-memory"><i class="fa fa-check"></i><b>3.2.1</b> Improving Musical Memory</a></li>
<li class="chapter" data-level="3.2.2" data-path="individual-differences.html"><a href="individual-differences.html#memory-for-melodies"><i class="fa fa-check"></i><b>3.2.2</b> Memory for Melodies</a></li>
<li class="chapter" data-level="3.2.3" data-path="individual-differences.html"><a href="individual-differences.html#musicians-cognitive-advantage"><i class="fa fa-check"></i><b>3.2.3</b> Musician’s Cognitive Advantage</a></li>
<li class="chapter" data-level="3.2.4" data-path="individual-differences.html"><a href="individual-differences.html#relationship-established"><i class="fa fa-check"></i><b>3.2.4</b> Relationship Established</a></li>
<li class="chapter" data-level="3.2.5" data-path="individual-differences.html"><a href="individual-differences.html#dictation-without-dictation"><i class="fa fa-check"></i><b>3.2.5</b> Dictation Without Dictation</a></li>
<li class="chapter" data-level="3.2.6" data-path="individual-differences.html"><a href="individual-differences.html#cognitive-measures-of-interest"><i class="fa fa-check"></i><b>3.2.6</b> Cognitive Measures of Interest</a></li>
<li class="chapter" data-level="3.2.7" data-path="individual-differences.html"><a href="individual-differences.html#structural-equation-modeling"><i class="fa fa-check"></i><b>3.2.7</b> Structural Equation Modeling</a></li>
<li class="chapter" data-level="3.2.8" data-path="individual-differences.html"><a href="individual-differences.html#hypotheses"><i class="fa fa-check"></i><b>3.2.8</b> Hypotheses</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="individual-differences.html"><a href="individual-differences.html#overview-of-experiment"><i class="fa fa-check"></i><b>3.3</b> Overview of Experiment</a><ul>
<li class="chapter" data-level="3.3.1" data-path="individual-differences.html"><a href="individual-differences.html#participants"><i class="fa fa-check"></i><b>3.3.1</b> Participants</a></li>
<li class="chapter" data-level="3.3.2" data-path="individual-differences.html"><a href="individual-differences.html#materials"><i class="fa fa-check"></i><b>3.3.2</b> Materials</a></li>
<li class="chapter" data-level="3.3.3" data-path="individual-differences.html"><a href="individual-differences.html#procedure"><i class="fa fa-check"></i><b>3.3.3</b> Procedure</a></li>
<li class="chapter" data-level="3.3.4" data-path="individual-differences.html"><a href="individual-differences.html#results"><i class="fa fa-check"></i><b>3.3.4</b> Results</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="individual-differences.html"><a href="individual-differences.html#discussion"><i class="fa fa-check"></i><b>3.4</b> Discussion</a><ul>
<li class="chapter" data-level="3.4.1" data-path="individual-differences.html"><a href="individual-differences.html#model-fits"><i class="fa fa-check"></i><b>3.4.1</b> Model Fits</a></li>
<li class="chapter" data-level="3.4.2" data-path="individual-differences.html"><a href="individual-differences.html#relating-to-melodic-dictation"><i class="fa fa-check"></i><b>3.4.2</b> Relating to Melodic Dictation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="computation-chapter.html"><a href="computation-chapter.html"><i class="fa fa-check"></i><b>4</b> Computation Chapter</a><ul>
<li class="chapter" data-level="4.1" data-path="computation-chapter.html"><a href="computation-chapter.html#rationale-2"><i class="fa fa-check"></i><b>4.1</b> Rationale</a></li>
<li class="chapter" data-level="4.2" data-path="computation-chapter.html"><a href="computation-chapter.html#agreeing-on-complexity"><i class="fa fa-check"></i><b>4.2</b> Agreeing on Complexity</a><ul>
<li class="chapter" data-level="4.2.1" data-path="computation-chapter.html"><a href="computation-chapter.html#methods"><i class="fa fa-check"></i><b>4.2.1</b> Methods</a></li>
<li class="chapter" data-level="4.2.2" data-path="computation-chapter.html"><a href="computation-chapter.html#agreement-among-peagogues"><i class="fa fa-check"></i><b>4.2.2</b> Agreement Among Peagogues</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="computation-chapter.html"><a href="computation-chapter.html#modeling-complexity"><i class="fa fa-check"></i><b>4.3</b> Modeling Complexity</a><ul>
<li class="chapter" data-level="4.3.1" data-path="computation-chapter.html"><a href="computation-chapter.html#what-are-features"><i class="fa fa-check"></i><b>4.3.1</b> What Are Features?</a></li>
<li class="chapter" data-level="4.3.2" data-path="computation-chapter.html"><a href="computation-chapter.html#back-to-the-classroom"><i class="fa fa-check"></i><b>4.3.2</b> Back to the Classroom</a></li>
<li class="chapter" data-level="4.3.3" data-path="computation-chapter.html"><a href="computation-chapter.html#dynamic"><i class="fa fa-check"></i><b>4.3.3</b> Dynamic</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="computation-chapter.html"><a href="computation-chapter.html#frequency-facilitation-hypothesis"><i class="fa fa-check"></i><b>4.4</b> Frequency Facilitation Hypothesis</a><ul>
<li class="chapter" data-level="4.4.1" data-path="computation-chapter.html"><a href="computation-chapter.html#corpus-analysis"><i class="fa fa-check"></i><b>4.4.1</b> Corpus Analysis</a></li>
<li class="chapter" data-level="4.4.2" data-path="computation-chapter.html"><a href="computation-chapter.html#implications"><i class="fa fa-check"></i><b>4.4.2</b> Implications</a></li>
<li class="chapter" data-level="4.4.3" data-path="computation-chapter.html"><a href="computation-chapter.html#limitations-of-ffh"><i class="fa fa-check"></i><b>4.4.3</b> Limitations of FFH</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="computation-chapter.html"><a href="computation-chapter.html#conclusions-1"><i class="fa fa-check"></i><b>4.5</b> Conclusions</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="chapterfour.html"><a href="chapterfour.html"><i class="fa fa-check"></i><b>5</b> Hello, Corpus</a><ul>
<li class="chapter" data-level="5.1" data-path="chapterfour.html"><a href="chapterfour.html#rationale-3"><i class="fa fa-check"></i><b>5.1</b> Rationale</a></li>
<li class="chapter" data-level="5.2" data-path="chapterfour.html"><a href="chapterfour.html#history"><i class="fa fa-check"></i><b>5.2</b> History</a></li>
<li class="chapter" data-level="5.3" data-path="chapterfour.html"><a href="chapterfour.html#melosol-corpus"><i class="fa fa-check"></i><b>5.3</b> MeloSol Corpus</a></li>
<li class="chapter" data-level="5.4" data-path="chapterfour.html"><a href="chapterfour.html#comparison-of-corpora"><i class="fa fa-check"></i><b>5.4</b> Comparison of Corpora</a><ul>
<li class="chapter" data-level="5.4.1" data-path="chapterfour.html"><a href="chapterfour.html#corpus-analysis-1"><i class="fa fa-check"></i><b>5.4.1</b> Corpus Analysis</a></li>
<li class="chapter" data-level="5.4.2" data-path="chapterfour.html"><a href="chapterfour.html#discussion-1"><i class="fa fa-check"></i><b>5.4.2</b> Discussion</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="experiment.html"><a href="experiment.html"><i class="fa fa-check"></i><b>6</b> Experiment</a><ul>
<li class="chapter" data-level="6.1" data-path="experiment.html"><a href="experiment.html#rationale-4"><i class="fa fa-check"></i><b>6.1</b> Rationale</a></li>
<li class="chapter" data-level="6.2" data-path="experiment.html"><a href="experiment.html#introduction"><i class="fa fa-check"></i><b>6.2</b> Introduction</a><ul>
<li class="chapter" data-level="6.2.1" data-path="experiment.html"><a href="experiment.html#memory-for-melodies-1"><i class="fa fa-check"></i><b>6.2.1</b> Memory for Melodies</a></li>
<li class="chapter" data-level="6.2.2" data-path="experiment.html"><a href="experiment.html#musical-factors-1"><i class="fa fa-check"></i><b>6.2.2</b> Musical Factors</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="experiment.html"><a href="experiment.html#methods-1"><i class="fa fa-check"></i><b>6.3</b> Methods</a><ul>
<li class="chapter" data-level="6.3.1" data-path="experiment.html"><a href="experiment.html#participants-1"><i class="fa fa-check"></i><b>6.3.1</b> Participants</a></li>
<li class="chapter" data-level="6.3.2" data-path="experiment.html"><a href="experiment.html#materials-1"><i class="fa fa-check"></i><b>6.3.2</b> Materials</a></li>
<li class="chapter" data-level="6.3.3" data-path="experiment.html"><a href="experiment.html#procedure-1"><i class="fa fa-check"></i><b>6.3.3</b> Procedure</a></li>
<li class="chapter" data-level="6.3.4" data-path="experiment.html"><a href="experiment.html#scoring-melodies"><i class="fa fa-check"></i><b>6.3.4</b> Scoring Melodies</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="experiment.html"><a href="experiment.html#results-1"><i class="fa fa-check"></i><b>6.4</b> Results</a><ul>
<li class="chapter" data-level="6.4.1" data-path="experiment.html"><a href="experiment.html#data-screening"><i class="fa fa-check"></i><b>6.4.1</b> Data Screening</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="experiment.html"><a href="experiment.html#discussion-2"><i class="fa fa-check"></i><b>6.5</b> Discussion</a></li>
<li class="chapter" data-level="6.6" data-path="experiment.html"><a href="experiment.html#conculusions"><i class="fa fa-check"></i><b>6.6</b> Conculusions</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="computational-model.html"><a href="computational-model.html"><i class="fa fa-check"></i><b>7</b> Computational Model</a><ul>
<li class="chapter" data-level="7.1" data-path="computational-model.html"><a href="computational-model.html#levels-of-abstraction"><i class="fa fa-check"></i><b>7.1</b> Levels of Abstraction</a></li>
<li class="chapter" data-level="7.2" data-path="computational-model.html"><a href="computational-model.html#model-overview"><i class="fa fa-check"></i><b>7.2</b> Model Overview</a></li>
<li class="chapter" data-level="7.3" data-path="computational-model.html"><a href="computational-model.html#verbal-model"><i class="fa fa-check"></i><b>7.3</b> Verbal Model</a><ul>
<li class="chapter" data-level="7.3.1" data-path="computational-model.html"><a href="computational-model.html#model-representational-assumptions"><i class="fa fa-check"></i><b>7.3.1</b> Model Representational Assumptions</a></li>
<li class="chapter" data-level="7.3.2" data-path="computational-model.html"><a href="computational-model.html#contents-of-the-prior-knowledge"><i class="fa fa-check"></i><b>7.3.2</b> Contents of the Prior Knowledge</a></li>
<li class="chapter" data-level="7.3.3" data-path="computational-model.html"><a href="computational-model.html#modeling-information-content"><i class="fa fa-check"></i><b>7.3.3</b> Modeling Information Content</a></li>
<li class="chapter" data-level="7.3.4" data-path="computational-model.html"><a href="computational-model.html#setting-limits-with-transcribe"><i class="fa fa-check"></i><b>7.3.4</b> Setting Limits with Transcribe</a></li>
<li class="chapter" data-level="7.3.5" data-path="computational-model.html"><a href="computational-model.html#pattern-matching"><i class="fa fa-check"></i><b>7.3.5</b> Pattern Matching</a></li>
<li class="chapter" data-level="7.3.6" data-path="computational-model.html"><a href="computational-model.html#dictation-re-entry"><i class="fa fa-check"></i><b>7.3.6</b> Dictation Re-Entry</a></li>
<li class="chapter" data-level="7.3.7" data-path="computational-model.html"><a href="computational-model.html#completion"><i class="fa fa-check"></i><b>7.3.7</b> Completion</a></li>
<li class="chapter" data-level="7.3.8" data-path="computational-model.html"><a href="computational-model.html#model-output"><i class="fa fa-check"></i><b>7.3.8</b> Model Output</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="computational-model.html"><a href="computational-model.html#formal-model"><i class="fa fa-check"></i><b>7.4</b> Formal Model</a><ul>
<li class="chapter" data-level="7.4.1" data-path="computational-model.html"><a href="computational-model.html#computational-model-1"><i class="fa fa-check"></i><b>7.4.1</b> Computational Model</a></li>
<li class="chapter" data-level="7.4.2" data-path="computational-model.html"><a href="computational-model.html#example"><i class="fa fa-check"></i><b>7.4.2</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="computational-model.html"><a href="computational-model.html#conclusions-2"><i class="fa fa-check"></i><b>7.5</b> Conclusions</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="reference-log.html"><a href="reference-log.html"><i class="fa fa-check"></i><b>8</b> Reference Log</a><ul>
<li class="chapter" data-level="8.1" data-path="reference-log.html"><a href="reference-log.html#to-incorporate"><i class="fa fa-check"></i><b>8.1</b> To Incorporate</a></li>
<li class="chapter" data-level="8.2" data-path="reference-log.html"><a href="reference-log.html#chapter-3"><i class="fa fa-check"></i><b>8.2</b> Chapter 3</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">MODELING MELODIC DICTATION</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="intro" class="section level1">
<h1><span class="header-section-number">Chapter 2</span> Theoretical Background and Rationale</h1>
<div id="melodic-dictation" class="section level2">
<h2><span class="header-section-number">2.1</span> Melodic Dictation</h2>
<p>Melodic dictation is the process in which an individual hears a melody, retains it in memory, and then uses their knowledge of Western musical notation to recreate the mental image of the melody on paper in a limited time frame.
For many, becoming proficient at this task is at the core of developing one’s aural skills <span class="citation">(Karpinski <a href="#ref-karpinskiModelMusicPerception1990">1990</a>)</span>.
For over a century, music pedagogues have valued melodic dictation<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a> which is evident from the fact that most aural skills texts with content devoted to honing one’s listening skills have sections on melodic dictation <span class="citation">(Karpinski <a href="#ref-karpinskiAuralSkillsAcquisition2000">2000</a>)</span>.</p>
<p>Yet despite this tradition and ubiquity, the rationales as to <em>why</em> it is important for students to learn this ability often comes from some sort of appeal to tradition or underwhelming anecdotal evidence.
The argument tends to go that time spent learning to take melodic dictation results in increases in near transfer abilities after an individual acquires a certain degree of proficiency learning to take melodic dictation.
Rationales given for why students should learn melodic dictation has even been described by Karpinski as being based on “comparatively vague aphorisms about mental relationships and intelligent listening” <span class="citation">(Karpinski <a href="#ref-karpinskiModelMusicPerception1990">1990</a>, 192)</span>, thus leaving the evidence for the argument for learning to take melodic dictation not being well supported.</p>
<p>Some researchers have taken a more skeptical stance and asserted that the rationale for why we teach melodic dictation deserves more critique.
For example, Klonoski in writing about aural skills education aptly questions “What specific deficiency is revealed with an incorrect response in melodic dictation settings?” <span class="citation">(Klonoski <a href="#ref-klonoskiImprovingDictationAuralSkills2006">2006</a>)</span>.
Earlier researchers like Potter, in their own publications, have noted how they have been baffled that many musicians do not actually keep up with their melodic dictation abilities after their formal education ends <span class="citation">(Potter <a href="#ref-potterIdentifyingSucessfulDictation1990">1990</a>)</span>, but presumably go on to have successful and fulfilling musical lives.
Additionally, suggesting that people who can hear music and then are unable to write it down, thus are unable to think <em>in</em> music <span class="citation">(Karpinski <a href="#ref-karpinskiAuralSkillsAcquisition2000">2000</a>)</span>, seems somewhat exclusionary to musical cultures that do not depend on any sort of written notation.</p>
<p>Though despite this skepticism towards the topic, melodic dictation remains at the forefront of many aural skills classrooms.
The act of becoming better at this skill may or may not lead to large in increases in far transfer of ability, but used as a pedagogical tool, the practice of learning to take melodic dictation intersects with concepts deemed relevant to the core of undergraduate music training.
While there has not been extensive research on melodic dictation research in recent years– in fact <span class="citation">Paney (<a href="#ref-paneyEffectDirectingAttention2016">2016</a>)</span> notes that since 2000, only four studies were published that directly examined melodic dictation– this skill set sits on the border between literature on music learning, melodic perception, memory, and music theory pedagogy.
Understanding and modeling the processes underlying melodic dictation remains as a untapped watershed of knowledge for the field of music theory, music education, and music perception and is deserving of more attention.</p>
<p>In this chapter I examine literature both directly and indirectly related to melodic dictation by first reviewing the prominent four-step model put forth by Karpinski in order to establish and describe what melodic dictation is.
After describing his model, I then critique what this model lacks and clarify what is missing by providing a taxonomy of parameters that presumably would contribute to an individual’s ability to take melodic dictation.
Using this taxonomy, I then review relevant literature and assert that the next steps forward in understanding how melodic dictation works come from examining the process both experimentally and computationally.
It has been nearly two decades since <em>Aural Skills Acquisition</em> was first published as the first major step to finally build a bridge between the field of music cognition and music theory pedagogy <span class="citation">(Butler <a href="#ref-davidbutlerWhyGulfMusic1997a">1997</a>; Karpinski <a href="#ref-karpinskiAuralSkillsAcquisition2000">2000</a>; Klonoski <a href="#ref-klonoskiPerceptualLearningHierarchy2000">2000</a>)</span> and as with all public works, this infrastructure deserves attention and support.</p>
<div id="describing-melodic-dictation" class="section level3">
<h3><span class="header-section-number">2.1.1</span> Describing Melodic Dictation</h3>
<p>The foundational pedagogical work on melodic dictation comes from the work of Gary Karpinski.
Summarized most recently in his <em>Aural Skills Acquisition</em> <span class="citation">(Karpinski <a href="#ref-karpinskiAuralSkillsAcquisition2000">2000</a>)</span>– though first presented in an earlier article <span class="citation">(Karpinski <a href="#ref-karpinskiModelMusicPerception1990">1990</a>)</span>– Karpinski proposes a four-step model of melodic dictation.</p>
<p>The four steps of Karpinski’s model include</p>
<ol style="list-style-type: decimal">
<li>Hearing</li>
<li>Short Term Melodic Memory</li>
<li>Musical Understanding</li>
<li>Notation</li>
</ol>
<p>and occur as a looping process which I have reproduced depicted in Figure <a href="intro.html#fig:flowchart">2.1</a>.
The model is Karpinski’s take on previous attempts to summarize the process.
Previous attempts to distill melodic dictation into a series of discrete steps have ranged from Michael Roger’s assertion of only needing two steps, to Ronald Thomas who claimed as many as 15 steps, to similar models proposed by Colin Wright that model inner hearing as a five step model <span class="citation">(Wright <a href="#ref-wrightInvestigatingAuralCase2016">2016</a>; Karpinski <a href="#ref-karpinskiAuralSkillsAcquisition2000">2000</a>)</span>.
Karpinski’s model is discussed extensively in both the original article <span class="citation">(Karpinski <a href="#ref-karpinskiModelMusicPerception1990">1990</a>)</span> and throughout the third chapter in his book <span class="citation">(Karpinski <a href="#ref-karpinskiAuralSkillsAcquisition2000">2000</a>)</span>.</p>
<div class="figure" style="text-align: center"><span id="fig:flowchart"></span>
<img src="img/karpinski31.png" alt="Karpinski Idealized Flowchart of Melodic Dictation" width="100%" />
<p class="caption">
Figure 2.1: Karpinski Idealized Flowchart of Melodic Dictation
</p>
</div>
<p>Karpinski’s hearing stage involves the initial perceptions of the sound at the psychoacoustical level and the listener’s attention to the incoming musical information.
If the listener is not actively engaging in the task because of extrinsic factors such as “boredom, lack of discipline, test anxiety, attention deficit disorder, or any number of other causes.” <span class="citation">(Karpinski <a href="#ref-karpinskiAuralSkillsAcquisition2000">2000</a>, 65)</span> then any further processes later down the model will be detrimentally affected.
Karpinski notes that these types of interferences are normally “beyond the traditional jurisdiction of aural skills instruction” <span class="citation">(Karpinski <a href="#ref-karpinskiAuralSkillsAcquisition2000">2000</a>, 65)</span>, but I will later argue that the concept of willful attention, when re-conceptualized as working memory, may actually play a larger role in the melodic dictation process as it is modeled here.</p>
<p>The short-term melodic memory stage in this process references the point in a melodic dictation where musical material is held in active memory.
From Figure <a href="intro.html#fig:flowchart">2.1</a> and Karpinski’s writing on the model, this stage is not explicitly declared as any sort of active process akin to a phonological loop <span class="citation">(Baddeley <a href="#ref-baddeleyEpisodicBufferNew2000">2000</a>)</span> where active rehearsal would occur, but describes where in the sequential order melodic information is represented.
Though Karpinski does not posit any sort of active process in the short term melodic memory stage, he does suggest there are two separate memory encoding mechanisms, one for contour, and one for pitch.
He arrives at these two mechanisms by using both empirical qualitative interview evidence, as well as noting literature from music perception that supports this claim for contour <span class="citation">(Dowling <a href="#ref-dowlingScaleContourTwo1978">1978</a>; Dewitt and Crowder <a href="#ref-dewittRecognitionNovelMelodies1986">1986</a>)</span> and literature suggesting that memory for melodic material is dependent on enculturation <span class="citation">(Oura and Hatano <a href="#ref-ouraMemoryMelodiesSubjects1988">1988</a>; Handel <a href="#ref-handelListeningIntroductionPerception1989">1989</a>; Dowling <a href="#ref-dowlingExpectancyAttentionMelody1990">1990</a>)</span>.
Since its publication in 2000, this area of research has expanded with other researchers also demonstrating the effects of musical enculturation via exposure <span class="citation">(Eerola, Louhivuori, and Lebaka <a href="#ref-eerolaExpectancySamiYoiks2009">2009</a>; Stevens <a href="#ref-stevensMusicPerceptionCognition2012">2012</a>; Pearce and Wiggins <a href="#ref-pearceAuditoryExpectationInformation2012">2012</a>; Pearce <a href="#ref-pearceStatisticalLearningProbabilistic2018a">2018</a>)</span>.</p>
<p>In describing the short term melodic memory stage, Karpinski also details two processes that he believes to be necessary for this part of melodic dictation: extractive listening and chunking.
Noting that there is a capacity limit to the perception of musical material; citing Miller <span class="citation">(<a href="#ref-millerMagicalNumberSeven1956">1956</a>)</span>, Karpinski explains how each strategy might be used.
Extractive listening is the process in which someone dictating the melody will selectively remember only a small part of the melody in order to lessen the load on memory.
Chunking is the process in which smaller musical elements can be fused together in order to expand how much information can be actively held in memory and manipulated.
The concept of chunking is very helpful as a pedagogical tool, but as detailed below, is complicated to formalize.</p>
<p>After musical material is extracted and then represented in memory, the next step in the process is musical understanding.
At this point in the dictation, the individual taking the dictation needs to mentalize the extracted musical material that is represented in memory and the use their music theoretic knowledge in order to comprehend any sort of hierarchical relationships between notes, common rhythmic groupings, or any sorts of tonal functions.
This is the point in the process where solmization of either or both pitch and rhythm, and musical material might be understood in terms of relative pitch.
While Karpinski reserves his discussion of solmization for the musical understanding phase, it is worth questioning if it is possible to disassociate relative pitch relations that would be ‘understood’ in this phase from the qualia of the tones themselves <span class="citation">(Arthur <a href="#ref-arthurPerceptualStudyScaledegree2018">2018</a>)</span>.
For Karpinski, the quicker what is represented in musical memory can be understood, the quicker it can then be translated at the final step of notation.</p>
<p>Notation, the final step of the dictation loop, requires that the individual taking the notation have sufficient knowledge of Western musical notation so that they are able to translate their musical understanding into written notation.
This last step is ripe for errors and has proved problematic for researchers attempting to study dictation <span class="citation">(Taylor and Pembrook <a href="#ref-taylorStrategiesMemoryShort1983">1983</a>; Klonoski <a href="#ref-klonoskiImprovingDictationAuralSkills2006">2006</a>)</span>.
It is also worth highlighting is that it is difficult to notate musical material if the individual who is dictating does not have the requisite musical category and knowledge for the sounds.
Lack of this knowledge will limit an individual’s ability to translate what is in their short term melodic memory into notation, even if it is perfectly represented in memory.</p>
<p>The final parts of the chapter, Karpinski notes that other factors like tempo, the length and number of playings, and the duration between playings will also play a role in determining how an individual will perform on a melodic dictation.
While this framework can help illuminate this cognitive process and help pedagogues understand how to best help their students, presumably there are many more factors that contribute to this process.
The model as it stands is not detailed enough for explanatory purposes and lacks in two areas that would need to be expanded if this model were to be explored experimentally and computationally.</p>
<p>First, having a single model for melodic dictation assumes that all individuals are likely to engage in this sequential ordering of events.
This could in fact be the case<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a>, but there is research from music perception <span class="citation">(Goldman, Jackson, and Sajda <a href="#ref-goldmanImprovisationExperiencePredicts2018a">2018</a>)</span> and other areas of memory psychology such as work on expert chess players <span class="citation">(Lane and Chang <a href="#ref-laneChessKnowledgePredicts2018">2018</a>)</span> that suggests that as individuals gain more expertise in a specific domain, their processing and categorization of information changes.
Additionally, different individuals will most likely have different experiences dictating melodies based on their own past listening experience, an area that Karpinski refers to when citing literature on musical enculturation based on statistical exposure.
The model does not have any flexibility in terms of individual differences.</p>
<p>Second, the model presumes the same sequence of events for every melody.
As a general heuristic for communicating the process, this model serves as an excellent didactic tool.
Though when this model is applied to more diverse repertoire, this same set of strategies performed in this order might prove to be inefficient.
For example, on page 103 of his text, Karpinski suggest that two listenings should be adequate for a listener with few to no chunking skills to listen to be able to dictate a melody of twelve to twenty notes.
This process might generalize to many tonal melodies, but presumably different strategies in recognition would be involved in dictating the two melodies of equal length shown in Figure <a href="intro.html#fig:shortmelody1">2.2</a> and <a href="intro.html#fig:shortmelody2">2.3</a>.
If asked to dictate <a href="intro.html#fig:shortmelody1">2.2</a>, long term memory processes might begin to play a role much sooner during this task.
If asked to dictate <a href="intro.html#fig:shortmelody2">2.3</a>, establishing a tonal center to act as a perceptual scaffolding for relative relationships might prove to be more difficult.
Presumably different people with different levels of abilities will perform differently on different melodies and while helpful as a pedagogical tool, this generalized approach to melodic dictation could be more robust.</p>
<div class="figure" style="text-align: center"><span id="fig:shortmelody1"></span>
<img src="img/musicalexamples/MMD_Figure2-1.png" alt="Tonal Melody" width="80%" />
<p class="caption">
Figure 2.2: Tonal Melody
</p>
</div>
<div class="figure" style="text-align: center"><span id="fig:shortmelody2"></span>
<img src="img/musicalexamples/MMD_Figure3-1.png" alt="Atonal Melody" width="80%" />
<p class="caption">
Figure 2.3: Atonal Melody
</p>
</div>
<p>This agnosticism for both variability for melodic and individual differences serves as a stepping off point for this study.
In order to have a more complete understanding of melodic dictation, there needs to be a model that is able to accommodate the exhaustive differences at both the individual and musical levels.
Additionally, the model should be able to be operationalized so that it can be explored in both experimental and computational settings.
By explicitly stating variables thought to contribute and understanding the underlying processes of melodic dictation, it will give aural skills pedagogues a better sense of the melodic dictation process, which will then enable a more complete understanding of melodic perception and subsequently allow for better teaching practices in aural skills classrooms.</p>
</div>
<div id="taxonomizing" class="section level3">
<h3><span class="header-section-number">2.1.2</span> Taxonomizing</h3>
<p>At this point, it is worth stepping back and noting that the sheer amount of variables at play here is cumbersome and haphazard.
In order to better understand and organize factors thought to contribute to this process, it would be advantageous to taxonomize the multitude of features thought to contribute to melodic dictation.
In doing this, it will allow for a clearer picture of what factors might contribute and what literatures to explore in order to learn more about them.</p>
<p>The taxonomy that I propose appears in Figure <a href="intro.html#fig:taxonomy">2.4</a> and bifurcates the possible factors thought to affect an individual’s ability to take melodic dictation create both individual parameters and musical parameters.
These categories are recursively partitioned into cognitive and environmental parameters as well as structural and experimental factors respectively.
Below I expand on what these categories entail, then explore each in-depth.</p>
<div class="figure" style="text-align: center"><span id="fig:taxonomy"></span>
<img src="img/taxonomy4.png" alt="Taxonomy of Factors Contributing to Aural Skills" width="80%" />
<p class="caption">
Figure 2.4: Taxonomy of Factors Contributing to Aural Skills
</p>
</div>
<p>The individual parameters are split broadly into cognitive factors and environmental factors.
Factors in the cognitive domain are assumed to be relatively consistent over time.
Factors in the environmental domain are subject to change via training and exposure.
These categories are not deterministic, nor exclusive, and almost inevitably interact with one another.</p>
<p>For example, it would be possible to imagine an individual with higher cognitive ability, the opportunity to have a high degree of training early on in their musical career, and personality traits that lead them to enjoy engaging with a task like melodic dictation.
This individual’s musical perception abilities might be markedly different than someone with lower cognitive abilities, no opportunity for individualized training, and not have a general inclination to even take music lessons.
This variability at the individual level might then lead to differences in their ability to take melodic dictation.</p>
<p>Complementing the individual differences, there would also be differences at the musical level which in turn divides into two categories.
On one hand exists the structural aspects of the melody itself.
These are aspects of the melody that would remain invariant when written down on musical notation that can only capture pitch changes over time.
Parameters in this category would include features generated by the interval structure of the pitches over time that allow the melody to be perceived as categorically distinct from other melodies.
These structural features are then complimented by the experimental features which are emergent properties of the structural relation of the pitches over time based on performance practice choices.
Examples of these parameters would include, key, tempo, note density, tonalness, timbral qualities, and the amount of times a melody is played during a melodic dictation.
Again, this division is not an exhaustive, categorical divide.
One could imagine exceptions to these rules where a melody is transformed to the minor key, ornamented, and then played with extensive rubato and experienced as a phenomenologically distinct, yet similar experience.
This division of structural and experimental features is morphologically similar to Leonard Meyer’s primary and secondary musical features <span class="citation">(Meyer <a href="#ref-meyerEmotionMeaningMusic1956">1956</a>)</span>.</p>
<p>Given all of these parameters that could contribute to the melodic dictation process, the remainder of this chapter will exploring literature using this taxonomy as a guide.
The chapter concludes with a reflection on operationalizing each of these factors and problems that can arise in modeling and reminds the reader about the dangers of statistical reification.
These are important to note since from an empirical standpoint, both the task as well as the process of melodic dictation as depicted by Karpinski resemble something that could be operationalized as both an experiment, as well as a computational model.</p>
</div>
</div>
<div id="individual-factors" class="section level2">
<h2><span class="header-section-number">2.2</span> Individual Factors</h2>
<div id="cognitive" class="section level3">
<h3><span class="header-section-number">2.2.1</span> Cognitive</h3>
<p>Research from cognitive psychology suggests that individuals differ in their perceptual and cognitive abilities in ways that are both stable throughout a lifetime and are not easily influenced by short term training.
When investigated on a large scale, these abilities– such as general intelligence or working memory capacity– predict a wealth of human behavior on a large scale ranging from longevity, annual income, ability to deal with stressful life events, and even the onset of Alzheimer’s disease <span class="citation">(Ritchie <a href="#ref-ritchieIntelligenceAllThat2015">2015</a>; Unsworth et al. <a href="#ref-unsworthAutomatedVersionOperation2005">2005</a>)</span>.
Given the strength and generality of these predictors, it is worth investigating the extent that these abilities might contribute when investigating any modeling of melodic dictation since melodic dictation depends on perceptual abilities.
It is important to understand the degree to which these cognitive factors might influence aural skills abilities in order to ensure that the types of assessments that are given in music schools validly measure abilities that individuals have the ability to improve.
If it is the case that much of the variance in a student’s aural skills academic performance can be attributed to something the student has little control over, this would call for a serious upheaval of the current model of aural skills teaching and assessment.</p>
<p>Recently there has been a surge of interest in work exploring how cognitive factors are related to abilities in music school.
This interest is probably best explained by the fact that educators are picking up on the fact that cognitive abilities are powerful predictors and need to be understood since they inevitably will play a role in pedagogical settings.</p>
<p>Before diving into a discussion regarding differences in cognitive ability, I should note that sometimes ideas regarding differences in cognitive ability been negatively received and for good reasons.
Research in this area can and has been taken advantage to further specious ideologies, but often arguments that assert meaningful differences in cognitive abilities between groups are founded on statistical misunderstandings and have been debunked in other literature <span class="citation">(Gould <a href="#ref-gouldMismeasureMan1996">1996</a>)</span>.
Considering that, it then becomes very difficult to maintain a scientific commitment to the theory of evolution <span class="citation">(Darwin <a href="#ref-darwinOriginSpecies1859">1859</a>)</span> and not expect variation in all aspects of human behavior, with cognition falling within that umbrella.
Even given this statement, measuring a theoretical construct such as an aspect of cognition deserves to be examined since the ability to validly and reliably measure an individual’s cognitive abilities is a fundamental assumption of this study.</p>
<div id="intelligence" class="section level4">
<h4><span class="header-section-number">2.2.1.1</span> Intelligence</h4>
<p>Attempting to measure and quantify aspects of cognition date back over a century.
Even before concepts of intelligence were posited by Charles Spearman via his conception of <em>g</em> <span class="citation">(Spearman <a href="#ref-spearmanGeneralIntelligenceObjectively1904">1904</a>)</span>, scientists were interested in establishing links between latent constructs they presumed to exist in the real world– yet were impossible to measure directly like intelligence– and physical manifestations in that could be measured such as body morphology <span class="citation">(Gould <a href="#ref-gouldMismeasureMan1996">1996</a>)</span>.
While scholars like Gould have documented and critiqued much of the history of early psychometrics<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a>, central to this study are two important schools of thought on intelligence testing commonly discussed in the current literature.</p>
<p>The first ideology originates from Cyril Burt and Charles Spearman who, in developing the statistical tool of factor analysis, posited that a construct of general intelligence exists as a part of human cognition and can be quantified.
Burt and Spearman claimed that a general intelligence factor existed in human cognition from evidence they found developing a batty of cognitive tests whose performance on one subtest could often reliably predict performance on another.
This phenomena of multiple related tests predicting each other’s performance is a manifestation referred to as the positive manifold.
Spearman and Burt asserted that an individual’s ability to solve problems without contextual background information could be understood as general intelligence or <em>g</em>.</p>
<p>Broadly speaking, the second ideology here stems from work by Alfred Binet who instead of conceptualizing intelligence as a monolithic whole, partitioned intelligence into what have today become understood to be defined as differences in general crystallized intelligence or ( <em>Gc</em> ) and general fluid intelligence ( <em>Gf</em> ).
General crystallized intelligence is the ability to solve problems given prior contextual information;
General fluid intelligence is the ability to solve problems in novel contexts <span class="citation">(Cattell <a href="#ref-cattellAbilitiesTheirGrowth1971">1971</a>; J Horn <a href="#ref-jhornTheoryFluidCrystalized1994">1994</a>)</span> .
Comparing <em>Gf</em> and <em>Gc</em> to <em>g</em>, the cognitive psychology literature has noted that <em>g</em> often shares a statistically equivalent relationship to idea conceptualized as general fluid intelligence <span class="citation">(Matzke, Dolan, and Molenaar <a href="#ref-matzkeIssuePowerIdentification2010">2010</a>)</span>.
These conceptions of intelligence and cognitive ability also differ from more current theories that synthesize these previous areas of research <span class="citation">(Kovacs and Conway <a href="#ref-kovacsProcessOverlapTheory2016">2016</a>)</span> using models that do not require taking an ontological stance of entity realism <span class="citation">(Borsboom, Mellenbergh, and van Heerden <a href="#ref-borsboomTheoreticalStatusLatent2003">2003</a>)</span>.</p>
<p>Even though both of these constructs are powerful predictors on a large scale and do predict variables such as educational success, income, and even life expectancy <span class="citation">(Ritchie <a href="#ref-ritchieIntelligenceAllThat2015">2015</a>)</span> when obvious confounding variables like socioeconomic status are held constant, only conceptualizing cognitive abilities in terms of even a handful of latent constructs still does not fully explain the diversity of human cognition.
Regardless of their origin, neglecting the predictive power of these variables in pedagogical settings would be a methodological oversight in attempting to explain variance in performance.</p>
</div>
<div id="working-memory-capacity" class="section level4">
<h4><span class="header-section-number">2.2.1.2</span> Working Memory Capacity</h4>
<p>Another second area in the field of cognitive psychology worthy of examination is working memory capacity.
In addition to concepts of intelligence, be it <em>Gf</em> or <em>Gc</em>, the working memory capacity literature directly relates to work on melodic dictation for reasons discussed below.</p>
<p>Working memory is one of the most investigated concepts in the cognitive psychology literature.
According to Nelson Cowan, the term working memory generally refers to</p>
<blockquote>
<p>the relatively small amount of information that one can hold in mind, attend to, or, technically speaking, maintain in a rapidly accessible state at one time. The term working is mean to indicate that mental work requires the use of such information. (p.1) <span class="citation">(Cowan <a href="#ref-cowanWorkingMemoryCapacity2005">2005</a>)</span></p>
</blockquote>
<p>The term, like most concepts in science, does not have an exact definition, nor does it have a definitive method of measurement.
While there is no universally recognized first use of the term, researchers began to postulate that there was some sort of system that mediated incoming sensory information with the world with the information in long term storage using modular models of memory in the mid-twentieth century.
Summarized in <span class="citation">Cowan (<a href="#ref-cowanWorkingMemoryCapacity2005">2005</a>)</span>, one of the first modal models of memory was proposed by <span class="citation">Broadbent (<a href="#ref-broadbentPerceptionCommunication1958">1958</a>)</span> and later expanded by <span class="citation">Atkinson and Shiffrin (<a href="#ref-atkinsonHUMANMEMORYPROPOSED1968">1968</a>)</span>.
As seen in Figure <a href="intro.html#fig:wmmodels">2.5</a> taken from <span class="citation">Cowan (<a href="#ref-cowanWorkingMemoryCapacity2005">2005</a>)</span>, both models here posit incoming information that is then put into some sort of limited capacity store.
These modal models were then expanded on by Baddeley and Hitch <span class="citation">(Baddeley and Hitch <a href="#ref-baddeleyWorkingMemory1974">1974</a>)</span> in their 1974 chapter with the name <em>Working Memory</em>, where they proposed a system with an central executive module that was able to carry out active maintenance and rehearsal of information that could be stored in either a phonological store for sounds or a visual sketchpad for images.</p>
<div class="figure" style="text-align: center"><span id="fig:wmmodels"></span>
<img src="img/wm_models.png" alt="Schematics of Models of Working Memory taken from Cowan, 2005" width="353" />
<p class="caption">
Figure 2.5: Schematics of Models of Working Memory taken from Cowan, 2005
</p>
</div>
<p>Later revisions of their model also incorporated an episodic buffer <span class="citation">(Baddeley <a href="#ref-baddeleyEpisodicBufferNew2000">2000</a>)</span> where the modules were explicitly depicted as being able to interface with long term memory in the rehearsal processes.
The model has even been expanded upon by other researchers throughout its lifetime.
The most relevant to this study is by <span class="citation">Berz (<a href="#ref-berzWorkingMemoryMusic1995">1995</a>)</span>, who postulated the addition of a musical rehearsal loop to the already established phonological loop and visual spatial sketchpad.
While Berz is most likely correct in asserting that the nature of storing and processing musical information is different to that of words or pictures and there has been experimental evidence to suggest this <span class="citation">(Williamson, Baddeley, and Hitch <a href="#ref-williamsonMusiciansNonmusiciansShortterm2010">2010</a>)</span> that has been interpreted in favor of multiple loops <span class="citation">(Wöllner and Halpern <a href="#ref-wollnerAttentionalFlexibilityMemory2016">2016</a>)</span>, it does introduce the theoretical problem of determining how and why incoming sensory information is partitioned into the respective loops.
Additionally, models that assert some sort of central executive component to attend to materials held in a sensory buffer also face the infinite regress homunculus problem.
Stated more clearly, if the central executive system is what attends to information in the sensory buffers, what attends to the central executive?</p>
<p>In addressing the problem of explicitly stating which rehearsal loops do and do not exist, Nelson Cowan proposed a separate model <span class="citation">(Cowan <a href="#ref-cowanEvolvingConceptionsMemory1988">1988</a>, <a href="#ref-cowanWorkingMemoryCapacity2005">2005</a>)</span> dubbed the Embedded Process Model which does not claim the existence of any domain specific module (e.g. positing a phonological loop, visual spatial sketchpad) but is rather based on an exhaustive model that did away with the problem of asserting specific buffers for specific types of information.</p>
<p>In Cowan’s own words comparing his model from that of Baddeley and Hitch:</p>
<blockquote>
<p>The aim was to see if the description of the processing structure could be exhaustive, even if not complete, in detail. By analogy, consider two descriptions of a house that has not been explored completely. Perhaps it has only been examined from the outside. Baddeley’s (1986) approach to modeling can be compared with hypothesizing that there is a kitchen, a bathroom, two equal-size square bedrooms, and a living room. This is not a bad guess, but it does not rule out the possibility that there actually are extra bedrooms or bathroom, that the bedroom space is apportioned into two rooms very different in size, or that other rooms exist in the house. Cowan’s (1988) approach, on the other hand, can be compared with hypothesizing that the house includes food preparation quarters, sleeping quarters, bathroom/toilet quarters, and other living quarters. It is meant to be exhaustive in that nothing was left out, even though it is noncommittal on the details of some of the rooms. (p.42) <span class="citation">(Cowan <a href="#ref-cowanWorkingMemoryCapacity2005">2005</a>)</span></p>
</blockquote>
<p>The system is depicted in the bottom tier of Figure <a href="intro.html#fig:wmmodels">2.5</a>, and conceptualizes the limited amount of information that is readily available as being in the focus of attention.
In this models, activated sensory and categorical features of the focus of attention are thus readily accessible.
Moving further from the locus of attention is long term memory, whose content can be turned to by using the central executive to access non-immediately available information.
The central executive system in this case acts as a spotlight on what is represented in long term memory, rather than a module used to direct attention to specific sensory information.
This change in definition does not completely escape the homunculus problem, but does change the central executive’s role in the memory process.
In contrast to the modular approaches, Cowan’s framework does not require the researchers to specify exactly how and where each the incoming information is being stored which makes it advantageous for studying complex stimuli such as music and melodies.
Using this definition of working memory would require collapsing the first two steps of the Karpinski model of melodic dictation into one step.</p>
<p>In addition to having multiple frameworks for studying working memory capacity, there is also the problem of limits to the working memory system, often referred to as the working memory capacity.
Most popularized by Miller in his famous <span class="citation">(George A Miller <a href="#ref-millerMagicalNumberSeven1956">1956</a>)</span> speech turned article, Miller suggests out of jest that the number 7 might be worthy of investigating in terms of how many items can be remembered, which has been used as a point of reference for many researchers since then.
It is worth nothing that Miller has since gone on record as noting that using 7 (plus or minus 2) was a rhetorical device used to string together his speech <span class="citation">(Miller <a href="#ref-millerHistoryPsychologyAutobiography1989">1989</a>)</span>.
Nevertheless, while the number seven is most likely a red herring, it did inspire a large amount of research on capacity limits.
In the decades since the number 7 has been reduced to about 4 <span class="citation">(Cowan <a href="#ref-cowanMagicalMysteryFour2010">2010</a>)</span> and research around capacity limits has been investigated using a variety of novel tasks, most notable the complex span task <span class="citation">(Unsworth et al. <a href="#ref-unsworthAutomatedVersionOperation2005">2005</a>, <a href="#ref-unsworthComplexWorkingMemory2009">2009</a>)</span>.<br />
When used as predictors in both higher and lower cognitive tasks, measures of working memory capacity predict performance well and additionally tend to be stable across a lifetime <span class="citation">(Unsworth et al. <a href="#ref-unsworthAutomatedVersionOperation2005">2005</a>)</span>.</p>
</div>
<div id="working-memory-and-melodic-dictation" class="section level4">
<h4><span class="header-section-number">2.2.1.3</span> Working Memory and Melodic Dictation</h4>
<p>Clearly an individual’s ability to take in sensory information, maintain it in memory, actively carry out other tasks (like notating a melody) are almost identical to tasks of working memory capacity.
Before venturing onward and further discussing the importance of this striking parallel, a few clear distinctions between methods used to study working memory and melodic dictation need to be made explicit.
While these two tasks resemble each other, a few key differences exist that researchers must note.</p>
<p>Tasks investigating working memory capacity differ from melodic dictation tasks in a few key ways.
The first is that musical information is always sequential: a melodic dictation task would never require the student to recall the pitches back in scrambled orders.
Serial order recall is an important characteristic in the scoring and analyzing of working memory tasks <span class="citation">(Conway et al. <a href="#ref-conwayWorkingMemorySpan2005">2005</a>)</span>, but musical tones do not appear in random order and are normally in discernible chunks as discussed by Karpinski <span class="citation">(Karpinski <a href="#ref-karpinskiAuralSkillsAcquisition2000">2000</a>)</span>.
The use of chunks is pervasive in much of the memory literature, but often is used as more of a heuristic to help explain that information in the environment and why it is often grouped together.
Of the problems with chunking, most are related to music and are related to melodic dictation.
Below I review the problems with chunking noted by <span class="citation">Cowan (<a href="#ref-cowanWorkingMemoryCapacity2005">2005</a>)</span>, and how each confound would manifest itself in music related research.</p>
<ol style="list-style-type: decimal">
<li><em>Chunks may have a hierarchical organization.</em> Tonal music has historically been understood to be hierarchical <span class="citation">(Krumhansl <a href="#ref-krumhanslCognitiveFoundationsMusical2001">2001</a>; Meyer <a href="#ref-meyerEmotionMeaningMusic1956">1956</a>; Schenker <a href="#ref-schenkerFreieSatz1935">1935</a>)</span> with the study for memory for tones being confounded by some pitches being understood by their relation to structurally more stable tones.</li>
<li><em>The working memory load may be reduced as working memory shifts between levels in hierarchy.</em> If an individual understands a chunk to be something such as a major triad, the load on working memory would be less since it that information could be understood as a singular chunk.</li>
<li><em>Chunks may be the endpoints of a continuum of associations.</em> Given tonal music’s sequential and statistical properties, two tones might be able to be loosely associated given a context which would make the tones fall between being identified as two tones and one distinct chunk.</li>
<li><em>Chunks may include asymmetrical information.</em> More tonal possibilities are possible from a stable note like tonic or dominant, whereas in a tonal context, a raised scale degree #<span class="math inline">\(\hat{4}\)</span> when understood in a functional context would be taken as having stricter transitional probabilities (#<span class="math inline">\(\hat{4} \rightarrow \hat{5}\)</span>).</li>
<li><em>There may be a complex network of associations.</em> If a set of pitches sounds like a similar set of pitches from long term memory, the incoming information cannot be understood as being separate units of working memory.</li>
<li><em>Chunks may increase in size rapidly over time.</em> Three tones that are seemingly unrelated when played sequentially such as E4, G5, C5 might enter sensory perception as three distinct tones, but then be fused together if understood as one chunk– a first inversion major triad.</li>
<li><em>Information in working memory may benefit from rapid storage in long term memory.</em> Given the amount of patterns that an individual learns and can understand, as soon as sensory information is fused, it could be encoded in long term memory. This is especially true if there is a salient feature in the incoming melodic information such as the immediate recognition of a mode or cadence.</li>
</ol>
<p>The points by Cowan are important to acknowledge in that it is not possible to directly lift work and paradigms from working memory capacity to work in music perception.
That said, the enormous amount of theoretical frameworks put forward by the working memory literature when understood in conjunction with theories in music psychology such as implicit statistical learning <span class="citation">(Saffran et al. <a href="#ref-saffranStatisticalLearningTone1999">1999</a>)</span> can provide for new, fruitful theories.
Past researchers have noted the strength and predictive abilities from the working memory capacity literature as aiding research in music perception.
In ending his article positing a musical memory loop to be annexed to the Baddley and Hitch modular model of working memory, <span class="citation">Berz (<a href="#ref-berzWorkingMemoryMusic1995">1995</a>)</span> captures the power of this concept in the last sentence of his article and warns:</p>
<blockquote>
<p>Individual differences portrayed in some music aptitude tests may [sic] represent not talent or musical intelligence but ability, reflecting differences in working memory capacity. p. 362</p>
</blockquote>
<p>Berz’s assertion has not been exhaustively tested since first published in 1995, but the subject of music, memory, and cognitive abilities has been the focus of research of both psychologists and musicologists alike.
Below I survey literature bordering on both music, as well as cognitive ability.</p>
</div>
<div id="working-memory-capacity-and-music" class="section level4">
<h4><span class="header-section-number">2.2.1.4</span> Working Memory Capacity and Music</h4>
<p>Of the papers in the music science literature that specifically investigates working memory, each uses different measures, though but all tend to converge on two general findings.
The first is that there are some sort of enhanced memory capabilities in individuals with musical training.
The second is that working memory capacity, however it is measured, often plays a significant role in musical tasks.
Evidence for the first point appears most convincingly in a recent meta analyses by Talamini and colleagues <span class="citation">(Talamini et al. <a href="#ref-talaminiMusiciansHaveBetter2017">2017</a>)</span> who demonstrated via three separate meta-analyses that musicians outperform their non-musical counterparts on tasks dealing with long-term memory, short-term memory, as well as working memory.
The authors also noted that the effects were the strongest in working memory tasks where the stimuli were tonal, which again suggests an advantage of exposure and understanding of the hierarchical organization of musical materials.
In this meta-analyses and others investigating music and cognitive ability, it is important to be reminded that the direction of causality still from these studies cannot be determined using these theoretical and statistical methodologies.
While it might seem that musical training tends to lead to these increases, it is also possible that higher functioning individuals will self select into musical activities.
Even if there is no selection bias in engaging with musical activity it also remains a possibility that of the people that do engage with musical activity, the higher functioning individuals will be less likely to quit over a lifetime.</p>
<p>In terms of musical performance abilities, working memory capacity has also been shown to be a significant predictor.
Kopiez and Lee suggested that working memory capacity should to contribute to sight reading tasks based on research where they found measures of working memory capacity, as measured by a matrix span task, to be significantly correlated with many of their measures hypothesized to be related to sight reading ability in pianists at lower difficulty grading <span class="citation">(Kopiez and Lee <a href="#ref-kopiezDynamicModelSkills2006">2006</a>, <a href="#ref-kopiezGeneralModelSkills2008">2008</a>)</span>.</p>
<p>Following up on this work on sight reading, Meinz and Hambrick <span class="citation">(Meinz and Hambrick <a href="#ref-meinzDeliberatePracticeNecessary2010">2010</a>)</span> found that working memory capacity, as measured by an operation span task, a reading span task, rotation span task, and a matrix span task was able to predict a small amount of variance <span class="math inline">\(R^2=.074(0.067)\)</span> above and beyond that of deliberate practice alone <span class="math inline">\(R^2=.451(.441)\)</span> in a sight-reading task.
More recently, two studies looking at specific sub-groups of musicians have shown working memory capacity to significantly contribute to models of performances on musical tasks related to novel stimuli.
<span class="citation">Wöllner and Halpern (<a href="#ref-wollnerAttentionalFlexibilityMemory2016">2016</a>)</span> found that although no differences were found between pianists and conductors in measures of working memory capacity as measured via a set of span tasks, conductors showed superior performance in their attention flexibility.
Following up on this line of research <span class="citation">Nichols, Wöllner, and Halpern (<a href="#ref-nicholsScoreOneJazz2018">2018</a>)</span> used the same battery of working memory tasks and found that jazz musicians excelled over their classically trained counterparts in a task which required them to hear notes and reproduce them on the piano.
The authors also noted that of their working memory battery, based on standard operation span methods <span class="citation">(Engle <a href="#ref-engleWorkingMemoryCapacity2002">2002</a>)</span>, that the auditory dictation condition scored surprisingly low and further research might consider further work on dictation abilities.
Additionally, <span class="citation">Colley, Keller, and Halpern (<a href="#ref-colleyWorkingMemoryAuditory2018">2018</a>)</span> found that working memory capacity, as measured by a backwards digit span and operation span, to be successful predictors in a tapping task requiring sensory motor prediction abilities.
As mentioned above, each of these tasks where working memory was a significant predictor of performance occurred where the task involved active engagement with novel musical material.</p>
<p>The growing evidence in this field suggests that the advantage of working memory capacity to be greatest in both musically trained people, dealing with novel information, using tonal materials.
Since all three of these factors are related to melodic dictation, it would seem sensible to continue to include these measures in tasks of musical perception and continue Berz’s assertion that research in music perception could inadvertently be picking up on individual differences in working memory abilities.</p>
</div>
<div id="intelligence-and-music" class="section level4">
<h4><span class="header-section-number">2.2.1.5</span> Intelligence and Music</h4>
<p>As discussed above, the idea of IQ or intelligence has a long and complex history.
When used as a predictor in statistical models, it often serves to predict traits that society values like longevity and general income so given its ability to predict in more domain general settings, surveying literature where it applies to musical activity warrants attention.
Below I use the term intelligence as a catch all term to avoid the historical context of IQ and specify where available which measure was actually used.
Before surveying the literature here it is also worth noting that research on music and intelligence is not as developed as some of the larger studies looking at intelligence which provides problems for both establishing causal directionality, as well as controlling for other factors like self theories of ability, socioeconomic status, and personality <span class="citation">(Müllensiefen et al. <a href="#ref-mullensiefenInvestigatingImportanceSelftheories2015">2015</a>)</span>.</p>
<p>As reviewed in <span class="citation">Schellenberg (<a href="#ref-schellenbergMusicNonmusicalAbilities2017">2017</a>)</span>, both children and adults who engage in musical activity tend to score higher on general measures of intelligence than their non-musical peers <span class="citation">(Gibson, Folley, and Park <a href="#ref-gibsonEnhancedDivergentThinking2009">2009</a>; Hille et al. <a href="#ref-hilleAssociationsMusicEducation2011">2011</a>; Schellenberg <a href="#ref-schellenbergExaminingAssociationMusic2011">2011</a>; Schellenberg and Mankarious <a href="#ref-schellenbergMusicTrainingEmotion2012">2012</a>)</span> with the duration of training sharing a relationship with the extent of the increases in IQ <span class="citation">(Corrigall and Schellenberg <a href="#ref-corrigallPredictingWhoTakes2015">2015</a>; Degé, Kubicek, and Schwarzer <a href="#ref-degeMusicLessonsIntelligence2011">2011</a>; Schellenberg <a href="#ref-schellenbergLongtermPositiveAssociations2006">2006</a>)</span>.
Though many of these studies are correlational, they also have made attempts to control for confounding variables like socio-economic status and parental involvement in out of school activities <span class="citation">(Corrigall and Trainor <a href="#ref-corrigallAssociationsLengthMusic2011">2011</a>; Degé, Kubicek, and Schwarzer <a href="#ref-degeMusicLessonsIntelligence2011">2011</a>; Schellenberg <a href="#ref-schellenbergExaminingAssociationMusic2011">2011</a>; Schellenberg and Mankarious <a href="#ref-schellenbergMusicTrainingEmotion2012">2012</a>)</span>
Schellenberg notes the problem of smaller sample sizes in his review <span class="citation">(Corrigall and Trainor <a href="#ref-corrigallAssociationsLengthMusic2011">2011</a>; Parbery-Clark et al. <a href="#ref-parbery-clarkMusicalExperienceAging2011">2011</a>; Strait et al. <a href="#ref-straitMusicalTrainingEarly2012">2012</a>)</span> in that studies that are typically smaller do not reach statistical significance.
Schellenberg also references evidence that when professional musicians are matched with non-musicians from the general population there do not seem to be these associations <span class="citation">(Schellenberg <a href="#ref-schellenbergLongtermPositiveAssociations2006">2006</a>)</span>.
His review suggests the current state of the literature points to support the hypothesis that higher functioning kids that take music lessons and they tend to stay in lessons longer.
Additionally, Schellenberg remains skeptical of any sorts of causal factors regarding increases in IQ <span class="citation">(Francois et al. <a href="#ref-francoisMusicTrainingDevelopment2013">2013</a>; Moreno et al. <a href="#ref-morenoMusicalTrainingInfluences2009">2009</a>)</span> noting methodological problems like how short exposure times were in studies claiming increases in effects or researchers who not holding pre-existing cognitive abilities constant <span class="citation">(Mehr et al. <a href="#ref-mehrTwoRandomizedTrials2013">2013</a>)</span>.
Continued work by Swaminithan, Schellenberg, and Khalil continue to support evidence for this selection bias in training resulting in higher cognitive abilities among musicians <span class="citation">(Swaminathan, Schellenberg, and Khalil <a href="#ref-swaminathanRevisitingAssociationMusic2017">2017</a>)</span>.</p>
</div>
</div>
<div id="environmental" class="section level3">
<h3><span class="header-section-number">2.2.2</span> Environmental</h3>
<p>Standing in contrast to factors that individuals do not have a much control over such as the size of their working memory capacity or factors related to their general fluid intelligence, most of the factors music pedagogues believe contribute to someone’s ability to take melodic dictation are related to what I have put forward as environmental factors.
In fact, one of the tacit assumptions of any formal education revolves around the belief that with deliberate and attentive practice, that an individual is able to move from novice status to some level of expertise in their chosen domain.
The idea that time invested results in beneficial returns is probably best formalized by work produced by <span class="citation">Ericsson, Krampe, and Tesch-Romer (<a href="#ref-ericssonRoleDeliberatePractice1993">1993</a>)</span> that suggests that performance at more elite levels results from deliberate practice.</p>
<p>As noted in studies above such as <span class="citation">Meinz and Hambrick (<a href="#ref-meinzDeliberatePracticeNecessary2010">2010</a>)</span>, deliberate practice is able to explain variance in task performance, but again other research suggests more variables are at play.
<span class="citation">Detterman and Ruthsatz (<a href="#ref-dettermanMoreComprehensiveTheory1999">1999</a>)</span> propose that three factors, general intelligence, domain specific ability, and practice are the cornerstones of developing expertise in music.
The first of their three is not normally believed to be malleable, while the former two are presumed to be plastic.
This reasoning has been explored by researchers such as <span class="citation">Ruthsatz et al. (<a href="#ref-ruthsatzBecomingExpertMusical2008">2008</a>)</span>, who investigated these assertions and provided empirical evidence to support this notion using a hierarchical multiple regression modelling and concluded that each of these variables does in fact contribute significantly to the target variable of musical performance.
Other researchers have since commented on these expertise models like <span class="citation">Mosing et al. (<a href="#ref-mosingPracticeDoesNot2014">2014</a>)</span> who have asserted that a genetic component, rather than those listed above best explain variance on musical ability.</p>
<p>One major problem with interpreting literature like the studies mentioned above is the general lack of agreement on what constitutes musical behaviors.
At a very high level, many of the aforementioned studies take a parochial view of what it means to engage in musical activity, a problem which is only exacerbated by not having uniform psychometric measurements <span class="citation">(D. Baker et al. <a href="#ref-bakerExaminingMusicalSophistication2018a">2018</a>; Talamini et al. <a href="#ref-talaminiMusiciansHaveBetter2017">2017</a>)</span>.
Interpreting this data then becomes difficult as what it means to be proficient at a musical task is culturally dependent.
Investigating musical talent as if were a universal is a problem well documented in both the ethnomusicological and music education literature <span class="citation">(Blacking <a href="#ref-blackingHowMusicalMan2000">2000</a>; Murphy <a href="#ref-murphyHowFarTests1999">1999</a>)</span>.</p>
<div id="aural-training" class="section level4">
<h4><span class="header-section-number">2.2.2.1</span> Aural Training</h4>
<p>In addition to individuals differing in their general musical abilities– however they are defined– individuals also differ in their abilities at the level of their aural skills.
The same problems that arise in operationalizing musicianship are apparent in defining aural skills.
Reviewing the literature, I take aural skills to encompass the many skills often taught in music school, not restricting those skills to any particular sets of exercises.
Some researchers like Chenttee <span class="citation">(Chenette <a href="#ref-chenetteReframingAuralSkills2019">2019</a>)</span> have taken stricter definitions attempting to state only skills that engage working memory capacity are those that are truly aural, but this operationalization would limit this review’s scope.</p>
<p>Though not as heavily researched in the past few decades <span class="citation">(Furby <a href="#ref-furbyEffectsPeerTutoring2016">2016</a>)</span>, there has been specific research looking at modeling how individuals perform in aural skills examinations.
<span class="citation">Harrison, Asmus, and Serpe (<a href="#ref-harrisonEffectsMusicalAptitude1994">1994</a>)</span> examined the effect of aural skills training on undergraduate students by creating a latent variable model investigating musical aptitude, academic ability, musical expertise, and motivation to study music in a sample of 142 undergraduate students and claimed to be able to explain 73% of the variance in aural skills abilities using the variables measured.
Work from Colin Wright’s dissertation took a mixed methods approach investigating correlations between aural ability and their degree success as well as interviewing university students regarding their the importance of aural skills education.
In his work he found a general positive correlation between aural ability and measures of degree success.</p>
<p>While results are still mixed regarding how best to measure and assess this ability, the near ubiquity of aural skills education has resulted in many investigations of how people might improve their ability.
As noted in <span class="citation">Furby (<a href="#ref-furbyEffectsPeerTutoring2016">2016</a>)</span>, researchers in the past have suggested a variety of techniques for improving their abilities in melodic dictation by isolating rhythm and melody <span class="citation">(Banton <a href="#ref-bantonRoleVisualAuditory1995">1995</a>; Bland <a href="#ref-blandSightSingingMelodic1984">1984</a>; Root <a href="#ref-rootMethodicalSightSingingLessons1931">1931</a>)</span>, listening attentively to the melody before writing <span class="citation">(Banton <a href="#ref-bantonRoleVisualAuditory1995">1995</a>)</span>, recognizing patterns <span class="citation">(Banton <a href="#ref-bantonRoleVisualAuditory1995">1995</a>; Bland <a href="#ref-blandSightSingingMelodic1984">1984</a>; Root <a href="#ref-rootMethodicalSightSingingLessons1931">1931</a>)</span> and silently vocalizing while dictating <span class="citation">(Klonoski <a href="#ref-klonoskiImprovingDictationAuralSkills2006">2006</a>)</span>.
Interpreting a clear best path forward from these studies again remains difficult due to the sheer amount of variables at play.</p>
<p>Often described as the other side of the same coin of melodic dictation, sight singing is an area of music pedagogy research that has received some attention, yet probably not the extent is deserved given its prevalence in school of music curricula.
Recently <span class="citation">Fournier et al. (<a href="#ref-fournierCognitiveStrategiesSightsinging2017a">2017</a>)</span> cataloged and categorized 14 different strategies that students used when learning to sight read.
The authors organized their 14 categories into four larger main categories and suggested that aural skills pedagogues should employ their framework in their aural skills pedagogy in order to better communicate effective sight singing strategies.</p>
<p>Similar to commentaries in literature on melodic dictation, <span class="citation">Fournier et al. (<a href="#ref-fournierCognitiveStrategiesSightsinging2017a">2017</a>)</span> also note a line of research that has documented that university students are often unprepared to sight-read single lines of music <span class="citation">(Asmus <a href="#ref-asmusMusicTeachingMusic2004">2004</a>; Thompson <a href="#ref-thompsonPitchInternalizationStrategies2003">2003</a>)</span> even though it is, like dictation, thought of as a means for deeper musical understanding <span class="citation">(Karpinski <a href="#ref-karpinskiAuralSkillsAcquisition2000">2000</a>; Rogers <a href="#ref-rogersTeachingApproachesMusic2004">2004</a>)</span>.
<span class="citation">Fournier et al. (<a href="#ref-fournierCognitiveStrategiesSightsinging2017a">2017</a>)</span> also note that sight-reading has been an active area of research due to the often reported relationship that performance on sight reading often predicts several studies have shown links between academic success in sight-singing and predictors such as entrance tests <span class="citation">(Harrison <a href="#ref-harrisonValidityMusicalAptitude1987">1987</a>)</span>, academic ability, and musical experience <span class="citation">(Harrison, Asmus, and Serpe <a href="#ref-harrisonEffectsMusicalAptitude1994">1994</a>)</span>.</p>
<p>Taken as a whole, the research tends to suggesting that learning to be a fluid and competent sight reader helps musicians hone their skills by bootstrapping other musical skills since the skills needed for sight-reading touch on many of the skills used in musical performance like pattern matching and listening for small changes in intonation.
While the above literature suggests there are empirical grounds to consider these individual factors in predicting how well an individual will do in melodic dictation, these factors will invariably interact with the other half of the taxonomy: the musical parameters.</p>
</div>
</div>
</div>
<div id="musical-factors" class="section level2">
<h2><span class="header-section-number">2.3</span> Musical Factors</h2>
<p>Transitioning to the other half of the taxonomy on Figure <a href="intro.html#fig:taxonomy">2.4</a>, the other main source of variance in any study investigating melodic dictation is the effect of the melody itself.
I find it safe to assume that not all melodies are equally difficult to dictate and assert that variance in the difficulty the melody can partitioned between both structural and experimental aspects of a melody.
As noted above, there is not a strict delineation between these two categories since once could imagine manipulations in experimental parameters in order to result in a phenomenologically different experience of melody.
Questions of transformations of melodies and musical similarity fall have been addressed in other research <span class="citation">(Cambouropoulos <a href="#ref-cambouropoulosHowSimilarSimilar2009">2009</a>; Wiggins <a href="#ref-wigginsModelsMusicalSimilarity2007">2007</a>)</span> and are beyond the scope of this study.</p>
<div id="discussing-melodic-structure" class="section level3">
<h3><span class="header-section-number">2.3.1</span> Discussing Melodic Structure</h3>
<p>The assumption that a musical score is able to provide insights towards meaningful understanding is a core tenant of music theory and analysis.
Throughout the 20th Century, music theorists have almost exclusively relied on musical scores as their central point of reference in their work.
According to Clarke <span class="citation">(Clarke <a href="#ref-clarkeWaysListeningEcological2005">2005</a>)</span>– though also discussed by others– this structuralist approach to music lays at the foundation of many academic discussions, possibly stemming from latent assumptions regarding absolutism in music.
General interest in structure has been a dominant part of the discourse as evidenced from the extensive lines of thought emanating from Heinrich Schenker <span class="citation">(Schenker <a href="#ref-schenkerFreieSatz1935">1935</a>; Salzer and Mannes <a href="#ref-salzerStructuralHearingTonal1982">1982</a>; Schachter <a href="#ref-schachterSchenkerStudies2006">2006</a>; Schenker, Siegel, and Schachter <a href="#ref-schenkerSchenkerStudies1990">1990</a>)</span> and variations on linking what one might colloquially refer to as “the notes” to some sort of musical meaning is the lifeblood of music theory.
While issues surrounding “the notes” as they pertain to discourse have been central to large debates within the musicological community <span class="citation">(Agawu <a href="#ref-agawuHowWeGot2004">2004</a>; Kerman <a href="#ref-kermanContemplatingMusicChallenges1986">1986</a>)</span>, tethering “the notes” to being able help to directly explain certain phenomenological listening experiences in music received much of its theoretical framework from the work of Leonard Meyer and assertions he put forward in <em>Emotion and Meaning in Music</em> <span class="citation">(Meyer <a href="#ref-meyerEmotionMeaningMusic1956">1956</a>)</span>.
In his text, Meyer posits that much of a listener’s experience in music can be understood by considering a listener’s expectations.</p>
<p>Research in Meyer’s tradition inspired work investigating the perception of melodic structures via the work of Eugene Narmour <span class="citation">(Narmour <a href="#ref-narmourAnalysisCognitionBasic1990">1990</a>, <a href="#ref-narmourAnalysisCognitionMelodic1992">1992</a>)</span>, Glenn Schellenberg <span class="citation">(Schellenberg <a href="#ref-schellenbergSimplifyingImplicationRealizationModel1997">1997</a>)</span>, Elizabeth Hellmuth Margulis <span class="citation">(Margulis <a href="#ref-margulisModelMelodicExpectation2005">2005</a>)</span>, and David Huron <span class="citation">(Huron <a href="#ref-huronSweetAnticipation2006">2006</a>)</span>.
Meyer has also been the cited source of inspiration for recent, successful implementations of models of human auditory cognition like that of Marcus Pearce’s Information Dynamics of Music <span class="citation">(Pearce <a href="#ref-pearceConstructionEvaluationStatistical2005">2005</a>, <a href="#ref-pearceStatisticalLearningProbabilistic2018a">2018</a>)</span> which derives from information theoretic models of musical perception put forward by Ian Witten and Darrel Conklin <span class="citation">(Conklin and Witten <a href="#ref-conklinMultipleViewpointSystems1995">1995</a>)</span>.</p>
<p>Though even prior to Meyer and Schenker, one of the earliest researchers that sought to make an explicit link between “the notes” and perception comes from outside the dominant academic discourse on music.
The first study to examine link between what might be understood as “the notes” and an aspect of perception was was Otto Ortmann in 1933 <span class="citation">(Ortmann <a href="#ref-ortmannTonalDeterminantsMelodic1933">1933</a>)</span>.
Ortmann used a series of twenty five-note melodies in order to examine the effects of repetition, pitch direction, conjunct-disjunct motion (contour), interval size, order, and chord structure, all of which he deems to be the determinants of an individual’s ability to recall melodic material.
Though Ortmann did not use any statistical methods to model his data, he did assert that each of his determinants contributed to an individual’s ability to dictate musical material.
This work was extended by <span class="citation">Taylor and Pembrook (<a href="#ref-taylorStrategiesMemoryShort1983">1983</a>)</span> which additionally incorporated using musical skill as a predictor and subsequently found evidence that these factors contributed to individual dictation abilities in a sample of 122 undergraduate students.</p>
<p>What Ortmann referred to as determinants are structural aspects of the melody that can then be mapped to some aspect of perception.
While Ortmann uses the term determinants, for the rest of this study I instead adopt the term feature which better reflects current terminology used to talk about these aspects of a melody.
Given Ortmann’s design of using isorhythmic five tone sequences, his detriments– or features– under my taxonomy from Figure <a href="intro.html#fig:taxonomy">2.4</a> would generally include only structural aspects.
Were Ortmann to have increased the tempo of the tones he presented, change the timbre of their instrumentation, or maybe give participants more attempts to give their responses, he would have then been adjusting what I am referring to as the experimental parameters.
In the section below, I first explore literature that set out to understand certain structural aspects of the musical side of my taxonomy, then begin to introduce studies that incorporate more parameters.</p>
<p>As with the above problems listed in attempting to measure latent psychological constructs, similar problems also arise in operationalizing many of the musical constructs in the experiments from above.
Unlike individual features, since musical scores can be digitized, attempting create more objective measurements for musical features is more straightforward than that of measuring latent psychological variables.
One way to accomplish this is to use symbolic features of the melodies themselves as a variable to be measured.
Unfortunately, much of the work from computational musicology such as David Huron’s Humdrum toolbox <span class="citation">(Huron <a href="#ref-huronHumdrumToolkitReference1994">1994</a>)</span> or Michael Cutberth’s Music21 <span class="citation">(Cuthbert and Ariza <a href="#ref-cuthbertMusic21ToolkitComputerAided2010">2010</a>)</span> pre-dates some of the earlier experimental work I will discuss below, but as these computations are more straightforward than considering larger experimental designs, I begin with them here.
While I reserve a longer discussion on the histories of computational musicology for Chapter Four<a href="chapterfour.html#chapterfour">5</a>, relevant to this review are the additional ways it is now possible to abstract features from symbolic melodies beyond what was capable in studies such as <span class="citation">Ortmann (<a href="#ref-ortmannTonalDeterminantsMelodic1933">1933</a>)</span> and <span class="citation">Taylor and Pembrook (<a href="#ref-taylorStrategiesMemoryShort1983">1983</a>)</span>.</p>
</div>
<div id="abstracted-features" class="section level3">
<h3><span class="header-section-number">2.3.2</span> Abstracted Features</h3>
<p>An abstracted symbolic features of a melody are emergent properties of the melody that results from performing a calculation on the melody when digitized into discrete, computer readable tokens.
Abstracted symbolic features of melodies can largely be conceptualized as being static or dynamic.
Static features of melodies are obtained by summarizing some aspect of the melody as if it were to be experience in suspended animation.
For example, a static feature of a melody might the melody’s range as calculated by the number of half steps from the lowest to the highest note or the number of notes in a melody.
Using static features helps quantify something that might be intuitive about a melody or piece of encoded music.
For example, David Huron’s contour class used in a study investigating melodic arches <span class="citation">(Huron <a href="#ref-huronMelodicArchWestern1996">1996</a>)</span> using the Essen Folksong Collection <span class="citation">(Schaffrath <a href="#ref-schaffrathEssenFolkSong1995">1995</a>)</span> can only be understood as a feature of the melody itself once the melody has been sounded and is recalled would be a static feature of a melody.
Other examples include a melody’s global note density, normalized pairwise variability index <span class="citation">(Grabe <a href="#ref-grabeDurationalVariabilitySpeech2002">2002</a>)</span>, and a melody’s tonalness as calculated by one of the various key profile algorithms <span class="citation">(Krumhansl <a href="#ref-krumhanslCognitiveFoundationsMusical2001">2001</a>)</span>.
These measures are useful when describing melodies and are predictive of various behavioral phenomena as detailed below, but at this point it has not been well established to what degree these summary features can be directly and reliably mapped to aspects of human behavior.</p>
<p>The quintessential and most comprehensive toolbox example of this is Daniel Müllensiefen’s Feature ANalysis Technology Accessing STatistics (In a Corpus) or FANTASTIC toolbox <span class="citation">(Mullensiefen <a href="#ref-mullensiefenFantasticFeatureANalysis2009">2009</a>)</span>.
FANTASTIC is software that is capable summarizing musical material at for monophonic melodies.
In addition to computing 38 features such as contour variation, tonalnesss, note density, note length, and measures inspired by computational linguistics <span class="citation">(Manning and Schütze <a href="#ref-manningFoundationsStatisticalNatural1999">1999</a>)</span> FANTASTIC is also able to calculate m-types (melodic-rhythmic motives) that are based on the frequency distributions of melodic segmentsin musical corpora.</p>
<p>Work using the FANTASTIC toolbox has been successful in predicting court case decisions <span class="citation">(Müllensiefen and Pendzich <a href="#ref-mullensiefenCourtDecisionsMusic2009">2009</a>)</span>, predicting chart successes of songs on the Beatles’ <em>Revolver</em> <span class="citation">(Kopiez and Mullensiefen <a href="#ref-kopiezAufSucheNach2011">2011</a>)</span>, memory for old and new melodies in signal detection experiments <span class="citation">(Müllensiefen and Halpern <a href="#ref-mullensiefenRoleFeaturesContext2014">2014</a>)</span>, memory for ear worms <span class="citation">(Jakubowski et al. <a href="#ref-jakubowskiDissectingEarwormMelodic2017">2017</a>; Williamson and Müllensiefen <a href="#ref-williamsonEarwormsThreeAngles2012">2012</a>)</span>, memorability of pop music hook <span class="citation">(Balen, Burgoyne, and Bountouridis <a href="#ref-balenCorpusAnalyisTools2015">2015</a>)</span>.
In experimental studies, FANTASTIC has also been used to determine item difficulty <span class="citation">(Baker and Müllensiefen <a href="#ref-bakerPerceptionLeitmotivesRichard2017">2017</a>; Harrison, Musil, and Müllensiefen <a href="#ref-harrisonModellingMelodicDiscrimination2016">2016</a>)</span> and has even been the basis of the development of a computer assisted platform for studying memory for melodies <span class="citation">(Rainsford, Palmer, and Paine <a href="#ref-rainsfordMUSOSMUsicSOftware2018">2018</a>)</span>.</p>
<p>In addition to using summary based features on melodies, it is also possible to model the perception of musical materials by using a dynamic approach that is dependent on the unfolding of musical material.
First explored in by Witten and Conklin <span class="citation">(Conklin and Witten <a href="#ref-conklinMultipleViewpointSystems1995">1995</a>)</span>, and then implemented as a dynamic model of human auditory cognition in his doctoral dissertation, Marcus Pearce’s Information Dynamics Of Melody (IDyOM) models musical expectancy using information theoretic concepts <span class="citation">(Shannon <a href="#ref-shannonMathematicalTheoryCommunication1948">1948</a>)</span>.
The model takes an unsupervised machine learning approach and calculates the information content based on multiple pre-specified viewpoints <span class="citation">(Pearce <a href="#ref-pearceConstructionEvaluationStatistical2005">2005</a>)</span>.
As a model, IDyOM has has been successful in modeling human responses to expectation, melodic boundary formation, and even measurements of cultural proximity <span class="citation">(Pearce and Wiggins <a href="#ref-pearceAuditoryExpectationInformation2012">2012</a>; Pearce <a href="#ref-pearceStatisticalLearningProbabilistic2018a">2018</a>)</span>.
The domain general application of IDyOM has given credence to Meyer’s assertion that the enculturation of musical styles stems from statistical exposure to musical genres and be somewhat reflective of the cognitive processes used in musical perception.
IDyOM has also been recently extended to look at expectation in polyphonic work <span class="citation">(Sauve <a href="#ref-sauvePredictionPolyphonyModelling2017">2017</a>)</span> and expectations of harmony <span class="citation">(Harrison and Pearce <a href="#ref-harrisonDissociatingSensoryCognitive2018">2018</a>)</span>.
The advantage of using a dynamic approach, as opposed to a static one, is that a dynamic approach theoretically reflects real-time perception of music with the structural characteristics of the music mapping on to real human behavior since expectancy values are calculated for every musical event.
While employing this type of model does allow for calculations to be made for every musical event in question, the assumption also brings into question that a computer model is able to calculate each musical event and be reflective of human cognition, does that mean that the human perceptual system also is making on-the-fly probability calculations during perception?
This problem is worthy of mention as it currently exists in literature on implicit statisical learning <span class="citation">(Perruchet and Pacton <a href="#ref-perruchetImplicitLearningStatistical2006">2006</a>)</span> and some researchers have put forward similarity based models that have been claimed to explain processes attributed to statistical learning, but do not depend on that meachanism <span class="citation">(Jamieson and Mewhort <a href="#ref-jamiesonApplyingExemplarModel2009">2009</a>)</span>.</p>
<div id="ecological-experiments" class="section level4">
<h4><span class="header-section-number">2.3.2.1</span> Ecological Experiments</h4>
<p>While the field of computational musicology has built models for quantifying these perceptual aspects of melody, work that is generally more aligned with research in music education takes a more ecological approach to inspecting how musical features affect perception.
For example, Long found that length, tonal structure, contour, and individual traits all contribute to performance on melodic dictation examinations and found that structure and tonalness to have significant, albeit small predictive powers in modeling <span class="citation">(Long <a href="#ref-longRelationshipsPitchMemory1977">1977</a>)</span>.
One problem with studies such as <span class="citation">Long (<a href="#ref-longRelationshipsPitchMemory1977">1977</a>)</span> is that these studies like Long’s make conspicuous methodological decisions such as eliminating individuals from their sample who met their criteria for bad singers.
Not only does this reduce the spectrum of ability levels (assuming that singing ability correlates with dictation ability, a finding since which has been established <span class="citation">(Norris <a href="#ref-norrisRelationshipSightSinging2003">2003</a>)</span>), but is additionally flawed in that it is at odds both with the intuition that an individual’s singing ability cannot be taken as a direct representation of their mental image of the melody.
In fact, more recent research might suggest that singing ability might instead relate to motor control ability over the vocal tract rather than pitch imagery abilities <span class="citation">(Pfordresher and Brown <a href="#ref-pfordresherPoorPitchSingingAbsence2007">2007</a>)</span>.</p>
<p>Other researchers have also put forward other parameters thought to contribute like tempo <span class="citation">(Hofstetter <a href="#ref-hofstetterComputerBaesedRecognitionPerceptual1981">1981</a>)</span>, tonality <span class="citation">(Dowling <a href="#ref-dowlingScaleContourTwo1978">1978</a>; Long <a href="#ref-longRelationshipsPitchMemory1977">1977</a>; Pembrook <a href="#ref-pembrookInterferenceTranscriptionProcess1986">1986</a>; Oura and Hatano <a href="#ref-ouraMemoryMelodiesSubjects1988">1988</a>)</span>, interval motion <span class="citation">(Ortmann <a href="#ref-ortmannTonalDeterminantsMelodic1933">1933</a>; Pembrook <a href="#ref-pembrookInterferenceTranscriptionProcess1986">1986</a>)</span>, length of melody <span class="citation">(Long <a href="#ref-longRelationshipsPitchMemory1977">1977</a>; Pembrook <a href="#ref-pembrookInterferenceTranscriptionProcess1986">1986</a>)</span>, number of presentations <span class="citation">(Hofstetter <a href="#ref-hofstetterComputerBaesedRecognitionPerceptual1981">1981</a>; Pembrook <a href="#ref-pembrookInterferenceTranscriptionProcess1986">1986</a>)</span>,
context of presentation <span class="citation">(Schellenberg and Moore <a href="#ref-schellenbergEffectTonalRhythmicContext1985">1985</a>)</span>,
the background of the listener <span class="citation">(Long <a href="#ref-longRelationshipsPitchMemory1977">1977</a>; Oura and Hatano <a href="#ref-ouraMemoryMelodiesSubjects1988">1988</a>; Schellenberg and Moore <a href="#ref-schellenbergEffectTonalRhythmicContext1985">1985</a>; Taylor and Pembrook <a href="#ref-taylorStrategiesMemoryShort1983">1983</a>)</span> as well as familiarity with a musical style <span class="citation">(Schellenberg and Moore <a href="#ref-schellenbergEffectTonalRhythmicContext1985">1985</a>)</span>.
Again we have a listing of studies that consider both structural and experimental aspects of the taxonomy.</p>
<p><span class="citation">Pembrook (<a href="#ref-pembrookInterferenceTranscriptionProcess1986">1986</a>)</span> provides an extensive detailing of a systematic study to melodic dictation where they used tonality, melody length, and type of motion as variables in their experiment.
They additionally also restricted their experimental melodies to those that were singable.
The authors found all three variables to be significant predictors with tonality explaining 13% of the variance, length explaining 3% of the variance and type of motion explaining 1% of the variance.
The paper also claims that people on average can hear and remember 10-16 notes with the quarter note set to 90 beats per minute.</p>
<p>Given the lack of consistent methodologies in administration and scoring of these experiments it becomes difficult to find ways to generalize basic findings like expected effect sizes– especially when the original materials and data have not been recorded– but there is often interesting theoretical insights to be gleaned.
For example <span class="citation">Oura (<a href="#ref-ouraConstructingRepresentationMelody1991a">1991</a>)</span> used a sample of eight people to suggest that when taking melodic dictation, individuals use a system of pattern matching that interfaces with their long term memory in order to complete dictation tasks.
While this paper does not bring with it exhaustive evidence supporting this claim, the idea is explored in detail in Chapter 6 when the idea of pattern matching is used in conjunction with Cowan’s Embedded Process model of working memory.</p>
<p>More recently the music education community has also began to do research around melodic dictation using both qualitative and quantitative methodologies.
<span class="citation">Paney and Buonviri (<a href="#ref-paneyTeachingMelodicDictation2014">2014</a>)</span> interviewed high school teachers on methods they used to teach melodic dictation and among more general findings on teaching methods, reported a general awareness and concern among pedagoges regarding the “psychological barriers inherent in learning aural skills”, as well as a general positive disposition to the use of standardized tests used in melodic dictation.
<span class="citation">Gillespie (<a href="#ref-gillespieMelodicDictationScoring2001">2001</a>)</span> surveyed over 40 individual aural skill instructors and reported large discrepencies in how aural skills pedagogues graded and gave feedback on student’s melodic dictations.
Other work by <span class="citation">Pembrook and Riggins (<a href="#ref-pembrookSendHelpAural1990">1990</a>)</span> surveyed various methodologies used by instructors in aural skills settings and reported inconsistencies in grading practice.
Some of these studies consider aural skills as a totality like <span class="citation">Norris (<a href="#ref-norrisRelationshipSightSinging2003">2003</a>)</span> who provided quantitative evidence to suggest most aural skills pedagogue’s intuition that there is some sort of relationship between melodic dictation and sight singing.
Looking at the notorious subset of students with absolute pitch (AP), <span class="citation">Dooley and Deutsch (<a href="#ref-dooleyAbsolutePitchCorrelates2010">2010</a>)</span> provided evidence demonstrated that students with AP tend to outperform their non-AP colleagues in tests of dictation.</p>
<p>Continuing exploring the pedagogical literature, Nathan Buonviri and colleagues have also made melodic dictation a central focus of recent work.
Using qualitative methods, <span class="citation">N. Buonviri (<a href="#ref-buonviriEffectsMusicNotation2015">2015</a>)</span> interviewed six sophomore music majors in order to find successful strategies that students engaged with when completing melodic dictations and found evidence to suggest that sucessful students engage in highly concentrated mental choreography when completing melodic dictations.<br />
<span class="citation">Paney (<a href="#ref-paneyEffectDirectingAttention2016">2016</a>)</span> reported beneficial effects to direct student’s attention and guide them through melodic dictation exercises suggesting that some sort of mental organization of the dictation process is helpful.
<span class="citation">Nathan O Buonviri and Paney (<a href="#ref-buonviriMelodicDictationInstruction2015">2015</a>)</span> found that having students sing a preparatory singing pattern after hearing the target melody, essentially a distraction task, hindered performance on melodic dictation.
<span class="citation">N. Buonviri (<a href="#ref-buonviriEffectsMusicNotation2015">2015</a>)</span> found no effects of test presentation format (visual versus aural-visual) using a melodic memory paradigm and more work by <span class="citation">Buonviri (<a href="#ref-buonviriEffectsTwoListening2017">2017</a>)</span> reported no significant advantage to listening strategies while partaking in a melodic dictation test.</p>
<p>Not specific to computational musicology or that of the music education literature, other research from music perception has also claimed other experimental features might play a role in dictation.
For example, a series of papers by Michael W. Weiss has found a general timbral advantage of voice in memory recall tasks <span class="citation">(Weiss et al. <a href="#ref-weissRapidCommunicationPianists2015">2015</a>; Weiss and Peretz <a href="#ref-weissAbilityProcessMusical2019">2019</a>)</span>, even finding the effect in amusics.
Vocal timbral perception presumably would then have an effect in the recall of music in dictation settings, but evidence supporting other surface features in memory processes is lacking <span class="citation">(Schellenberg, Stalinski, and Marks <a href="#ref-schellenbergMemorySurfaceFeatures2014">2014</a>)</span>.</p>
<p>As documented in this review of the literature on issues that contribute to an individual’s ability to take melodic dictation, the problem is complex.
Not only are there difficulties in finding adequate measures of latent psychological constructs assumed to exist and contribute like working memory capacity and musical training, but additionally the amount of musical variables at play that inevitable interact with one another is overwhelming.</p>
<p>Given all the variables that are at play, what then is the best way forward in understanding the proccesses underlying melodic dictation?
In my opinion, the path forward to understanding relies on adopting a polymorphic view of musical abilities for future modeling.</p>
</div>
</div>
</div>
<div id="polymorphism-of-ability" class="section level2">
<h2><span class="header-section-number">2.4</span> Polymorphism of Ability</h2>
<p>Given the current state of cognitive psychology and psychometrics, as well as advances in computational musicology, the possibilities for now operationalizing and then modeling aspects of melodic dictation are as advanced as they ever have been.
The research community can now operationalize every factor that is thought to contribute to this process and have literature to support the recording of almost any variable.
This includes concepts of musicianship, to features of a melody, and even unitless measures associated with an individual’s working memory capacity.</p>
<p>While this is certainly possible to do, continuing in this manner of picking variables deemed relevant from such an expansive catalogue of parameters will only obfuscate further research.
A clearer path forward is needed that reduces the signal to noise ratio in this resarch.
After reviewing this literature, below I list my reccomendations in answering this problem.</p>
<p>One of the most important changes to future studies on melodic dictation needs avoid the use of latent variables as predictors in statistical models.
While abstract concepts like intelligence and musical training are helpful concepts for explaining the variance in responses in aural skills settings, using such abstracted variables obfuscates causal mechanisms underlying this process.</p>
<p>The most illustrative example of this comes from the above study by <span class="citation">Harrison, Asmus, and Serpe (<a href="#ref-harrisonEffectsMusicalAptitude1994">1994</a>)</span> who created a latent variable model of aural skills that was able to predict 74% of the variance in aural skills performance.
This latent trait that the authors created is helpful in explaining the patterns of covariance in data, but this would be to reify a statistical abstraction and assume a stance of ontological realsim as noted before <span class="citation">(Borsboom, Mellenbergh, and van Heerden <a href="#ref-borsboomTheoreticalStatusLatent2003">2003</a>)</span>.
The idea of statistical reification has been critiqued outside of music <span class="citation">(Gould <a href="#ref-gouldMismeasureMan1996">1996</a>; Kovacs and Conway <a href="#ref-kovacsProcessOverlapTheory2016">2016</a>)</span> and additionally has served as the basis for an argument that work from my doctoral work has putforward <span class="citation">(D. Baker et al. <a href="#ref-bakerExaminingMusicalSophistication2018a">2018</a>)</span>.</p>
<p>The same arguments put forward in this literature also are relevant in reserach in aural skills.
In order to have a complete, causal model of the processes underlying melodic dictation, it is important to understand melodic dictation as a set of musical abilities that are related to other musical abilities, though may not be unified as a monolithic whole form which indiviuals draw from in order execute musical tasks.
This idea is not new even in music psychology, as the past two decades have seen calls for a more polymorphic definition of musical ability <span class="citation">(Levitin <a href="#ref-levitinWhatDoesIt2012">2012</a>; Peretz and Coltheart <a href="#ref-peretzModularityMusicProcessing2003">2003</a>)</span> whose modeling will require more concrete ways of defining underlying processes rather than correlating variable together that are helpful at prediction without explaining the process.
Using a polymorphic view of musical abilities coupled with a theoretical framework like Karpinski’s will then allow for a clearer understanding of the many variables at play during this process.</p>
</div>
<div id="conclusions" class="section level2">
<h2><span class="header-section-number">2.5</span> Conclusions</h2>
<p>In this chapter I first described what melodic dictation is using Karpinski’s model of melodic dictation.
Using his didactic model as a point of departure, I suggested what this model does not consider and then put forward a taxonomy of features meant to encompass what his model lacks.
I suggested there are both individual as well as musical features that need to be understood in order to have a comprehensive understanding of melodic dictation.
Of the two sets of features, individual features can be either cognitive or environmental and musical features can be either structural or experimental.
This taxonomy does not consist of exclusive categories and certainly permits interactions between any and all of the levels.
Using this taxonomy as a guide, I then survey relevant literature in order to discuss how a research might effectively effectively quantify each parameter of relevance.
Finally, I asserted that in order to provide a more cohesive research program going forward, research on melodic dictation should adopt a polymorphic view of musicianship in line with calls in the past to move away from high level modeling and focus as much as possible on the processes deemed relevant in the process.
The rest of this dissertation will synthesize these areas and put forth novel research contributing to the modeling and subsequent understanding of melodic dictation.</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-karpinskiModelMusicPerception1990">
<p>Karpinski, Gary. 1990. “A Model for Music Perception and Its Implications in Melodic Dictation.” <em>Journal of Music Theory Pedagogy</em> 4 (1): 191–229.</p>
</div>
<div id="ref-karpinskiAuralSkillsAcquisition2000">
<p>Karpinski, Gary Steven. 2000. <em>Aural Skills Acquisition: The Development of Listening, Reading, and Performing Skills in College-Level Musicians</em>. Oxford University Press.</p>
</div>
<div id="ref-klonoskiImprovingDictationAuralSkills2006">
<p>Klonoski, Edward. 2006. “Improving Dictation as an Aural-Skills Instructional Tool,” 6.</p>
</div>
<div id="ref-potterIdentifyingSucessfulDictation1990">
<p>Potter, Gary. 1990. “Identifying Sucessful Dictation Strategies.” <em>Journal of Music Theory Pedagogy</em> 4 (1): 63–72.</p>
</div>
<div id="ref-paneyEffectDirectingAttention2016">
<p>Paney, Andrew S. 2016. “The Effect of Directing Attention on Melodic Dictation Testing.” <em>Psychology of Music</em> 44 (1): 15–24. <a href="https://doi.org/10.1177/0305735614547409" class="uri">https://doi.org/10.1177/0305735614547409</a>.</p>
</div>
<div id="ref-davidbutlerWhyGulfMusic1997a">
<p>Butler, David. 1997. “Why the Gulf Between Music Perception Research and Aural Training?” <em>Bulletin of the Council for Research in Music Education</em>, no. 132.</p>
</div>
<div id="ref-klonoskiPerceptualLearningHierarchy2000">
<p>Klonoski, Edward. 2000. “A Perceptual Learning Hierarchy: An Imperative for Aural Skills Pedagogy.” <em>College Music Symposium</em> 4: 168–69.</p>
</div>
<div id="ref-wrightInvestigatingAuralCase2016">
<p>Wright, Colin Richard. 2016. “Investigating Aural: A Case Study of Its Relationship to Degree Success and Its Understanding by University Music Students.” University of Hull.</p>
</div>
<div id="ref-baddeleyEpisodicBufferNew2000">
<p>Baddeley, Alan. 2000. “The Episodic Buffer: A New Component of Working Memory?” <em>Trends in Cognitive Sciences</em> 4 (11): 417–23. <a href="https://doi.org/10.1016/S1364-6613(00)01538-2" class="uri">https://doi.org/10.1016/S1364-6613(00)01538-2</a>.</p>
</div>
<div id="ref-dowlingScaleContourTwo1978">
<p>Dowling, W. Jay. 1978. “Scale and Contour: Two Components of a Theory of Memory for Melodies.” <em>Psychological Review</em> 84 (4): 341–54.</p>
</div>
<div id="ref-dewittRecognitionNovelMelodies1986">
<p>Dewitt, Lucinda A., and Robert G. Crowder. 1986. “Recognition of Novel Melodies After Brief Delays.” <em>Music Perception: An Interdisciplinary Journal</em> 3 (3): 259–74. <a href="https://doi.org/10.2307/40285336" class="uri">https://doi.org/10.2307/40285336</a>.</p>
</div>
<div id="ref-ouraMemoryMelodiesSubjects1988">
<p>Oura, Yoko, and Giyoo Hatano. 1988. “Memory for Melodies Among Subjects Differing in Age and Experience in Music.” <em>Psychology of Music</em> 16: 91–109.</p>
</div>
<div id="ref-handelListeningIntroductionPerception1989">
<p>Handel, Stephen. 1989. <em>Listening: An Introduction to the Perception of Auditory Events</em>. Cambridge: MIT Press.</p>
</div>
<div id="ref-dowlingExpectancyAttentionMelody1990">
<p>Dowling, W. 1990. “Expectancy and Attention in Melody Perception.” <em>Psychomusicology: A Journal of Research in Music Cognition</em> 9 (2): 148–60. <a href="https://doi.org/10.1037/h0094150" class="uri">https://doi.org/10.1037/h0094150</a>.</p>
</div>
<div id="ref-eerolaExpectancySamiYoiks2009">
<p>Eerola, Tuomas, Jukka Louhivuori, and Edward Lebaka. 2009. “Expectancy in Sami Yoiks Revisited: The Role of Data-Driven and Schema-Driven Knowledge in the Formation of Melodic Expectations.” <em>Musicae Scientiae</em> 13 (2): 231–72. <a href="https://doi.org/10.1177/102986490901300203" class="uri">https://doi.org/10.1177/102986490901300203</a>.</p>
</div>
<div id="ref-stevensMusicPerceptionCognition2012">
<p>Stevens, Catherine J. 2012. “Music Perception and Cognition: A Review of Recent Cross-Cultural Research.” <em>Topics in Cognitive Science</em> 4 (4): 653–67. <a href="https://doi.org/10.1111/j.1756-8765.2012.01215.x" class="uri">https://doi.org/10.1111/j.1756-8765.2012.01215.x</a>.</p>
</div>
<div id="ref-pearceAuditoryExpectationInformation2012">
<p>Pearce, Marcus T., and Geraint A. Wiggins. 2012. “Auditory Expectation: The Information Dynamics of Music Perception and Cognition.” <em>Topics in Cognitive Science</em> 4 (4): 625–52. <a href="https://doi.org/10.1111/j.1756-8765.2012.01214.x" class="uri">https://doi.org/10.1111/j.1756-8765.2012.01214.x</a>.</p>
</div>
<div id="ref-pearceStatisticalLearningProbabilistic2018a">
<p>Pearce, Marcus T. 2018. “Statistical Learning and Probabilistic Prediction in Music Cognition: Mechanisms of Stylistic Enculturation: Enculturation: Statistical Learning and Prediction.” <em>Annals of the New York Academy of Sciences</em> 1423 (1): 378–95. <a href="https://doi.org/10.1111/nyas.13654" class="uri">https://doi.org/10.1111/nyas.13654</a>.</p>
</div>
<div id="ref-millerMagicalNumberSeven1956">
<p>Miller, George A. 1956. “The Magical Number Seven, Plus or Minus Two: Some Limits on Our Capacity for Processing Information.”</p>
</div>
<div id="ref-arthurPerceptualStudyScaledegree2018">
<p>Arthur, Claire. 2018. “A Perceptual Study of Scale-Degree Qualia in Context.” <em>Music Perception: An Interdisciplinary Journal</em> 35 (3): 295–314. <a href="https://doi.org/10.1525/mp.2018.35.3.295" class="uri">https://doi.org/10.1525/mp.2018.35.3.295</a>.</p>
</div>
<div id="ref-taylorStrategiesMemoryShort1983">
<p>Taylor, Jack A., and Randall G. Pembrook. 1983. “Strategies in Memory for Short Melodies: An Extension of Otto Ortmann’s 1933 Study.” <em>Psychomusicology: A Journal of Research in Music Cognition</em> 3 (1): 16–35. <a href="https://doi.org/10.1037/h0094258" class="uri">https://doi.org/10.1037/h0094258</a>.</p>
</div>
<div id="ref-goldmanImprovisationExperiencePredicts2018a">
<p>Goldman, Andrew, Tyreek Jackson, and Paul Sajda. 2018. “Improvisation Experience Predicts How Musicians Categorize Musical Structures.” <em>Psychology of Music</em>, June, 030573561877944. <a href="https://doi.org/10.1177/0305735618779444" class="uri">https://doi.org/10.1177/0305735618779444</a>.</p>
</div>
<div id="ref-laneChessKnowledgePredicts2018">
<p>Lane, David M., and Yu-Hsuan A. Chang. 2018. “Chess Knowledge Predicts Chess Memory Even After Controlling for Chess Experience: Evidence for the Role of High-Level Processes.” <em>Memory &amp; Cognition</em> 46 (3): 337–48. <a href="https://doi.org/10.3758/s13421-017-0768-2" class="uri">https://doi.org/10.3758/s13421-017-0768-2</a>.</p>
</div>
<div id="ref-meyerEmotionMeaningMusic1956">
<p>Meyer, Leonard. 1956. <em>Emotion and Meaning in Music</em>. Chicago: University of Chicago Press.</p>
</div>
<div id="ref-ritchieIntelligenceAllThat2015">
<p>Ritchie, Stuart. 2015. <em>Intelligence: All That Matters</em>. All That Matters. Hodder &amp; Stoughton.</p>
</div>
<div id="ref-unsworthAutomatedVersionOperation2005">
<p>Unsworth, Nash, Richard P. Heitz, Josef C. Schrock, and Randall W. Engle. 2005. “An Automated Version of the Operation Span Task.” <em>Behavior Research Methods</em> 37 (3): 498–505. <a href="https://doi.org/10.3758/BF03192720" class="uri">https://doi.org/10.3758/BF03192720</a>.</p>
</div>
<div id="ref-gouldMismeasureMan1996">
<p>Gould, Stephan Jay. 1996. <em>The Mismeasure of Man</em>. WW Norton &amp; Company.</p>
</div>
<div id="ref-darwinOriginSpecies1859">
<p>Darwin, Charles. 1859. <em>On the Origin of Species</em>. Routledge.</p>
</div>
<div id="ref-spearmanGeneralIntelligenceObjectively1904">
<p>Spearman, Charles. 1904. “&quot;General Intelligence,&quot; Objectively Determined and Measured,” 93.</p>
</div>
<div id="ref-cattellAbilitiesTheirGrowth1971">
<p>Cattell, R. 1971. <em>Abilities: Their Growth, Structure, and Action</em>. Boston, MA: Houghton Mifflin.</p>
</div>
<div id="ref-jhornTheoryFluidCrystalized1994">
<p>J Horn. 1994. “Theory of Fluid and Crystalized Intelligence.” In <em>Encyclopedia of Human Intelligence</em>, 443–51. R. Sternberg. New York: MacMillan Reference Library.</p>
</div>
<div id="ref-matzkeIssuePowerIdentification2010">
<p>Matzke, Dora, Conor V. Dolan, and Dylan Molenaar. 2010. “The Issue of Power in the Identification of ‘G’ with Lower-Order Factors.” <em>Intelligence</em> 38 (3): 336–44. <a href="https://doi.org/10.1016/j.intell.2010.02.001" class="uri">https://doi.org/10.1016/j.intell.2010.02.001</a>.</p>
</div>
<div id="ref-kovacsProcessOverlapTheory2016">
<p>Kovacs, Kristof, and Andrew R. A. Conway. 2016. “Process Overlap Theory: A Unified Account of the General Factor of Intelligence.” <em>Psychological Inquiry</em> 27 (3): 151–77. <a href="https://doi.org/10.1080/1047840X.2016.1153946" class="uri">https://doi.org/10.1080/1047840X.2016.1153946</a>.</p>
</div>
<div id="ref-borsboomTheoreticalStatusLatent2003">
<p>Borsboom, Denny, Gideon J. Mellenbergh, and Jaap van Heerden. 2003. “The Theoretical Status of Latent Variables.” <em>Psychological Review</em> 110 (2): 203–19. <a href="https://doi.org/10.1037/0033-295X.110.2.203" class="uri">https://doi.org/10.1037/0033-295X.110.2.203</a>.</p>
</div>
<div id="ref-cowanWorkingMemoryCapacity2005">
<p>Cowan, Nelson. 2005. <em>Working Memory Capacity</em>. Working Memory Capacity. New York, NY, US: Psychology Press. <a href="https://doi.org/10.4324/9780203342398" class="uri">https://doi.org/10.4324/9780203342398</a>.</p>
</div>
<div id="ref-broadbentPerceptionCommunication1958">
<p>Broadbent, Donald E (Donald Eric). 1958. <em>Perception and Communication</em>. Pergamon.</p>
</div>
<div id="ref-atkinsonHUMANMEMORYPROPOSED1968">
<p>Atkinson, R C, and R M Shiffrin. 1968. “HUMAN MEMORY: A PROPOSED SYSTEM AND ITS CONTROL PROCESSES!” <em>Human Memory</em>, 54.</p>
</div>
<div id="ref-baddeleyWorkingMemory1974">
<p>Baddeley, Alan D., and Graham Hitch. 1974. “Working Memory.” In <em>Psychology of Learning and Motivation</em>, 8:47–89. Elsevier. <a href="https://doi.org/10.1016/S0079-7421(08)60452-1" class="uri">https://doi.org/10.1016/S0079-7421(08)60452-1</a>.</p>
</div>
<div id="ref-berzWorkingMemoryMusic1995">
<p>Berz, William L. 1995. “Working Memory in Music: A Theoretical Model.” <em>Music Perception: An Interdisciplinary Journal</em> 12 (3): 353–64. <a href="https://doi.org/10.2307/40286188" class="uri">https://doi.org/10.2307/40286188</a>.</p>
</div>
<div id="ref-williamsonMusiciansNonmusiciansShortterm2010">
<p>Williamson, Victoria J., Alan D. Baddeley, and Gramham J. Hitch. 2010. “Musicians’ and Nonmusicians’ Short-Term Memory for Verbal and Musical Sequences: Comparing Phonological Similarity and Pitch Proximity.” <em>Memory &amp; Cognition</em> 38 (2): 163–75. <a href="https://doi.org/10.3758/MC.38.2.163" class="uri">https://doi.org/10.3758/MC.38.2.163</a>.</p>
</div>
<div id="ref-wollnerAttentionalFlexibilityMemory2016">
<p>Wöllner, Clemens, and Andrea R. Halpern. 2016. “Attentional Flexibility and Memory Capacity in Conductors and Pianists.” <em>Attention, Perception, &amp; Psychophysics</em> 78 (1): 198–208. <a href="https://doi.org/10.3758/s13414-015-0989-z" class="uri">https://doi.org/10.3758/s13414-015-0989-z</a>.</p>
</div>
<div id="ref-cowanEvolvingConceptionsMemory1988">
<p>Cowan, Nelson. 1988. “Evolving Conceptions of Memory Storage, Selective Attention, and Their Mutual Constraints Within the Human Information-Processing System.” <em>Psychological Bulletin</em> 104 (2): 163–91.</p>
</div>
<div id="ref-millerHistoryPsychologyAutobiography1989">
<p>Miller, George A. 1989. <em>A History of Psychology in Autobiography</em>. L. Gardner. Vol. VIII. Stanford, CA: Stanford University Press.</p>
</div>
<div id="ref-cowanMagicalMysteryFour2010">
<p>Cowan, Nelson. 2010. “The Magical Mystery Four: How Is Working Memory Capacity Limited, and Why?” <em>Current Directions in Psychological Science</em> 19 (1): 51–57. <a href="https://doi.org/10.1177/0963721409359277" class="uri">https://doi.org/10.1177/0963721409359277</a>.</p>
</div>
<div id="ref-unsworthComplexWorkingMemory2009">
<p>Unsworth, Nash, Thomas S. Redick, Richard P. Heitz, James M. Broadway, and Randall W. Engle. 2009. “Complex Working Memory Span Tasks and Higher-Order Cognition: A Latent-Variable Analysis of the Relationship Between Processing and Storage.” <em>Memory</em> 17 (6): 635–54. <a href="https://doi.org/10.1080/09658210902998047" class="uri">https://doi.org/10.1080/09658210902998047</a>.</p>
</div>
<div id="ref-conwayWorkingMemorySpan2005">
<p>Conway, Andrew R. A., Michael J. Kane, Michael F. Bunting, D. Zach Hambrick, Oliver Wilhelm, and Randall W. Engle. 2005. “Working Memory Span Tasks: A Methodological Review and User’s Guide.” <em>Psychonomic Bulletin &amp; Review</em> 12 (5): 769–86. <a href="https://doi.org/10.3758/BF03196772" class="uri">https://doi.org/10.3758/BF03196772</a>.</p>
</div>
<div id="ref-krumhanslCognitiveFoundationsMusical2001">
<p>Krumhansl, Carol. 2001. <em>Cognitive Foundations of Musical Pitch</em>. Oxford University Press.</p>
</div>
<div id="ref-schenkerFreieSatz1935">
<p>Schenker, Heinrich. 1935. <em>Der Freie Satz</em>. Vol. III. Universal Edition.</p>
</div>
<div id="ref-saffranStatisticalLearningTone1999">
<p>Saffran, Jenny R, Elizabeth K Johnson, Richard N Aslin, and Elissa L Newport. 1999. “Statistical Learning of Tone Sequences by Human Infants and Adults.” <em>Cognition</em> 70 (1): 27–52. <a href="https://doi.org/10.1016/S0010-0277(98)00075-4" class="uri">https://doi.org/10.1016/S0010-0277(98)00075-4</a>.</p>
</div>
<div id="ref-talaminiMusiciansHaveBetter2017">
<p>Talamini, Francesca, Gianmarco Altoè, Barbara Carretti, and Massimo Grassi. 2017. “Musicians Have Better Memory Than Nonmusicians: A Meta-Analysis.” Edited by Lutz Jäncke. <em>PLOS ONE</em> 12 (10): e0186773. <a href="https://doi.org/10.1371/journal.pone.0186773" class="uri">https://doi.org/10.1371/journal.pone.0186773</a>.</p>
</div>
<div id="ref-kopiezDynamicModelSkills2006">
<p>Kopiez, Reinhard, and Ji Lee. 2006. “Towards a Dynamic Model of Skills Involved in Sight Reading Music.” <em>Music Education Research</em> 8 (1): 97–120. <a href="https://doi.org/10.1080/14613800600570785" class="uri">https://doi.org/10.1080/14613800600570785</a>.</p>
</div>
<div id="ref-kopiezGeneralModelSkills2008">
<p>Kopiez, Reinhard, and Ji Lee. 2008. “Towards a General Model of Skills Involved in Sight Reading Music.” <em>Music Education Research</em> 10 (1): 41–62. <a href="https://doi.org/10.1080/14613800701871363" class="uri">https://doi.org/10.1080/14613800701871363</a>.</p>
</div>
<div id="ref-meinzDeliberatePracticeNecessary2010">
<p>Meinz, Elizabeth J., and David Z. Hambrick. 2010. “Deliberate Practice Is Necessary but Not Sufficient to Explain Individual Differences in Piano Sight-Reading Skill: The Role of Working Memory Capacity.” <em>Psychological Science</em> 21 (7): 914–19. <a href="https://doi.org/10.1177/0956797610373933" class="uri">https://doi.org/10.1177/0956797610373933</a>.</p>
</div>
<div id="ref-nicholsScoreOneJazz2018">
<p>Nichols, Bryan E., Clemens Wöllner, and Andrea R. Halpern. 2018. “Score One for Jazz: Working Memory in Jazz and Classical Musicians.” <em>Psychomusicology: Music, Mind, and Brain</em> 28 (2): 101–7. <a href="https://doi.org/10.1037/pmu0000211" class="uri">https://doi.org/10.1037/pmu0000211</a>.</p>
</div>
<div id="ref-engleWorkingMemoryCapacity2002">
<p>Engle, Randall W. 2002. “Working Memory Capacity as Executive Attention.” <em>Current Directions in Psychological Science</em> 11 (1): 19–23. <a href="https://doi.org/10.1111/1467-8721.00160" class="uri">https://doi.org/10.1111/1467-8721.00160</a>.</p>
</div>
<div id="ref-colleyWorkingMemoryAuditory2018">
<p>Colley, Ian D, Peter E Keller, and Andrea R Halpern. 2018. “Working Memory and Auditory Imagery Predict Sensorimotor Synchronisation with Expressively Timed Music.” <em>Quarterly Journal of Experimental Psychology</em> 71 (8): 1781–96. <a href="https://doi.org/10.1080/17470218.2017.1366531" class="uri">https://doi.org/10.1080/17470218.2017.1366531</a>.</p>
</div>
<div id="ref-mullensiefenInvestigatingImportanceSelftheories2015">
<p>Müllensiefen, Daniel, Peter Harrison, Francesco Caprini, and Amy Fancourt. 2015. “Investigating the Importance of Self-Theories of Intelligence and Musicality for Students’ Academic and Musical Achievement.” <em>Frontiers in Psychology</em> 6 (November). <a href="https://doi.org/10.3389/fpsyg.2015.01702" class="uri">https://doi.org/10.3389/fpsyg.2015.01702</a>.</p>
</div>
<div id="ref-schellenbergMusicNonmusicalAbilities2017">
<p>Schellenberg, E Glenn. 2017. “Music and Nonmusical Abilities,” 17.</p>
</div>
<div id="ref-gibsonEnhancedDivergentThinking2009">
<p>Gibson, Crystal, Bradley S. Folley, and Sohee Park. 2009. “Enhanced Divergent Thinking and Creativity in Musicians: A Behavioral and Near-Infrared Spectroscopy Study.” <em>Brain and Cognition</em> 69 (1): 162–69. <a href="https://doi.org/10.1016/j.bandc.2008.07.009" class="uri">https://doi.org/10.1016/j.bandc.2008.07.009</a>.</p>
</div>
<div id="ref-hilleAssociationsMusicEducation2011">
<p>Hille, Katrin, Kilian Gust, Ulrich Bitz, and Thomas Kammer. 2011. “Associations Between Music Education, Intelligence, and Spelling Ability in Elementary School.” <em>Advances in Cognitive Psychology</em> 7 (-1): 1–6. <a href="https://doi.org/10.2478/v10053-008-0082-4" class="uri">https://doi.org/10.2478/v10053-008-0082-4</a>.</p>
</div>
<div id="ref-schellenbergExaminingAssociationMusic2011">
<p>Schellenberg, E. 2011. “Examining the Association Between Music Lessons and Intelligence: Music Lessons and Intelligence.” <em>British Journal of Psychology</em> 102 (3): 283–302. <a href="https://doi.org/10.1111/j.2044-8295.2010.02000.x" class="uri">https://doi.org/10.1111/j.2044-8295.2010.02000.x</a>.</p>
</div>
<div id="ref-schellenbergMusicTrainingEmotion2012">
<p>Schellenberg, E. Glenn, and Monika Mankarious. 2012. “Music Training and Emotion Comprehension in Childhood.” <em>Emotion</em> 12 (5): 887–91. <a href="https://doi.org/10.1037/a0027971" class="uri">https://doi.org/10.1037/a0027971</a>.</p>
</div>
<div id="ref-corrigallPredictingWhoTakes2015">
<p>Corrigall, Kathleen A., and E. Glenn Schellenberg. 2015. “Predicting Who Takes Music Lessons: Parent and Child Characteristics.” <em>Frontiers in Psychology</em> 6 (March). <a href="https://doi.org/10.3389/fpsyg.2015.00282" class="uri">https://doi.org/10.3389/fpsyg.2015.00282</a>.</p>
</div>
<div id="ref-degeMusicLessonsIntelligence2011">
<p>Degé, Franziska, Claudia Kubicek, and Gudrun Schwarzer. 2011. “Music Lessons and Intelligence: A Relation Mediated by Executive Functions.” <em>Music Perception: An Interdisciplinary Journal</em> 29 (2): 195–201. <a href="https://doi.org/10.1525/mp.2011.29.2.195" class="uri">https://doi.org/10.1525/mp.2011.29.2.195</a>.</p>
</div>
<div id="ref-schellenbergLongtermPositiveAssociations2006">
<p>Schellenberg, E. 2006. “Long-Term Positive Associations Between Music Lessons and IQ.” <em>Journal of Educational Psychology</em> 98 (2): 457–68. <a href="https://doi.org/10.1037/0022-0663.98.2.457" class="uri">https://doi.org/10.1037/0022-0663.98.2.457</a>.</p>
</div>
<div id="ref-corrigallAssociationsLengthMusic2011">
<p>Corrigall, Kathleen A., and Laurel J. Trainor. 2011. “Associations Between Length of Music Training and Reading Skills in Children.” <em>Music Perception: An Interdisciplinary Journal</em> 29 (2): 147–55. <a href="https://doi.org/10.1525/mp.2011.29.2.147" class="uri">https://doi.org/10.1525/mp.2011.29.2.147</a>.</p>
</div>
<div id="ref-parbery-clarkMusicalExperienceAging2011">
<p>Parbery-Clark, Alexandra, Dana L. Strait, Samira Anderson, Emily Hittner, and Nina Kraus. 2011. “Musical Experience and the Aging Auditory System: Implications for Cognitive Abilities and Hearing Speech in Noise.” Edited by Peter Csermely. <em>PLoS ONE</em> 6 (5): e18082. <a href="https://doi.org/10.1371/journal.pone.0018082" class="uri">https://doi.org/10.1371/journal.pone.0018082</a>.</p>
</div>
<div id="ref-straitMusicalTrainingEarly2012">
<p>Strait, Dana L., Alexandra Parbery-Clark, Emily Hittner, and Nina Kraus. 2012. “Musical Training During Early Childhood Enhances the Neural Encoding of Speech in Noise.” <em>Brain and Language</em> 123 (3): 191–201. <a href="https://doi.org/10.1016/j.bandl.2012.09.001" class="uri">https://doi.org/10.1016/j.bandl.2012.09.001</a>.</p>
</div>
<div id="ref-francoisMusicTrainingDevelopment2013">
<p>Francois, C., J. Chobert, M. Besson, and D. Schon. 2013. “Music Training for the Development of Speech Segmentation.” <em>Cerebral Cortex</em> 23 (9): 2038–43. <a href="https://doi.org/10.1093/cercor/bhs180" class="uri">https://doi.org/10.1093/cercor/bhs180</a>.</p>
</div>
<div id="ref-morenoMusicalTrainingInfluences2009">
<p>Moreno, Sylvain, Carlos Marques, Andreia Santos, Manuela Santos, São Luís Castro, and Mireille Besson. 2009. “Musical Training Influences Linguistic Abilities in 8-Year-Old Children: More Evidence for Brain Plasticity.” <em>Cerebral Cortex</em> 19 (3): 712–23. <a href="https://doi.org/10.1093/cercor/bhn120" class="uri">https://doi.org/10.1093/cercor/bhn120</a>.</p>
</div>
<div id="ref-mehrTwoRandomizedTrials2013">
<p>Mehr, Samuel A., Adena Schachner, Rachel C. Katz, and Elizabeth S. Spelke. 2013. “Two Randomized Trials Provide No Consistent Evidence for Nonmusical Cognitive Benefits of Brief Preschool Music Enrichment.” Edited by Marina Pavlova. <em>PLoS ONE</em> 8 (12): e82007. <a href="https://doi.org/10.1371/journal.pone.0082007" class="uri">https://doi.org/10.1371/journal.pone.0082007</a>.</p>
</div>
<div id="ref-swaminathanRevisitingAssociationMusic2017">
<p>Swaminathan, Swathi, E. Glenn Schellenberg, and Safia Khalil. 2017. “Revisiting the Association Between Music Lessons and Intelligence: Training Effects or Music Aptitude?” <em>Intelligence</em> 62 (May): 119–24. <a href="https://doi.org/10.1016/j.intell.2017.03.005" class="uri">https://doi.org/10.1016/j.intell.2017.03.005</a>.</p>
</div>
<div id="ref-ericssonRoleDeliberatePractice1993">
<p>Ericsson, K Anders, Ralf Th Krampe, and Clemens Tesch-Romer. 1993. “The Role of Deliberate Practice in the Acquisition of Expert Performance.”</p>
</div>
<div id="ref-dettermanMoreComprehensiveTheory1999">
<p>Detterman, Douglas K., and Joanne Ruthsatz. 1999. “Toward a More Comprehensive Theory of Exceptional Abilities.” <em>Journal for the Education of the Gifted</em> 22 (2): 148–58. <a href="https://doi.org/10.1177/016235329902200204" class="uri">https://doi.org/10.1177/016235329902200204</a>.</p>
</div>
<div id="ref-ruthsatzBecomingExpertMusical2008">
<p>Ruthsatz, Joanne, Douglas Detterman, William S. Griscom, and Britney A. Cirullo. 2008. “Becoming an Expert in the Musical Domain: It Takes More Than Just Practice.” <em>Intelligence</em> 36 (4): 330–38. <a href="https://doi.org/10.1016/j.intell.2007.08.003" class="uri">https://doi.org/10.1016/j.intell.2007.08.003</a>.</p>
</div>
<div id="ref-mosingPracticeDoesNot2014">
<p>Mosing, Miriam A., Guy Madison, Nancy L. Pedersen, Ralf Kuja-Halkola, and Fredrik Ullén. 2014. “Practice Does Not Make Perfect: No Causal Effect of Music Practice on Music Ability.” <em>Psychological Science</em> 25 (9): 1795–1803. <a href="https://doi.org/10.1177/0956797614541990" class="uri">https://doi.org/10.1177/0956797614541990</a>.</p>
</div>
<div id="ref-bakerExaminingMusicalSophistication2018a">
<p>Baker, David, Juan Ventura, Matthew Calamia, Daniel Shanahan, and Emily Elliott. 2018. “Examining Musical Sophistication: A Replication and Theoretical Commentary on the Goldsmiths Musical Sophistication Index.” <em>Musicae Scientiae</em>.</p>
</div>
<div id="ref-blackingHowMusicalMan2000">
<p>Blacking, John. 2000. <em>How Musical Is Man?</em> 6. printing. The Jessie and John Danz Lectures. Seattle: University of Washington Press.</p>
</div>
<div id="ref-murphyHowFarTests1999">
<p>Murphy, Christopher. 1999. “How Far Do Tests of Musical Ability Shed Light on the Nature of Musical Intelligence?” <em>British Journal of Music Education</em> 16 (1): 39–50. <a href="https://doi.org/10.1017/S0265051799000133" class="uri">https://doi.org/10.1017/S0265051799000133</a>.</p>
</div>
<div id="ref-chenetteReframingAuralSkills2019">
<p>Chenette, Timothy K. 2019. “Reframing Aural Skills Instruction Based on Research in Working Memory.” <em>Journal for Music Theory Pedagogy</em>, 18.</p>
</div>
<div id="ref-furbyEffectsPeerTutoring2016">
<p>Furby, Victoria J. 2016. “The Effects of Peer Tutoring on the Aural Skills Performance of Undergraduate Music Majors.” <em>Update: Applications of Research in Music Education</em> 34 (3): 33–39. <a href="https://doi.org/10.1177/8755123314556547" class="uri">https://doi.org/10.1177/8755123314556547</a>.</p>
</div>
<div id="ref-harrisonEffectsMusicalAptitude1994">
<p>Harrison, Carole S., Edward P. Asmus, and Richard T. Serpe. 1994. “Effects of Musical Aptitude, Academic Ability, Music Experience, and Motivation on Aural Skills.” <em>Journal of Research in Music Education</em> 42 (2): 131. <a href="https://doi.org/10.2307/3345497" class="uri">https://doi.org/10.2307/3345497</a>.</p>
</div>
<div id="ref-bantonRoleVisualAuditory1995">
<p>Banton, Louise. 1995. “The Role of Visual and Auditory Feedback During Sight-Reading of Music.” <em>Psychology of Music</em> 23 (1): 3–16.</p>
</div>
<div id="ref-blandSightSingingMelodic1984">
<p>Bland, Leland. 1984. <em>Sight Singing Through Melodic Analysis: A Guide to the Study of Sight Singing and an Aid to Ear Training Instruction</em>. Scarecrow Press.</p>
</div>
<div id="ref-rootMethodicalSightSingingLessons1931">
<p>Root, Frederick. 1931. <em>Methodical Sight-Singing: Lessons in Vocal Culture</em>. Bryn Mawr, Pennsylvania: Theodore Presser Co.</p>
</div>
<div id="ref-fournierCognitiveStrategiesSightsinging2017a">
<p>Fournier, Guillaume, Maria Teresa Moreno Sala, Francis Dubé, and Susan O’Neill. 2017. “Cognitive Strategies in Sight-Singing: The Development of an Inventory for Aural Skills Pedagogy.” <em>Psychology of Music</em>, December, 030573561774514. <a href="https://doi.org/10.1177/0305735617745149" class="uri">https://doi.org/10.1177/0305735617745149</a>.</p>
</div>
<div id="ref-asmusMusicTeachingMusic2004">
<p>Asmus, Edward P. 2004. “Music Teaching and Music Literacy.” <em>Journal of Music Teacher Education</em> 13 (2): 6–8. <a href="https://doi.org/10.1177/10570837040130020102" class="uri">https://doi.org/10.1177/10570837040130020102</a>.</p>
</div>
<div id="ref-thompsonPitchInternalizationStrategies2003">
<p>Thompson, Kathy. 2003. “Pitch Internalization Strategies of Professional Musicians.” PhD thesis, University of Oklahoma.</p>
</div>
<div id="ref-rogersTeachingApproachesMusic2004">
<p>Rogers, Michael R. 2004. <em>Teaching Approaches in Music Theory: An Overview of Pedagogical Philosophies</em>. 2nd ed. Carbondale: Southern Illinois University Press.</p>
</div>
<div id="ref-harrisonValidityMusicalAptitude1987">
<p>Harrison, Carole. 1987. “The Validity and Musical Aptitude Profile for Predicting Grades in Freshman Music Theory.” <em>Educational and Psychological Measurment</em> 47 (2): 477–82.</p>
</div>
<div id="ref-cambouropoulosHowSimilarSimilar2009">
<p>Cambouropoulos, Emilios. 2009. “How Similar Is Similar?” <em>Musicae Scientiae</em> 13 (1_suppl): 7–24. <a href="https://doi.org/10.1177/102986490901300102" class="uri">https://doi.org/10.1177/102986490901300102</a>.</p>
</div>
<div id="ref-wigginsModelsMusicalSimilarity2007">
<p>Wiggins, Geraint A. 2007. “Models of Musical Similarity.” <em>Musicae Scientiae</em> 11 (1_suppl): 315–38. <a href="https://doi.org/10.1177/102986490701100112" class="uri">https://doi.org/10.1177/102986490701100112</a>.</p>
</div>
<div id="ref-clarkeWaysListeningEcological2005">
<p>Clarke, Eric F. 2005. <em>Ways of Listening: An Ecological Approach to the Perception of Musical Meaning</em>. Oxford: Oxford Univ. Pr.</p>
</div>
<div id="ref-salzerStructuralHearingTonal1982">
<p>Salzer, Felix, and Leopold Mannes. 1982. <em>Structural Hearing: Tonal Coherence in Music</em>. New York: Dover.</p>
</div>
<div id="ref-schachterSchenkerStudies2006">
<p>Schachter, Carl. 2006. <em>Schenker Studies 2</em>. Cambridge: Cambridge University Press.</p>
</div>
<div id="ref-schenkerSchenkerStudies1990">
<p>Schenker, Heinrich, Hedi Siegel, and Carl Schachter, eds. 1990. <em>Schenker Studies</em>. Cambridge ; New York: Cambridge University Press.</p>
</div>
<div id="ref-agawuHowWeGot2004">
<p>Agawu, Kofi. 2004. “How We Got Out of Analysis, and How to Get Back in Again.” <em>Music Analysis</em> 23 (2-3): 267–86. <a href="https://doi.org/10.1111/j.0262-5245.2004.00204.x" class="uri">https://doi.org/10.1111/j.0262-5245.2004.00204.x</a>.</p>
</div>
<div id="ref-kermanContemplatingMusicChallenges1986">
<p>Kerman, Joseph. 1986. <em>Contemplating Music: Challenges to Musicology</em>. Cambridge, Mass: Harvard Univ. Press.</p>
</div>
<div id="ref-narmourAnalysisCognitionBasic1990">
<p>Narmour, Eugene. 1990. <em>The Analysis and Cognition of Basic Melodic Structures: The Implication-Realization Model.</em> Chicago: University of Chicago Press.</p>
</div>
<div id="ref-narmourAnalysisCognitionMelodic1992">
<p>Narmour, Eugene. 1992. <em>The Analysis and Cognition of Melodic Complexity: The Implication-Realization Model</em>. University of Chicago Press.</p>
</div>
<div id="ref-schellenbergSimplifyingImplicationRealizationModel1997">
<p>Schellenberg, E. Glenn. 1997. “Simplifying the Implication-Realization Model of Melodic Expectancy.” <em>Music Perception: An Interdisciplinary Journal</em> 14 (3): 295–318. <a href="https://doi.org/10.2307/40285723" class="uri">https://doi.org/10.2307/40285723</a>.</p>
</div>
<div id="ref-margulisModelMelodicExpectation2005">
<p>Margulis, Elizabeth Hellmuth. 2005. “A Model of Melodic Expectation.” <em>Music Perception: An Interdisciplinary Journal</em> 22 (4): 663–714. <a href="https://doi.org/10.1525/mp.2005.22.4.663" class="uri">https://doi.org/10.1525/mp.2005.22.4.663</a>.</p>
</div>
<div id="ref-huronSweetAnticipation2006">
<p>Huron, David. 2006. <em>Sweet Anticipation</em>. MIT Press.</p>
</div>
<div id="ref-pearceConstructionEvaluationStatistical2005">
<p>Pearce, Marcus. 2005. “The Construction and Evaluation of Statistical Models of Melodic Structure in Music Perception and Composition.” PhD thesis, Department of Computer Science: City University of London.</p>
</div>
<div id="ref-conklinMultipleViewpointSystems1995">
<p>Conklin, Darrell, and Ian H. Witten. 1995. “Multiple Viewpoint Systems for Music Prediction.” <em>Journal of New Music Research</em> 24 (1): 51–73. <a href="https://doi.org/10.1080/09298219508570672" class="uri">https://doi.org/10.1080/09298219508570672</a>.</p>
</div>
<div id="ref-ortmannTonalDeterminantsMelodic1933">
<p>Ortmann, Otto. 1933. “Some Tonal Determinants of Melodic Memory.” <em>Journal of Educational Psychology</em> 24 (6): 454–67. <a href="https://doi.org/10.1037/h0075218" class="uri">https://doi.org/10.1037/h0075218</a>.</p>
</div>
<div id="ref-huronHumdrumToolkitReference1994">
<p>Huron, David. 1994. “The Humdrum Toolkit: Reference Manual.” Center for Computer Assisted Research in the Humanities.</p>
</div>
<div id="ref-cuthbertMusic21ToolkitComputerAided2010">
<p>Cuthbert, Michael Scott, and Christopher Ariza. 2010. “Music21: A Toolkit for Computer-Aided Musicology and Symbolic Music Data,” 7.</p>
</div>
<div id="ref-huronMelodicArchWestern1996">
<p>Huron, David. 1996. “The Melodic Arch in Western Folk Songs.” <em>Computing in Musicology</em> 10: 3–23.</p>
</div>
<div id="ref-schaffrathEssenFolkSong1995">
<p>Schaffrath, Helmuth. 1995. “The Essen Folk Song Collection, D. Huron.”</p>
</div>
<div id="ref-grabeDurationalVariabilitySpeech2002">
<p>Grabe, Esther. 2002. “Durational Variability in Speech and the Rhythm Class Hypothesis.” <em>Papers in Laboratory Phonology</em>, 16.</p>
</div>
<div id="ref-mullensiefenFantasticFeatureANalysis2009">
<p>Mullensiefen, Daniel. 2009. “Fantastic: Feature ANalysis Technology Accessing STatistics (in a Corpus): Technical Report V1.5.”</p>
</div>
<div id="ref-manningFoundationsStatisticalNatural1999">
<p>Manning, Christopher D., and Hinrich Schütze. 1999. <em>Foundations of Statistical Natural Language Processing</em>. Cambridge, Mass: MIT Press.</p>
</div>
<div id="ref-mullensiefenCourtDecisionsMusic2009">
<p>Müllensiefen, Daniel, and Marc Pendzich. 2009. “Court Decisions on Music Plagiarism and the Predictive Value of Similarity Algorithms.” <em>Musicae Scientiae</em> 48: 257–95.</p>
</div>
<div id="ref-kopiezAufSucheNach2011">
<p>Kopiez, Reinhard, and Daniel Mullensiefen. 2011. “Auf Der Suche Nach Den ‘Popularitätsfaktoren’ in Den Song-Melodien Des Beatles-Albums Revolver: Eine Computergestützte Feature-Analyse [in Search of Features Explaining the Popularity of the Tunes from the Beatles Album Revolver: A Computer-Assisted Feature Analysis].” <em>Musik Und Popularitat. Beitrage Zu Einer Kulturgeschichte Zwischen</em>, 207–25.</p>
</div>
<div id="ref-mullensiefenRoleFeaturesContext2014">
<p>Müllensiefen, Daniel, and Andrea R. Halpern. 2014. “The Role of Features and Context in Recognition of Novel Melodies.” <em>Music Perception: An Interdisciplinary Journal</em> 31 (5): 418–35. <a href="https://doi.org/10.1525/mp.2014.31.5.418" class="uri">https://doi.org/10.1525/mp.2014.31.5.418</a>.</p>
</div>
<div id="ref-jakubowskiDissectingEarwormMelodic2017">
<p>Jakubowski, Kelly, Sebastian Finkel, Lauren Stewart, and Daniel Müllensiefen. 2017. “Dissecting an Earworm: Melodic Features and Song Popularity Predict Involuntary Musical Imagery.” <em>Journal of Aesthetics, Creativity, and the Arts</em> 11 (2): 112–35.</p>
</div>
<div id="ref-williamsonEarwormsThreeAngles2012">
<p>Williamson, Victoria J, and Daniel Müllensiefen. 2012. “Earworms from Three Angles: Situational Antecedents, Personality Predisposition and the Quest for a Musical Formula.” <em>Proceedings from the 12th International Conference on Music Perception and Cognition</em>, 1124–32.</p>
</div>
<div id="ref-balenCorpusAnalyisTools2015">
<p>Balen, Jan Van, John Ashley Burgoyne, and Dimitrios Bountouridis. 2015. “Corpus Analyis Tools for Computational Hook Discovery.” <em>Proceedings of International Society for Music Information Retrevial 15</em>.</p>
</div>
<div id="ref-bakerPerceptionLeitmotivesRichard2017">
<p>Baker, David J., and Daniel Müllensiefen. 2017. “Perception of Leitmotives in Richard Wagner’s Der Ring Des Nibelungen.” <em>Frontiers in Psychology</em> 8 (May). <a href="https://doi.org/10.3389/fpsyg.2017.00662" class="uri">https://doi.org/10.3389/fpsyg.2017.00662</a>.</p>
</div>
<div id="ref-harrisonModellingMelodicDiscrimination2016">
<p>Harrison, Peter M.C., Jason Jiří Musil, and Daniel Müllensiefen. 2016. “Modelling Melodic Discrimination Tests: Descriptive and Explanatory Approaches.” <em>Journal of New Music Research</em> 45 (3): 265–80. <a href="https://doi.org/10.1080/09298215.2016.1197953" class="uri">https://doi.org/10.1080/09298215.2016.1197953</a>.</p>
</div>
<div id="ref-rainsfordMUSOSMUsicSOftware2018">
<p>Rainsford, M., M. A. Palmer, and G. Paine. 2018. “The MUSOS (MUsic SOftware System) Toolkit: A Computer-Based, Open Source Application for Testing Memory for Melodies.” <em>Behavior Research Methods</em> 50 (2): 684–702. <a href="https://doi.org/10.3758/s13428-017-0894-6" class="uri">https://doi.org/10.3758/s13428-017-0894-6</a>.</p>
</div>
<div id="ref-shannonMathematicalTheoryCommunication1948">
<p>Shannon, Claude. 1948. “A Mathematical Theory of Communication.” <em>Bell System Technical Journal</em> 27: 379–423.</p>
</div>
<div id="ref-sauvePredictionPolyphonyModelling2017">
<p>Sauve, Sarah. 2017. “Prediction in Polyphony: Modelling Auditory Scene Analysis.” PhD thesis, Centre for Digital Music: Queen Mary, University of London.</p>
</div>
<div id="ref-harrisonDissociatingSensoryCognitive2018">
<p>Harrison, Peter M . C., and Marcus Thomas Pearce. 2018. “Dissociating Sensory and Cognitive Theories of Harmony Perception Through Computational Modeling.” <a href="https://doi.org/10.31234/osf.io/wgjyv" class="uri">https://doi.org/10.31234/osf.io/wgjyv</a>.</p>
</div>
<div id="ref-perruchetImplicitLearningStatistical2006">
<p>Perruchet, Pierre, and Sebastien Pacton. 2006. “Implicit Learning and Statistical Learning: One Phenomenon, Two Approaches.” <em>Trends in Cognitive Sciences</em> 10 (5): 233–38. <a href="https://doi.org/10.1016/j.tics.2006.03.006" class="uri">https://doi.org/10.1016/j.tics.2006.03.006</a>.</p>
</div>
<div id="ref-jamiesonApplyingExemplarModel2009">
<p>Jamieson, Randall K., and D. J. K. Mewhort. 2009. “Applying an Exemplar Model to the Serial Reaction-Time Task: Anticipating from Experience.” <em>Quarterly Journal of Experimental Psychology</em> 62 (9): 1757–83. <a href="https://doi.org/10.1080/17470210802557637" class="uri">https://doi.org/10.1080/17470210802557637</a>.</p>
</div>
<div id="ref-longRelationshipsPitchMemory1977">
<p>Long, Peggy A. 1977. “Relationships Between Pitch Memory in Short Melodies and Selected Factors.” <em>Journal of Research in Music Education</em> 25 (4): 272–82. <a href="https://doi.org/10.2307/3345268" class="uri">https://doi.org/10.2307/3345268</a>.</p>
</div>
<div id="ref-norrisRelationshipSightSinging2003">
<p>Norris, Charles. 2003. “The Relationship Between Sight Singing Achievement and Melodic Dictation Achievement.” <em>Contributions to Music Education</em> 30 (1): 39–53.</p>
</div>
<div id="ref-pfordresherPoorPitchSingingAbsence2007">
<p>Pfordresher, Peter Q., and Steven Brown. 2007. “Poor-Pitch Singing in the Absence of &quot;Tone Deafness&quot;.” <em>Music Perception: An Interdisciplinary Journal</em> 25 (2): 95–115. <a href="https://doi.org/10.1525/mp.2007.25.2.95" class="uri">https://doi.org/10.1525/mp.2007.25.2.95</a>.</p>
</div>
<div id="ref-hofstetterComputerBaesedRecognitionPerceptual1981">
<p>Hofstetter, Fred T. 1981. “Computer-Baesed Recognition of Perceptual Patterns and Learning Styles in Rhythmic Dictation Exercises.” <em>Journal of Research in Music Education</em> 29 (4): 265–77. <a href="https://doi.org/10.2307/3345003" class="uri">https://doi.org/10.2307/3345003</a>.</p>
</div>
<div id="ref-pembrookInterferenceTranscriptionProcess1986">
<p>Pembrook, Randall G. 1986. “Interference of the Transcription Process and Other Selected Variables on Perception and Memory During Melodic Dictation.” <em>Journal of Research in Music Education</em> 34 (4): 238. <a href="https://doi.org/10.2307/3345259" class="uri">https://doi.org/10.2307/3345259</a>.</p>
</div>
<div id="ref-schellenbergEffectTonalRhythmicContext1985">
<p>Schellenberg, Stephen, and Randall S. Moore. 1985. “The Effect of Tonal-Rhythmic Context on Short-Term Memory of Rhythmic and Melodic Sequences.” <em>Bulletin of the Council for Research in Music Education</em>, no. 85: 207–17.</p>
</div>
<div id="ref-ouraConstructingRepresentationMelody1991a">
<p>Oura, Yoko. 1991. “Constructing a Representation of a Melody: Transforming Melodic Segments into Reduced Pitch Patterns Operated on by Modifiers.” <em>Music Perception: An Interdisciplinary Journal</em> 9 (2): 251–65. <a href="https://doi.org/10.2307/40285531" class="uri">https://doi.org/10.2307/40285531</a>.</p>
</div>
<div id="ref-paneyTeachingMelodicDictation2014">
<p>Paney, Andrew S., and Nathan O. Buonviri. 2014. “Teaching Melodic Dictation in Advanced Placement Music Theory.” <em>Journal of Research in Music Education</em> 61 (4): 396–414. <a href="https://doi.org/10.1177/0022429413508411" class="uri">https://doi.org/10.1177/0022429413508411</a>.</p>
</div>
<div id="ref-gillespieMelodicDictationScoring2001">
<p>Gillespie, Jeffrey L. 2001. “Melodic Dictation Scoring Methods: An Exploratory Study.” <em>Journal for Music Theory Pedagogy</em> 15.</p>
</div>
<div id="ref-pembrookSendHelpAural1990">
<p>Pembrook, Randall G., and H. Lee Riggins. 1990. “&quot;Send Help!&quot;: Aural Skills Instruction in U.S. Colleges and Universities.” <em>Journal of Music Theory Pedagogy</em> 4 (1): 230–41.</p>
</div>
<div id="ref-dooleyAbsolutePitchCorrelates2010">
<p>Dooley, Kevin, and Diana Deutsch. 2010. “Absolute Pitch Correlates with High Performance on Musical Dictation.” <em>The Journal of the Acoustical Society of America</em> 128 (2): 890–93. <a href="https://doi.org/10.1121/1.3458848" class="uri">https://doi.org/10.1121/1.3458848</a>.</p>
</div>
<div id="ref-buonviriEffectsMusicNotation2015">
<p>Buonviri, Nathan. 2015. “Effects of Music Notation Reinforcement on Aural Memory for Melodies.” <em>International Journal of Music Education</em> 33 (4): 442–50. <a href="https://doi.org/10.1177/0255761415582345" class="uri">https://doi.org/10.1177/0255761415582345</a>.</p>
</div>
<div id="ref-buonviriMelodicDictationInstruction2015">
<p>Buonviri, Nathan O, and Andrew S Paney. 2015. “Melodic Dictation Instruction.” <em>Journal of Research in Music Education</em> 62 (2): 224–37.</p>
</div>
<div id="ref-buonviriEffectsTwoListening2017">
<p>Buonviri, Nathan O. 2017. “Effects of Two Listening Strategies for Melodic Dictation.” <em>Journal of Research in Music Education</em> 65 (3): 347–59. <a href="https://doi.org/10.1177/0022429417728925" class="uri">https://doi.org/10.1177/0022429417728925</a>.</p>
</div>
<div id="ref-weissRapidCommunicationPianists2015">
<p>Weiss, Michael W., Patrícia Vanzella, E. Glenn Schellenberg, and Sandra E. Trehub. 2015. “Rapid Communication: Pianists Exhibit Enhanced Memory for Vocal Melodies but Not Piano Melodies.” <em>Quarterly Journal of Experimental Psychology</em> 68 (5): 866–77. <a href="https://doi.org/10.1080/17470218.2015.1020818" class="uri">https://doi.org/10.1080/17470218.2015.1020818</a>.</p>
</div>
<div id="ref-weissAbilityProcessMusical2019">
<p>Weiss, Michael W., and Isabelle Peretz. 2019. “Ability to Process Musical Pitch Is Unrelated to the Memory Advantage for Vocal Music.” <em>Brain and Cognition</em> 129 (February): 35–39. <a href="https://doi.org/10.1016/j.bandc.2018.11.011" class="uri">https://doi.org/10.1016/j.bandc.2018.11.011</a>.</p>
</div>
<div id="ref-schellenbergMemorySurfaceFeatures2014">
<p>Schellenberg, E. Glenn, Stephanie M. Stalinski, and Bradley M. Marks. 2014. “Memory for Surface Features of Unfamiliar Melodies: Independent Effects of Changes in Pitch and Tempo.” <em>Psychological Research</em> 78 (1): 84–95. <a href="https://doi.org/10.1007/s00426-013-0483-y" class="uri">https://doi.org/10.1007/s00426-013-0483-y</a>.</p>
</div>
<div id="ref-levitinWhatDoesIt2012">
<p>Levitin, Daniel J. 2012. “What Does It Mean to Be Musical?” <em>Neuron</em> 73 (4): 633–37. <a href="https://doi.org/10.1016/j.neuron.2012.01.017" class="uri">https://doi.org/10.1016/j.neuron.2012.01.017</a>.</p>
</div>
<div id="ref-peretzModularityMusicProcessing2003">
<p>Peretz, Isabelle, and Max Coltheart. 2003. “Modularity of Music Processing.” <em>Nature Neuroscience</em> 6 (7): 688–91. <a href="https://doi.org/10.1038/nn1083" class="uri">https://doi.org/10.1038/nn1083</a>.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="3">
<li id="fn3"><p>In his highly influential book <em>Aural Skills Acquisition: The Development of Listening, Reading, and Performing Skills in College-Level Musicians</em>, <span class="citation">Karpinski (<a href="#ref-karpinskiAuralSkillsAcquisition2000">2000</a>)</span> documents this sentiment in music pedagogy circles by highlighting poetic adages from Romantic composer Robert Schumann in the mid 19th century through 21st century music educator Charles Elliott in the opening of his book, thus providing concrete examples of the belief that improving one’s aural skills, or <em>ear</em>, is a highly sought after advanced skill.<a href="intro.html#fnref3" class="footnote-back">↩</a></p></li>
<li id="fn4"><p>And in his Figure 3.1 he does caption it as an <em>idealized</em> dictation process<a href="intro.html#fnref4" class="footnote-back">↩</a></p></li>
<li id="fn5"><p>Gould puts forward a complete, yet very charged reading of the early history of cognitive testing and his writings on the subject have been accused of falling prey to the same logic he rails against <span class="citation">(Warne et al. <a href="#ref-warneStephenJayGould2019">2019</a>)</span><a href="intro.html#fnref5" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="individual-differences.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/01-theoretical-bkgrd-rationale.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": ["mmd-draft-djb.pdf", "mmd-draft-djb.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
