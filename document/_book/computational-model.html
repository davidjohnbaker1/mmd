<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Chapter 7 Computational Model | MODELING MELODIC DICTATION</title>
  <meta name="description" content="This dissertation models both individual and musical features that contribute to processes involved in melodic dictation.">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Chapter 7 Computational Model | MODELING MELODIC DICTATION" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This dissertation models both individual and musical features that contribute to processes involved in melodic dictation." />
  <meta name="github-repo" content="davidjohnbaker1/document" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 7 Computational Model | MODELING MELODIC DICTATION" />
  
  <meta name="twitter:description" content="This dissertation models both individual and musical features that contribute to processes involved in melodic dictation." />
  

<meta name="author" content="A Dissertation">
<meta name="author" content="Submitted to the Graduate Faculty of the Louisiana State University and Agricultural and Mechanical College in partial fulfillment of the requirements for the degree of Doctor of Philosophy">
<meta name="author" content="in">
<meta name="author" content="The School of Music">
<meta name="author" content="by David John Baker">
<meta name="author" content="B.M., Baldwin Wallace University, 2012">
<meta name="author" content="MSc., Goldsmiths, University of London, 2015">
<meta name="author" content="May 2019">


<meta name="date" content="2019-04-15">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="experiment.html">
<link rel="next" href="references.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Modeling Melodic Dictation</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Significance of the Study</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#rationale"><i class="fa fa-check"></i><b>1.1</b> Rationale</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#chapter-overview"><i class="fa fa-check"></i><b>1.2</b> Chapter Overview</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Context, Literature, and Rationale</a><ul>
<li class="chapter" data-level="2.1" data-path="intro.html"><a href="intro.html#melodic-dictation"><i class="fa fa-check"></i><b>2.1</b> Melodic Dictation</a><ul>
<li class="chapter" data-level="2.1.1" data-path="intro.html"><a href="intro.html#describing-melodic-dictation"><i class="fa fa-check"></i><b>2.1.1</b> Describing Melodic Dictation</a></li>
<li class="chapter" data-level="2.1.2" data-path="intro.html"><a href="intro.html#taxonomizing"><i class="fa fa-check"></i><b>2.1.2</b> Taxonomizing</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="intro.html"><a href="intro.html#individual-factors"><i class="fa fa-check"></i><b>2.2</b> Individual Factors</a><ul>
<li class="chapter" data-level="2.2.1" data-path="intro.html"><a href="intro.html#cognitive"><i class="fa fa-check"></i><b>2.2.1</b> Cognitive</a></li>
<li class="chapter" data-level="2.2.2" data-path="intro.html"><a href="intro.html#environmental"><i class="fa fa-check"></i><b>2.2.2</b> Environmental</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="intro.html"><a href="intro.html#musical-factors"><i class="fa fa-check"></i><b>2.3</b> Musical Factors</a><ul>
<li class="chapter" data-level="2.3.1" data-path="intro.html"><a href="intro.html#the-notes"><i class="fa fa-check"></i><b>2.3.1</b> “The Notes”</a></li>
<li class="chapter" data-level="2.3.2" data-path="intro.html"><a href="intro.html#abstracted-features"><i class="fa fa-check"></i><b>2.3.2</b> Abstracted Features</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="intro.html"><a href="intro.html#polymorphism-of-ability"><i class="fa fa-check"></i><b>2.4</b> Polymorphism of Ability</a></li>
<li class="chapter" data-level="2.5" data-path="intro.html"><a href="intro.html#conclusions"><i class="fa fa-check"></i><b>2.5</b> Conclusions</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="individual-differences.html"><a href="individual-differences.html"><i class="fa fa-check"></i><b>3</b> Individual Differences</a><ul>
<li class="chapter" data-level="3.1" data-path="individual-differences.html"><a href="individual-differences.html#rationale-1"><i class="fa fa-check"></i><b>3.1</b> Rationale</a></li>
<li class="chapter" data-level="3.2" data-path="individual-differences.html"><a href="individual-differences.html#individual-differences-1"><i class="fa fa-check"></i><b>3.2</b> Individual Differences</a><ul>
<li class="chapter" data-level="3.2.1" data-path="individual-differences.html"><a href="individual-differences.html#improving-musical-memory"><i class="fa fa-check"></i><b>3.2.1</b> Improving Musical Memory</a></li>
<li class="chapter" data-level="3.2.2" data-path="individual-differences.html"><a href="individual-differences.html#memory-for-melodies"><i class="fa fa-check"></i><b>3.2.2</b> Memory for Melodies</a></li>
<li class="chapter" data-level="3.2.3" data-path="individual-differences.html"><a href="individual-differences.html#musicians-cognitive-advantage"><i class="fa fa-check"></i><b>3.2.3</b> Musician’s Cognitive Advantage</a></li>
<li class="chapter" data-level="3.2.4" data-path="individual-differences.html"><a href="individual-differences.html#relationship-established"><i class="fa fa-check"></i><b>3.2.4</b> Relationship Established</a></li>
<li class="chapter" data-level="3.2.5" data-path="individual-differences.html"><a href="individual-differences.html#dictation-without-dictation"><i class="fa fa-check"></i><b>3.2.5</b> Dictation Without Dictation</a></li>
<li class="chapter" data-level="3.2.6" data-path="individual-differences.html"><a href="individual-differences.html#cognitive-measures-of-interest"><i class="fa fa-check"></i><b>3.2.6</b> Cognitive Measures of Interest</a></li>
<li class="chapter" data-level="3.2.7" data-path="individual-differences.html"><a href="individual-differences.html#structural-equation-modeling"><i class="fa fa-check"></i><b>3.2.7</b> Structural Equation Modeling</a></li>
<li class="chapter" data-level="3.2.8" data-path="individual-differences.html"><a href="individual-differences.html#hypotheses"><i class="fa fa-check"></i><b>3.2.8</b> Hypotheses</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="individual-differences.html"><a href="individual-differences.html#overview-of-experiment"><i class="fa fa-check"></i><b>3.3</b> Overview of Experiment</a><ul>
<li class="chapter" data-level="3.3.1" data-path="individual-differences.html"><a href="individual-differences.html#participants"><i class="fa fa-check"></i><b>3.3.1</b> Participants</a></li>
<li class="chapter" data-level="3.3.2" data-path="individual-differences.html"><a href="individual-differences.html#materials"><i class="fa fa-check"></i><b>3.3.2</b> Materials</a></li>
<li class="chapter" data-level="3.3.3" data-path="individual-differences.html"><a href="individual-differences.html#procedure"><i class="fa fa-check"></i><b>3.3.3</b> Procedure</a></li>
<li class="chapter" data-level="3.3.4" data-path="individual-differences.html"><a href="individual-differences.html#results"><i class="fa fa-check"></i><b>3.3.4</b> Results</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="individual-differences.html"><a href="individual-differences.html#discussion"><i class="fa fa-check"></i><b>3.4</b> Discussion</a><ul>
<li class="chapter" data-level="3.4.1" data-path="individual-differences.html"><a href="individual-differences.html#model-fits"><i class="fa fa-check"></i><b>3.4.1</b> Model Fits</a></li>
<li class="chapter" data-level="3.4.2" data-path="individual-differences.html"><a href="individual-differences.html#relating-to-melodic-dictation"><i class="fa fa-check"></i><b>3.4.2</b> Relating to Melodic Dictation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="computation-chapter.html"><a href="computation-chapter.html"><i class="fa fa-check"></i><b>4</b> Computation Chapter</a><ul>
<li class="chapter" data-level="4.1" data-path="computation-chapter.html"><a href="computation-chapter.html#rationale-2"><i class="fa fa-check"></i><b>4.1</b> Rationale</a></li>
<li class="chapter" data-level="4.2" data-path="computation-chapter.html"><a href="computation-chapter.html#agreeing-on-complexity"><i class="fa fa-check"></i><b>4.2</b> Agreeing on Complexity</a><ul>
<li class="chapter" data-level="4.2.1" data-path="computation-chapter.html"><a href="computation-chapter.html#methods"><i class="fa fa-check"></i><b>4.2.1</b> Methods</a></li>
<li class="chapter" data-level="4.2.2" data-path="computation-chapter.html"><a href="computation-chapter.html#agreement-among-peagogues"><i class="fa fa-check"></i><b>4.2.2</b> Agreement Among Peagogues</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="computation-chapter.html"><a href="computation-chapter.html#modeling-complexity"><i class="fa fa-check"></i><b>4.3</b> Modeling Complexity</a><ul>
<li class="chapter" data-level="4.3.1" data-path="computation-chapter.html"><a href="computation-chapter.html#what-are-features"><i class="fa fa-check"></i><b>4.3.1</b> What Are Features?</a></li>
<li class="chapter" data-level="4.3.2" data-path="computation-chapter.html"><a href="computation-chapter.html#back-to-the-classroom"><i class="fa fa-check"></i><b>4.3.2</b> Back to the Classroom</a></li>
<li class="chapter" data-level="4.3.3" data-path="computation-chapter.html"><a href="computation-chapter.html#dynamic"><i class="fa fa-check"></i><b>4.3.3</b> Dynamic</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="computation-chapter.html"><a href="computation-chapter.html#frequency-facilitation-hypothesis"><i class="fa fa-check"></i><b>4.4</b> Frequency Facilitation Hypothesis</a><ul>
<li class="chapter" data-level="4.4.1" data-path="computation-chapter.html"><a href="computation-chapter.html#corpus-analysis"><i class="fa fa-check"></i><b>4.4.1</b> Corpus Analysis</a></li>
<li class="chapter" data-level="4.4.2" data-path="computation-chapter.html"><a href="computation-chapter.html#implications"><i class="fa fa-check"></i><b>4.4.2</b> Implications</a></li>
<li class="chapter" data-level="4.4.3" data-path="computation-chapter.html"><a href="computation-chapter.html#limitations-of-frequency-facilitation-hypothesis"><i class="fa fa-check"></i><b>4.4.3</b> Limitations of Frequency Facilitation Hypothesis</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="computation-chapter.html"><a href="computation-chapter.html#conclusions-1"><i class="fa fa-check"></i><b>4.5</b> Conclusions</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="chapterfour.html"><a href="chapterfour.html"><i class="fa fa-check"></i><b>5</b> Hello, Corpus</a><ul>
<li class="chapter" data-level="5.1" data-path="chapterfour.html"><a href="chapterfour.html#rationale-3"><i class="fa fa-check"></i><b>5.1</b> Rationale</a></li>
<li class="chapter" data-level="5.2" data-path="chapterfour.html"><a href="chapterfour.html#history"><i class="fa fa-check"></i><b>5.2</b> History</a></li>
<li class="chapter" data-level="5.3" data-path="chapterfour.html"><a href="chapterfour.html#melosol-corpus"><i class="fa fa-check"></i><b>5.3</b> MeloSol Corpus</a></li>
<li class="chapter" data-level="5.4" data-path="chapterfour.html"><a href="chapterfour.html#comparison-of-corpora"><i class="fa fa-check"></i><b>5.4</b> Comparison of Corpora</a><ul>
<li class="chapter" data-level="5.4.1" data-path="chapterfour.html"><a href="chapterfour.html#corpus-analysis-1"><i class="fa fa-check"></i><b>5.4.1</b> Corpus Analysis</a></li>
<li class="chapter" data-level="5.4.2" data-path="chapterfour.html"><a href="chapterfour.html#discussion-1"><i class="fa fa-check"></i><b>5.4.2</b> Discussion</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="experiment.html"><a href="experiment.html"><i class="fa fa-check"></i><b>6</b> Experiment</a><ul>
<li class="chapter" data-level="6.1" data-path="experiment.html"><a href="experiment.html#rationale-4"><i class="fa fa-check"></i><b>6.1</b> Rationale</a></li>
<li class="chapter" data-level="6.2" data-path="experiment.html"><a href="experiment.html#introduction"><i class="fa fa-check"></i><b>6.2</b> Introduction</a><ul>
<li class="chapter" data-level="6.2.1" data-path="experiment.html"><a href="experiment.html#memory-for-melodies-1"><i class="fa fa-check"></i><b>6.2.1</b> Memory for Melodies</a></li>
<li class="chapter" data-level="6.2.2" data-path="experiment.html"><a href="experiment.html#musical-factors-1"><i class="fa fa-check"></i><b>6.2.2</b> Musical Factors</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="experiment.html"><a href="experiment.html#methods-1"><i class="fa fa-check"></i><b>6.3</b> Methods</a><ul>
<li class="chapter" data-level="6.3.1" data-path="experiment.html"><a href="experiment.html#participants-1"><i class="fa fa-check"></i><b>6.3.1</b> Participants</a></li>
<li class="chapter" data-level="6.3.2" data-path="experiment.html"><a href="experiment.html#materials-1"><i class="fa fa-check"></i><b>6.3.2</b> Materials</a></li>
<li class="chapter" data-level="6.3.3" data-path="experiment.html"><a href="experiment.html#procedure-1"><i class="fa fa-check"></i><b>6.3.3</b> Procedure</a></li>
<li class="chapter" data-level="6.3.4" data-path="experiment.html"><a href="experiment.html#scoring-melodies"><i class="fa fa-check"></i><b>6.3.4</b> Scoring Melodies</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="experiment.html"><a href="experiment.html#results-1"><i class="fa fa-check"></i><b>6.4</b> Results</a><ul>
<li class="chapter" data-level="6.4.1" data-path="experiment.html"><a href="experiment.html#data-screening"><i class="fa fa-check"></i><b>6.4.1</b> Data Screening</a></li>
<li class="chapter" data-level="6.4.2" data-path="experiment.html"><a href="experiment.html#modeling-1"><i class="fa fa-check"></i><b>6.4.2</b> Modeling</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="experiment.html"><a href="experiment.html#discussion-2"><i class="fa fa-check"></i><b>6.5</b> Discussion</a></li>
<li class="chapter" data-level="6.6" data-path="experiment.html"><a href="experiment.html#conclusions-2"><i class="fa fa-check"></i><b>6.6</b> Conclusions</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="computational-model.html"><a href="computational-model.html"><i class="fa fa-check"></i><b>7</b> Computational Model</a><ul>
<li class="chapter" data-level="7.1" data-path="computational-model.html"><a href="computational-model.html#levels-of-abstraction"><i class="fa fa-check"></i><b>7.1</b> Levels of Abstraction</a></li>
<li class="chapter" data-level="7.2" data-path="computational-model.html"><a href="computational-model.html#model-overview"><i class="fa fa-check"></i><b>7.2</b> Model Overview</a></li>
<li class="chapter" data-level="7.3" data-path="computational-model.html"><a href="computational-model.html#verbal-model"><i class="fa fa-check"></i><b>7.3</b> Verbal Model</a><ul>
<li class="chapter" data-level="7.3.1" data-path="computational-model.html"><a href="computational-model.html#model-representational-assumptions"><i class="fa fa-check"></i><b>7.3.1</b> Model Representational Assumptions</a></li>
<li class="chapter" data-level="7.3.2" data-path="computational-model.html"><a href="computational-model.html#contents-of-the-prior-knowledge"><i class="fa fa-check"></i><b>7.3.2</b> Contents of the Prior Knowledge</a></li>
<li class="chapter" data-level="7.3.3" data-path="computational-model.html"><a href="computational-model.html#modeling-information-content"><i class="fa fa-check"></i><b>7.3.3</b> Modeling Information Content</a></li>
<li class="chapter" data-level="7.3.4" data-path="computational-model.html"><a href="computational-model.html#setting-limits-with-transcribe"><i class="fa fa-check"></i><b>7.3.4</b> Setting Limits with Transcribe</a></li>
<li class="chapter" data-level="7.3.5" data-path="computational-model.html"><a href="computational-model.html#pattern-matching"><i class="fa fa-check"></i><b>7.3.5</b> Pattern Matching</a></li>
<li class="chapter" data-level="7.3.6" data-path="computational-model.html"><a href="computational-model.html#dictation-re-entry"><i class="fa fa-check"></i><b>7.3.6</b> Dictation Re-Entry</a></li>
<li class="chapter" data-level="7.3.7" data-path="computational-model.html"><a href="computational-model.html#completion"><i class="fa fa-check"></i><b>7.3.7</b> Completion</a></li>
<li class="chapter" data-level="7.3.8" data-path="computational-model.html"><a href="computational-model.html#model-output"><i class="fa fa-check"></i><b>7.3.8</b> Model Output</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="computational-model.html"><a href="computational-model.html#formal-model"><i class="fa fa-check"></i><b>7.4</b> Formal Model</a><ul>
<li class="chapter" data-level="7.4.1" data-path="computational-model.html"><a href="computational-model.html#computational-model-1"><i class="fa fa-check"></i><b>7.4.1</b> Computational Model</a></li>
<li class="chapter" data-level="7.4.2" data-path="computational-model.html"><a href="computational-model.html#example"><i class="fa fa-check"></i><b>7.4.2</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="computational-model.html"><a href="computational-model.html#conclusions-3"><i class="fa fa-check"></i><b>7.5</b> Conclusions</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/davidjohnbaker1/document" target="blank">Modeling Melodic Dictation</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">MODELING MELODIC DICTATION</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="computational-model" class="section level1">
<h1><span class="header-section-number">Chapter 7</span> Computational Model</h1>
<div id="levels-of-abstraction" class="section level2">
<h2><span class="header-section-number">7.1</span> Levels of Abstraction</h2>
<p>In his 2007 article <em>Models of Music Similarity</em>, Geraint Wiggins distinguishes between <em>descriptive</em> and <em>explanatory</em> models in describing the modeling of human behavior <span class="citation">(Wiggins <a href="#ref-wigginsModelsMusicalSimilarity2007">2007</a>)</span>.
Descriptive models assert what will happen in response to an event.
For example, as discussed in the previous chapter, as the note density of a melody increases and the tonalness of a melody decreases, a melody becomes more difficult to dictate.
While the increase in note density is assumed to drive the decrease in dictation scores, merely stating that there is an established relationship between one variable and the other says nothing about the inner workings of this process.
An explanatory model on the other hand, not only describes what will happen, but additionally notes why and how this process occurs.
For example, much of the work from musical expectation demonstrates that as an individual’s exposure to a musical style increases, so does their ability to predict specific events within a given musical texture <span class="citation">(Pearce <a href="#ref-pearceStatisticalLearningProbabilistic2018a">2018</a>)</span>.</p>
<p>Not only does more exposure predict more accurate responses, but many of these models of musical expectation derive their underlying predictive power from the brain’s ability to implicitly track statistical regularities in an auditory scene <span class="citation">(Saffran et al. <a href="#ref-saffranStatisticalLearningTone1999">1999</a>; Margulis <a href="#ref-margulisRepeatHowMusic2014">2014</a>)</span>.
The <em>how</em> derives from the tracking of statistical regularities in musical information and the <em>why</em> derives from evolutionary demands; Organisms that are able to make more accurate predictions about their environment are more likely to survive and pass on their genes <span class="citation">(Huron <a href="#ref-huronSweetAnticipation2006">2006</a>)</span>.</p>
<p>Wiggins writes that although there can be both explanatory and descriptive theories, depending on the level of abstraction, a theory may be explanatory at one level, yet descriptive at another.
Using the mind-brain dichotomy, he asserts that the example of a theory of musical expectation could be explanatory at the level of behavior as noted above, but says nothing about what is happening at the neural level.
Both descriptive and explanatory theories are needed: descriptive theories are used to test explanatory theories.
By stringing together different layers of abstraction, we can arrive at a better understanding of how the world works.</p>
<p>Returning to melodic dictation, under Wiggins’ framework the Karpinski model of melodic dictation <span class="citation">(Karpinski <a href="#ref-karpinskiAuralSkillsAcquisition2000">2000</a>, <a href="#ref-karpinskiModelMusicPerception1990">1990</a>)</span> qualifies as a descriptive model.
The model explains what happens over the time course of a melodic dictation– specifying four discrete stages discussed in earlier chapters– but does not explicitly state <em>how</em> or <em>why</em> this process happens.
In order to have a more complete understanding of melodic dictation, an explanatory model is needed.</p>
<p>In this chapter, I introduce an explanatory model of melodic dictation.
The model is inspired by work from both computational musicology and cognitive psychology.
From computational musicology, I draw on the work of Marcus Pearce’s IDyOM <span class="citation">(Pearce <a href="#ref-pearceConstructionEvaluationStatistical2005">2005</a>)</span> and from cognitive psychology, I draw on Nelson Cowan’s Embedded Process model of working memory <span class="citation">(Cowan <a href="#ref-cowanEvolvingConceptionsMemory1988">1988</a>, <a href="#ref-cowanMagicalMysteryFour2010">2010</a>)</span> to explain the perceptual components.
In addition to quantifying each step, the model incorporates flexible parameters that can be adjusted in order to accommodate individual differences, while still relying on a domain general process.
By relying on cognitive mechanisms based in statistical learning rather than a rule based system for music analysis <span class="citation">(Lerdahl and Jackendoff <a href="#ref-lerdahlGenerativeTheoryTonal1986">1986</a>; Narmour <a href="#ref-narmourAnalysisCognitionBasic1990">1990</a>, <a href="#ref-narmourAnalysisCognitionMelodic1992">1992</a>; Temperley <a href="#ref-temperleyCognitionBasicMusical2004">2004</a>)</span>, this model allows for the heterogeneity of musical experience among a diversity of music listeners.</p>
</div>
<div id="model-overview" class="section level2">
<h2><span class="header-section-number">7.2</span> Model Overview</h2>
<p>The model consists of three main modules, each with its own set of parameters:</p>
<ol style="list-style-type: decimal">
<li>Prior Knowledge</li>
<li>Selective Attention</li>
<li>Transcription and Re-entry</li>
</ol>
<p>The Prior Knowledge module reflects the previous knowledge an individual brings to the melodic dictation.
The Selective Attention module– somewhat akin to Karpinski’s extractive listening <span class="citation">(Karpinski <a href="#ref-karpinskiAuralSkillsAcquisition2000">2000</a>, <a href="#ref-karpinskiModelMusicPerception1990">1990</a>)</span>– segments incoming musical information by using the window of attention as conceptualized as the limits of working memory capacity as a sensory bottleneck to constrict the size of musical chunk that an individual could transcribe.
Once musical material is in the focus of attention, the Transcription function pattern matches against the Prior Knowledge’s corpus of information in order to find a match of explicitly known musical information.
The Transcription function will recursively truncate what musical information is in Selective Attention if no match is found.
In addition to Transcription, there is also a Re-entry function that will restart the entire loop.
This process reflects, but is not intended to mirror the cognitive process used in melodic dictation.
Rather it attempts to be phenomenologically similar to the decision making process used when attempting to notate novel melodies.
Based on both the prior knowledge and individual differences of the individual, the model will scale in ability, with the general retrieval mechanisms in place.
The exact details of the assumptions, parameters, and complete formula of the model are discussed below.</p>
</div>
<div id="verbal-model" class="section level2">
<h2><span class="header-section-number">7.3</span> Verbal Model</h2>
<p>Below I describe my model’s assumptions, parameters, as well as the steps taken when the model is run.
After detailing the inner workings of each of the assumptions and the modules, described in roughly the order that they occur, I present the model using pseudocode with the terminology described below.
I discuss the issues of assumptions and representations as they arise in describing the model.</p>
<div id="model-representational-assumptions" class="section level3">
<h3><span class="header-section-number">7.3.1</span> Model Representational Assumptions</h3>
<p>In order to write a computer program that mirrors the melodic dictation process, how the mind perceives and represents musical information must be defined <em>a priori</em>.
Before delving into questions of representation, this model assumes that the musical surface as represented by the notes via Western musical notation are salient and can be perceived as distinct perceptual phenomena
Although there is work that suggests that different cultures and levels of experience might not categorize musical information universally <span class="citation">(McDermott et al. <a href="#ref-mcdermottIndifferenceDissonanceNative2016">2016</a>)</span>, other work suggests that experiencing pitches as discrete, categorical phenomena is categorized as a statistical human universal <span class="citation">(Savage et al. <a href="#ref-savageStatisticalUniversalsReveal2015">2015</a>)</span>.
In either case, the most explanatory models of any account of music perception should incorporate many features <span class="citation">(Harrison and Pearce <a href="#ref-harrisonInstantaneousConsonancePerception">n.d.</a>)</span>.
For the purposes of this model, I assume that individuals do in fact perceive the musical surface similarly to the written score.</p>
<p>Knowing that it is melodic information or melodic data that needs to be represented, the question then becomes what is the best way in which to represent it.
This issue becomes increasingly complex when considering literature suggesting that the human mind represents musical information in a variety of different forms <span class="citation">(Krumhansl <a href="#ref-krumhanslCognitiveFoundationsMusical2001">2001</a>; Levitin and Tirovolas <a href="#ref-levitinCurrentAdvancesCognitive2009">2009</a>)</span>.
For the purposes of this model and further examples, I choose to represent musical information using both the pitch (note and scale degree) and timing (rhythm and inter-onset-interval) representation described in <span class="citation">Pearce (<a href="#ref-pearceStatisticalLearningProbabilistic2018a">2018</a>)</span>.
Using these two parameters reflects only a subset of the possibilities that could be modeled when using a multiple viewpoint system <span class="citation">(Conklin and Witten <a href="#ref-conklinMultipleViewpointSystems1995">1995</a>)</span>.
Future research comparing this model’s output using different representations will also contribute to conversations regarding pedagogy.
If one form of representation mirrors human behavior better than others, it would provide evidence in support of the pedagogy of one system over another.
How the model represents musical information is the first important parameter value that needs to be chosen before running the model and this establishes the Prior Knowledge.</p>
</div>
<div id="contents-of-the-prior-knowledge" class="section level3">
<h3><span class="header-section-number">7.3.2</span> Contents of the Prior Knowledge</h3>
<p>The Prior Knowledge consists of a corpus of digitally represented melodies taken to reflect the implicitly understood structural patterns in a musical style that the listener has been exposed to.
The logic of representing an individual’s prior knowledge follows the assumptions of both the Statistical Learning Hypothesis (SLH) and the Probabilistic Prediction Hypothesis (PPH), both core theoretical assumptions of the Information Dynamic of Music (IDyOM) model of Marcus Pearce <span class="citation">(Pearce <a href="#ref-pearceConstructionEvaluationStatistical2005">2005</a>, <a href="#ref-pearceStatisticalLearningProbabilistic2018a">2018</a>)</span>.
Using a corpus of melodies to represent an individual’s Prior Knowledge relies on the Statistical Learning Hypothesis which states:</p>
<blockquote>
<p>musical enculturation is a process of implicit statistical learning in which listeners progressively acquire internal models of the statistical and structural regularities present in the musical styles to which they are exposed, over short (e.g., an individual piece of music) and long time scales (e.g., an entire lifetime of listening). p.2 (Pearce, 2018)</p>
</blockquote>
<p>The logic here is that the more an individual is exposed musical material, the more they will implicitly understand it, which leads the corroborating probabilistic prediction hypothesis which states:</p>
<blockquote>
<p>while listening to new music, an enculturated listener applies models learned via the SLH to generate probabilistic predictions that enable them to organize and process their mental representations of the music and generate culturally appropriate responses. p.2 (Pearce, 2018).</p>
</blockquote>
<p>Taken together and then quantified using Shannon information content <span class="citation">(Shannon <a href="#ref-shannonMathematicalTheoryCommunication1948">1948</a>)</span>, it then becomes possible using the IDyOM framework to have a quantifiable measure that reliably predicts the amount of perceived unexpectedness in a musical melody that can change pending on the musical corpus that the model is trained on.
As a model, IDyOM has been successful mirroring human behavior in melodies in various styles <span class="citation">(Pearce <a href="#ref-pearceStatisticalLearningProbabilistic2018a">2018</a>)</span>, harmony– outperforming <span class="citation">(Harrison and Pearce <a href="#ref-harrisonDissociatingSensoryCognitive2018">2018</a>)</span> sensory models of harmony <span class="citation">(Bigand et al. <a href="#ref-bigandEmpiricalEvidenceMusical2014">2014</a>)</span>–, and is also being developed to handle polyphonic materials <span class="citation">(Sauve <a href="#ref-sauvePredictionPolyphonyModelling2017">2017</a>)</span>.</p>
<p>Stepping beyond the assumptions of IDyOM, the Prior Knowledge also needs to have a implicit/explicitly known parameter which indicates whether or not a pattern of music– or n-gram pattern– is explicitly learned.
This threshold can be set relative to the entire distribution of all n-grams in the corpus.</p>
</div>
<div id="modeling-information-content" class="section level3">
<h3><span class="header-section-number">7.3.3</span> Modeling Information Content</h3>
<p>Having established that the models’ first parameters to be decided are the representation of strings and the implicit/explicit threshold, the next decision that has to be made is how the model decides segmentation for the second stage of Selective Attention.
Although there has been a large amount of work on different ways to segment the musical surface using rule based methods <span class="citation">(Lerdahl and Jackendoff <a href="#ref-lerdahlGenerativeTheoryTonal1986">1986</a>; Margulis <a href="#ref-margulisModelMelodicExpectation2005">2005</a>; Narmour <a href="#ref-narmourAnalysisCognitionBasic1990">1990</a>, <a href="#ref-narmourAnalysisCognitionMelodic1992">1992</a>)</span>, which rely on matching a music theorist’s intuition with a set of descriptive rules somewhat like the boundary formation rules put forward in <em>A Generative Theory of Tonal Music</em>, as noted by Pearce <span class="citation">(Pearce <a href="#ref-pearceStatisticalLearningProbabilistic2018a">2018</a>)</span>, rule based models often fail when applied to music outside the Western classical tradition.
Additionally, since melodic dictation is an active memory process, rather than a semi-passive process of listening, this model needs to be able to quantify musical information on two conditions.
The first is that it must be dependent on prior musical experience.
The second is that it should allow for a movable boundary for selective attention so that musical information that is in memory can be actively maintained while carrying out another cognitive process, that of notating the melody.</p>
<p>In order to create this metric, I rely on IDyOM’s use of information content <span class="citation">(Shannon <a href="#ref-shannonMathematicalTheoryCommunication1948">1948</a>)</span>, which quantifies the information content of melodies based on a corpus of materials utilizing a multiple viewpoints framework <span class="citation">(Conklin and Witten <a href="#ref-conklinMultipleViewpointSystems1995">1995</a>)</span>.</p>
<p>For example, when trained against a corpus of melodies, this excerpt in Figure <a href="computational-model.html#fig:schubertF">7.1</a> from the fourth movement of Schubert’s <em>Octet in F Major</em> (D.803) lists the information content of the excerpt calculated for each note atop the notation<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a>.
Appearing in Figure <a href="computational-model.html#fig:cumSchubert">7.2</a>, I plot the cumulative information content of the melody, along with both an arbitrary threshold for the limits of working memory capacity, and where the subsequent segmentation boundary for musical material to be put in the Selective Attention buffer would be.
These values chosen show a small example of how the Selective Attention module works.
The advantage of operationalizing how an individual hears a melody like this is that melodies with lower information content, derived from an understanding of having more predictable patterns from the corpus, will allow for larger chunks to be put inside of the selective attention buffer.
Additionally, individuals with higher working memory capacity would be able to take in more musical information.</p>
<div class="figure" style="text-align: center"><span id="fig:schubertF"></span>
<img src="img/SchubertF.png" alt="Cadential Excerpt from Schubert's Octet in F Major" width="100%" />
<p class="caption">
Figure 7.1: Cadential Excerpt from Schubert’s Octet in F Major
</p>
</div>
<div class="figure" style="text-align: center"><span id="fig:cumSchubert"></span>
<img src="img/SchubertPlotNew.png" alt="Cumulative Information in Schubert Octet Excerpt" width="100%" />
<p class="caption">
Figure 7.2: Cumulative Information in Schubert Octet Excerpt
</p>
</div>
<p>It is important to highlight that the notes above the melody here are dependent on what is current in the Prior Knowledge module.
A corpus of Prior Knowledge with less musical events would lead to higher information content measures for each set of notes, while a Prior Knowledge that has extensive tracking of the patterns would lead to lower information content <span class="citation">(Conklin and Witten <a href="#ref-conklinMultipleViewpointSystems1995">1995</a>)</span>.
This increase in predictive accuracy mathematically reflects the intuition that those with more listening experience can process greater chunks of musical information.
I visualize what setting the explicit/implicit threshold on a set of various n-grams would look like in Figure <a href="computational-model.html#fig:thresholds">7.3</a>.
While I have arbitrarily set the thresholds here for illustrative purposes, in practice this could be done a number of different ways.
For example, these thresholds could be set based on frequency counts in the corpus, relative distributions, or the parameters might be computationally derived.</p>
<div class="figure" style="text-align: center"><span id="fig:thresholds"></span>
<img src="img/pk_grampanel.png" alt="Setting Prior Knowledge Limits" width="100%" />
<p class="caption">
Figure 7.3: Setting Prior Knowledge Limits
</p>
</div>
</div>
<div id="setting-limits-with-transcribe" class="section level3">
<h3><span class="header-section-number">7.3.4</span> Setting Limits with Transcribe</h3>
<p>With each note then quantified with a measure of information content, it then becomes possible to set a limit on the maximum amount of information that the individual would be able to hold in memory as defined by the Selective Attention module.
A higher threshold would allow for more musical material to be put in the attentional buffer, and a lower threshold would restrict the amount of information held in an attentional buffer.
By putting a threshold on this value, this serves as something akin to a perceptual bottleneck based on the assumption that there is a capacity limit to that of working memory <span class="citation">(Cowan <a href="#ref-cowanEvolvingConceptionsMemory1988">1988</a>, <a href="#ref-cowanMagicalMysteryFour2010">2010</a>)</span>.
Modulating this boundary will help provide insights into the degree to which melodic material can be retained between high and low working memory span individuals.</p>
<p>In practice, notes would enter the attentional buffer until the information content from the melody is equal to the memory threshold.
At this point, the notes that are in the attentional buffer are segmented and will be actively maintained in the Selective Attention buffer.
In theory, the maximum of the attentional buffer should not be reached since the individual performing the dictation would still need mental resources and attention to actively manipulate the information in the attentional buffer for the process of notating.</p>
</div>
<div id="pattern-matching" class="section level3">
<h3><span class="header-section-number">7.3.5</span> Pattern Matching</h3>
<p>The subset of notes of the melody represented in the attentional buffer, whether or not the melody becomes notated depends on whether or not the melody or string in the buffer can be matched with a string that is explicitly known in the corpus.
Mirroring a search pattern akin to Cowan’s Embedded Process model <span class="citation">(Cowan <a href="#ref-cowanEvolvingConceptionsMemory1988">1988</a>, <a href="#ref-cowanMagicalMysteryFour2010">2010</a>)</span>, the individual would search across their long term memory, or Prior Knowledge, for anything close to or resembling the pattern in the Selective Attention buffer.
Cowan’s model differs from other more modular based models of working memory like those of <span class="citation">Baddeley and Hitch (<a href="#ref-baddeleyWorkingMemory1974">1974</a>)</span> by positing that working memory should be conceptualized as a small window of conscious attention.
As an individual directs their attention to concepts represented in their long term memory, they can only spotlight a finite amount of information where categorical information regarding what is in the window of attention is not far from retrieval.
Using this logic, longer pattern strings or n-grams would be less likely to be recalled exactly since they occur less frequently in the Prior Knowledge.</p>
<p>If a pattern match that has been moved to Selective Attention is immediately found, the contents of Selective Attention would be considered to be notated.
The model would register that a loop had taken place and document the n-gram match.
Of course, finding an immediate pattern match each time is highly unlikely, and the model needs to be able to compensate if that happens.</p>
<p>If a pattern is not found in the initial search that is explicitly known, one token of the n-gram would be dropped off the string and the search would happen again.
This recursive search would happen until an explicit long term memory match is made.
Like humans taking melodic dictation, the computer would succeed more often finding patterns that fall within the largest density of a corpus of intervals distribution.
Additionally, like students performing a dictation, if a student does not explicitly know an interval, or a 2-gram, the dictation would not be able to be completed.
If this happens, both the model and student would have to move on to the next segment via the Re-entry function.</p>
<p>Eventually there would be a successful explicit match of a string in the Transcription module and that section of the melody would be considered to be dictated.
The model here would register that one iteration of the function has been run and the chunk transcribed would then be recorded.
After recording this history, the process would happen again starting at either the next note from where the model left off, the note in the entire string with the lowest information content, another pre-determined setting.
This parameter is defined before the model is run and the question of dictation re-entry certainly warrants further research and investigation.</p>
<p>This type of pattern search is also dependent on the way that the Prior Knowledge is represented.
In the example here, both pitch and rhythmic information are represented in the string that holds the contents of Selective Attention.
Since there is probably a very low likelihood of finding an exact match for every n-gram with both pitch and rhythm, this pattern search can happen again with both rhythms and pitch information queried separately.
If not found, exact pitch-temporal matches are found and the search is run again on either the pitch or rhythmic information separately.
This would be computationally akin to Karpinski’s proto-notation that he suggests students use in learning how to take melodic dictation <span class="citation">(Karpinski <a href="#ref-karpinskiAuralSkillsAcquisition2000">2000</a>, 88)</span>.
This feature of the model would predict that more efficient dictations would happen when pitch and interval information is dictated simultaneously.
Running the model prioritizing the secondary search with either pitch or rhythmic information will provide new insights into practical applications of dictation strategies.
Using this separate search feature as an option of the model seems to match with the intuitions strategies that a student dictating a melody might use.</p>
</div>
<div id="dictation-re-entry" class="section level3">
<h3><span class="header-section-number">7.3.6</span> Dictation Re-Entry</h3>
<p>Upon the successful pattern match of a string, the Selective Attention and Transcription module would need to then be run again.
This process is done via the Re-entry function.
As noted above, Re-entry in the melody could be a highly subjective point of discussion.
The model could either re-enter at the last note where the function successfully left off, the note in the melody with the lowest information content, the n-gram most salient in the corpus, or theoretically any other way that could be computationally implemented.
Entering at the last note not transcribed is logical from a computational standpoint, but this linear approach seems to be at odds with anecdotal experience.
Entering at the note with the lowest information content seems to provide an intuitive point of re-entry in that it would then be easier to transcribe.
Entering at the most represented n-gram seems to align with intuition in that people would want to tackle the easier tasks first, but this rests on the assumption that humans are able to reliably detect the sections of a melody that are easiest to transcribe based on implicitly learned statistical patterns.
For example, some people might instead choose to go to the end of a melody after successful transcription of the start of the melody.
This might be because this part of the melody is most active in memory due to a recency effect, or it could be that that cadential gestures are more common in being represented in the Prior Knowledge.</p>
</div>
<div id="completion" class="section level3">
<h3><span class="header-section-number">7.3.7</span> Completion</h3>
<p>Given the recursive nature of this process, if all 1-grams are explicitly represented in the Prior Knowledge then the target melody will be transcribed.
If only represented using such a small chunk, the model will have to loop over the melody many times, thus indicating that the transcriber had a high degree of difficulty dictating the melody.
If there is a gap in explicit knowledge in the prior knowledge, only patches of the melody will be recorded and the melody will not be recorded in its entirety.
An easier transcription will result in less iterations of the model with larger chunks.
Though the current instantiation of the model does not incorporate how multiple hearings might change how a melody is dictated, one could constrain the process to only allow a certain number of iterations to reflect this.
Of course as a new melody is learned, it is slowly being introduced into long term memory and could be completely capable of being represented in long term memory without being explicitly notated at the end of a dictation with time running out and thus not possible to be completed.
This would be imposing some sort of experimental constraint on the process and since this is meant to be a cognitive computational model of melodic dictation this caveat would complicate the model.
Future research could be done to optimize the choices that the model makes in order to satisfy whatever constraints are imposed and could be an interesting avenue of future research, but is beyond the initial goals of the model.</p>
</div>
<div id="model-output" class="section level3">
<h3><span class="header-section-number">7.3.8</span> Model Output</h3>
<p>The model then outputs each n-gram transcribed and can be counted as a series with less attempts mapping to an easier transcription.
This aligns with intuitions about the process of melodic dictation.
It first creates a linear mapping of attempts to dictate with difficulty of the melody.
It relies on a distinction between explicit and implicit statistical knowledge.
It is based on the Embedded Process Model from working memory and attention, so is part of a larger generative model, giving more credibility to explaining the underlying processes of melodic dictation.
The output of this model is inspired by work on mental rotation <span class="citation">(Shepard and Metzler <a href="#ref-shepardMentalRotationThreeDimensional1971">1971</a>)</span>, which assumes that time to complete a task is directly proportional to cognitive effort required.
More iterations of the model as reflected in more searches should reflect more mental effort and vice versa.</p>
</div>
</div>
<div id="formal-model" class="section level2">
<h2><span class="header-section-number">7.4</span> Formal Model</h2>
<p>I present the computational model in pseudocode as described in Figure <a href="computational-model.html#fig:mymodel">7.4</a>.
First, listed are the defined inputs, the functions needed to run the algorithm, and then the sequence the model runs.
To aid distinguishing between functions and objects, I put functions in italics and objects in bold.
Below the model in Figure <a href="computational-model.html#fig:walkthru">7.5</a> I provide a brief walk through of one iteration of the model.</p>
<div id="computational-model-1" class="section level3">
<h3><span class="header-section-number">7.4.1</span> Computational Model</h3>
<div class="figure" style="text-align: center"><span id="fig:mymodel"></span>
<img src="img/Model.png" alt="Formal Model" width="100%" />
<p class="caption">
Figure 7.4: Formal Model
</p>
</div>
</div>
<div id="example" class="section level3">
<h3><span class="header-section-number">7.4.2</span> Example</h3>
<div class="figure" style="text-align: center"><span id="fig:walkthru"></span>
<img src="img/Model%20Graphic.png" alt="Model Example" width="100%" />
<p class="caption">
Figure 7.5: Model Example
</p>
</div>
<p>Figure <a href="#fig:walktru"><strong>??</strong></a> depicts one iteration of the model run using the musical example from above using a hypothetical corpus for the pattern matching.
Using the model above, the following inputs were defined <em>a priori</em>:</p>
<ul>
<li>The <strong>Prior Knowledge</strong> is a hypothetical corpus of symbolic strings representing all n-grams of melodies</li>
<li>The <strong>Threshold</strong> is set to <strong>five</strong> exact matches in the <strong>Prior Knowledge</strong></li>
<li>The <strong>WMC</strong> is set at 17</li>
<li>The <strong>Target Melody</strong> is the Schubert excerpt from above</li>
<li>The <strong>String Position</strong> object is used to track the position in the dictation</li>
<li>The <strong>Difficulty</strong> object starts at 0</li>
<li>The <strong>Dictation</strong> object is <code>NULL</code> to begin, and each new n-gram successfully transcribed is annexed to it</li>
</ul>
<p>Figure <a href="computational-model.html#fig:walkthru">7.5</a> progresses from left to right over the course of time.
The algorithm begins by first running the <code>listen()</code> function on the <strong>Target Melody</strong>.
First, the model checks that there are notes to transcribe; this being the first loop of the model, this is statement will be <code>FALSE</code> so the next step is taken.
Notes of the <strong>Target Melody</strong> are read in to the <strong>Selective Attention</strong> buffer until the information content of the melody exceeds that of the working memory threshold.
This is depicted graphically in the leftmost panel of Figure <a href="computational-model.html#fig:walkthru">7.5</a>.
Each note unfolding over time fills up the <strong>Selective Attention</strong> working memory buffer.
When the amount of information reaches the perceptual bottleneck– as indicated by the dashed line– the <strong>Selective Attention</strong> buffer stops receiving information.
At this point the model will mark where in the melody it stopped taking in new information for later.
Here the contents in <strong>Selective Attention</strong> are moved to the <code>transcribe()</code> function.</p>
<p>With the contents of <strong>Selective Attention</strong> passed to <code>transcribe()</code>, the model adds one to the counter indicating the first search is about to run.
Moving to the middle panel of Figure <a href="computational-model.html#fig:walkthru">7.5</a>, the symbol string of notes in the first column is indexed against the <strong>Prior Knowledge</strong>.
Only if a five note pattern has appeared more than or equal to five times, as determined by the <strong>Threshold</strong> input, will the corresponding <code>EXPLICIT</code> column be <code>TRUE</code>.
In this case, this pattern has occurred over the threshold of 5 and thus a successful match is found.</p>
<p>It is at this step that the search resembles that of Cowan’s model of working memory as active attention.
The pattern being searched for is compared against a vast amount of information, with cues from the contents of what is in <strong>Selective Attention</strong> grouping similar patterns together.
At the neural level, this is most likely a much more complex process, but to show this grouping I note that this search is at least organized by the first pitch.
I assume it would be reasonable that patterns starting on G as <span class="math inline">\(\hat{5}\)</span> might happen together.
Since this string does have a <code>TRUE</code> match with <code>EXPLICIT</code>, the contents of <strong>Selective Attention</strong> are considered notated.
At this point the model would record the 5-gram, along with the string that it was matched with.
The function would then re-run the <code>listen</code> function via the <code>notateReentry()</code> function at the next point in the melody as tracked by the <strong>String Position</strong> object.</p>
<p>If there were not to have been an exact match, the model would remove one token from the melody and perform the search again on the knowledge of all 4-grams and add one to the <strong>Difficulty</strong> counter.
This process would happen recursively until a match is found.
If no match is found in either the complex representation, or that of the two rhythm and pitch corpora, the fifth step of <code>transcribe()</code> would trigger <code>notateReentry()</code> to be run without documenting the n-gram currently being dictated.
This would be akin to a student not being able to identify a difficult interval, thus having to restart the melody at a new position.
Decisions about re-entry warrant further research and discussion, but for the sake of parsimony this model assumes linear continuation.
As notated in <a href="computational-model.html#dictation-re-entry">Dictation Re-Entry</a>, other modes of re-entry could be incorporated into the model.</p>
<p>This looping process would occur again and again until the entire melody is notated.
With each iteration of each n-gram notated, the difficulty counter would increase in relation to the representation of that string in the corpus.
This provides an algorithmic implementation of the intuition that less common n-grams or intervals (2-grams) are going to lead to higher difficulty in dictation.
Also worth noting is steps 3 and 4 in the <code>transcribe()</code> function are akin to Karpinski’s proto-notation.
Further research might consider advantages in the order of searching the <strong>Prior Knowledge</strong> corpora.</p>
</div>
</div>
<div id="conclusions-3" class="section level2">
<h2><span class="header-section-number">7.5</span> Conclusions</h2>
<p>In this chapter, I presented an explanatory, computational model of melodic dictation.
The model combines work from computational musicology and work from cognitive psychology.
In addition to being a complete model that explicates every step of the dictation process, the model seems to match phenomenological intuitions as to the process of melodic dictation.
Given the current state of the model, it makes predictions about the dictation process and can eventually be implemented and tested against human behavioral data to provide evidence in support of its verisimilitude.
For example, the model predicts:</p>
<ul>
<li>Segments of melodies are likely successfully to be dictated relative to the frequency distribution of their prior knowledge.</li>
<li>Higher working memory span individuals will be able to dictate larger chunks of melodies, and thus perform better at dictation</li>
<li>Using an <em>atomistic</em> dictation strategy will result in not as effective dictations than attempting to identify larger patterns</li>
<li>Determining the difficulty of melodies of equal length is predictable from the frequency the melody’s cumulative n-gram distribution.</li>
<li>Some <em>atonal</em> melodies will be easier to dictate than tonal melodies if they consist of patterns that are more frequent in a listener’s Prior Knowledge.</li>
<li>Higher exposure to sight-singing results in more explicitly learned patterns, thus the ability to identify larger patterns of music</li>
</ul>
<p>Although many of these hypotheses might seem intuitive to any instructor who has taught aural skills before, work from this dissertation provides a theory as to why each appears to be true.
Future research beyond this dissertation will explore predictions as a result of this work in more detail.
Most importantly from a pedagogical standpoint, the model and underlying theory gives exact language to discuss the underlying processes of melodic dictation, which can serve as a valuable pedagogical and research contribution.</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-wigginsModelsMusicalSimilarity2007">
<p>Wiggins, Geraint A. 2007. “Models of Musical Similarity.” <em>Musicae Scientiae</em> 11 (1_suppl): 315–38. <a href="https://doi.org/10.1177/102986490701100112" class="uri">https://doi.org/10.1177/102986490701100112</a>.</p>
</div>
<div id="ref-pearceStatisticalLearningProbabilistic2018a">
<p>Pearce, Marcus T. 2018. “Statistical Learning and Probabilistic Prediction in Music Cognition: Mechanisms of Stylistic Enculturation: Enculturation: Statistical Learning and Prediction.” <em>Annals of the New York Academy of Sciences</em> 1423 (1): 378–95. <a href="https://doi.org/10.1111/nyas.13654" class="uri">https://doi.org/10.1111/nyas.13654</a>.</p>
</div>
<div id="ref-saffranStatisticalLearningTone1999">
<p>Saffran, Jenny R, Elizabeth K Johnson, Richard N Aslin, and Elissa L Newport. 1999. “Statistical Learning of Tone Sequences by Human Infants and Adults.” <em>Cognition</em> 70 (1): 27–52. <a href="https://doi.org/10.1016/S0010-0277(98)00075-4" class="uri">https://doi.org/10.1016/S0010-0277(98)00075-4</a>.</p>
</div>
<div id="ref-margulisRepeatHowMusic2014">
<p>Margulis, Elizabeth Hellmuth. 2014. <em>On Repeat: How Music Plays the Mind</em>. Oxford: Oxford University Press.</p>
</div>
<div id="ref-huronSweetAnticipation2006">
<p>Huron, David. 2006. <em>Sweet Anticipation</em>. MIT Press.</p>
</div>
<div id="ref-karpinskiAuralSkillsAcquisition2000">
<p>Karpinski, Gary Steven. 2000. <em>Aural Skills Acquisition: The Development of Listening, Reading, and Performing Skills in College-Level Musicians</em>. Oxford University Press.</p>
</div>
<div id="ref-karpinskiModelMusicPerception1990">
<p>Karpinski, Gary. 1990. “A Model for Music Perception and Its Implications in Melodic Dictation.” <em>Journal of Music Theory Pedagogy</em> 4 (1): 191–229.</p>
</div>
<div id="ref-pearceConstructionEvaluationStatistical2005">
<p>Pearce, Marcus. 2005. “The Construction and Evaluation of Statistical Models of Melodic Structure in Music Perception and Composition.” Department of Computer Science: City University of London.</p>
</div>
<div id="ref-cowanEvolvingConceptionsMemory1988">
<p>Cowan, Nelson. 1988. “Evolving Conceptions of Memory Storage, Selective Attention, and Their Mutual Constraints Within the Human Information-Processing System.” <em>Psychological Bulletin</em> 104 (2): 163–91.</p>
</div>
<div id="ref-cowanMagicalMysteryFour2010">
<p>Cowan, Nelson. 2010. “The Magical Mystery Four: How Is Working Memory Capacity Limited, and Why?” <em>Current Directions in Psychological Science</em> 19 (1): 51–57. <a href="https://doi.org/10.1177/0963721409359277" class="uri">https://doi.org/10.1177/0963721409359277</a>.</p>
</div>
<div id="ref-lerdahlGenerativeTheoryTonal1986">
<p>Lerdahl, Fred, and Ray Jackendoff. 1986. <em>A Generative Theory of Tonal Music.</em> Cambridge: MIT Press.</p>
</div>
<div id="ref-narmourAnalysisCognitionBasic1990">
<p>Narmour, Eugene. 1990. <em>The Analysis and Cognition of Basic Melodic Structures: The Implication-Realization Model.</em> Chicago: University of Chicago Press.</p>
</div>
<div id="ref-narmourAnalysisCognitionMelodic1992">
<p>Narmour, Eugene. 1992. <em>The Analysis and Cognition of Melodic Complexity: The Implication-Realization Model</em>. University of Chicago Press.</p>
</div>
<div id="ref-temperleyCognitionBasicMusical2004">
<p>Temperley, David. 2004. <em>The Cognition of Basic Musical Structures</em>. Cambridge: MIT Press.</p>
</div>
<div id="ref-mcdermottIndifferenceDissonanceNative2016">
<p>McDermott, Josh H., Alan F. Schultz, Eduardo A. Undurraga, and Ricardo A. Godoy. 2016. “Indifference to Dissonance in Native Amazonians Reveals Cultural Variation in Music Perception.” <em>Nature</em> 535 (7613): 547–50. <a href="https://doi.org/10.1038/nature18635" class="uri">https://doi.org/10.1038/nature18635</a>.</p>
</div>
<div id="ref-savageStatisticalUniversalsReveal2015">
<p>Savage, Patrick E., Steven Brown, Emi Sakai, and Thomas E. Currie. 2015. “Statistical Universals Reveal the Structures and Functions of Human Music.” <em>Proceedings of the National Academy of Sciences</em> 112 (29): 8987–92. <a href="https://doi.org/10.1073/pnas.1414495112" class="uri">https://doi.org/10.1073/pnas.1414495112</a>.</p>
</div>
<div id="ref-harrisonInstantaneousConsonancePerception">
<p>Harrison, Peter, and Marcus Thomas Pearce. n.d. “Instantaneous Consonance in the Perception and Composition of Western Music.” Accessed April 7, 2019. <a href="https://doi.org/10.31234/osf.io/6jsug" class="uri">https://doi.org/10.31234/osf.io/6jsug</a>.</p>
</div>
<div id="ref-krumhanslCognitiveFoundationsMusical2001">
<p>Krumhansl, Carol. 2001. <em>Cognitive Foundations of Musical Pitch</em>. Oxford University Press.</p>
</div>
<div id="ref-levitinCurrentAdvancesCognitive2009">
<p>Levitin, Daniel J., and Anna K. Tirovolas. 2009. “Current Advances in the Cognitive Neuroscience of Music.” <em>Annals of the New York Academy of Sciences</em> 1156 (1): 211–31. <a href="https://doi.org/10.1111/j.1749-6632.2009.04417.x" class="uri">https://doi.org/10.1111/j.1749-6632.2009.04417.x</a>.</p>
</div>
<div id="ref-conklinMultipleViewpointSystems1995">
<p>Conklin, Darrell, and Ian H. Witten. 1995. “Multiple Viewpoint Systems for Music Prediction.” <em>Journal of New Music Research</em> 24 (1): 51–73. <a href="https://doi.org/10.1080/09298219508570672" class="uri">https://doi.org/10.1080/09298219508570672</a>.</p>
</div>
<div id="ref-shannonMathematicalTheoryCommunication1948">
<p>Shannon, Claude. 1948. “A Mathematical Theory of Communication.” <em>Bell System Technical Journal</em> 27: 379–423.</p>
</div>
<div id="ref-harrisonDissociatingSensoryCognitive2018">
<p>Harrison, Peter M . C., and Marcus Thomas Pearce. 2018. “Dissociating Sensory and Cognitive Theories of Harmony Perception Through Computational Modeling.” <a href="https://doi.org/10.31234/osf.io/wgjyv" class="uri">https://doi.org/10.31234/osf.io/wgjyv</a>.</p>
</div>
<div id="ref-bigandEmpiricalEvidenceMusical2014">
<p>Bigand, Emmanuel, Charles DelbÃ, BÃnÃdicte Poulin-Charronnat, Marc Leman, and Barbara Tillmann. 2014. “Empirical Evidence for Musical Syntax Processing? Computer Simulations Reveal the Contribution of Auditory Short-Term Memory.” <em>Frontiers in Systems Neuroscience</em> 8 (June). <a href="https://doi.org/10.3389/fnsys.2014.00094" class="uri">https://doi.org/10.3389/fnsys.2014.00094</a>.</p>
</div>
<div id="ref-sauvePredictionPolyphonyModelling2017">
<p>Sauve, Sarah. 2017. “Prediction in Polyphony: Modelling Auditory Scene Analysis.” Centre for Digital Music: Queen Mary, University of London.</p>
</div>
<div id="ref-margulisModelMelodicExpectation2005">
<p>Margulis, Elizabeth Hellmuth. 2005. “A Model of Melodic Expectation.” <em>Music Perception: An Interdisciplinary Journal</em> 22 (4): 663–714. <a href="https://doi.org/10.1525/mp.2005.22.4.663" class="uri">https://doi.org/10.1525/mp.2005.22.4.663</a>.</p>
</div>
<div id="ref-baddeleyWorkingMemory1974">
<p>Baddeley, Alan D., and Graham Hitch. 1974. “Working Memory.” In <em>Psychology of Learning and Motivation</em>, 8:47–89. Elsevier. <a href="https://doi.org/10.1016/S0079-7421(08)60452-1" class="uri">https://doi.org/10.1016/S0079-7421(08)60452-1</a>.</p>
</div>
<div id="ref-shepardMentalRotationThreeDimensional1971">
<p>Shepard, R. N., and J. Metzler. 1971. “Mental Rotation of Three-Dimensional Objects.” <em>Science</em> 171 (3972): 701–3. <a href="https://doi.org/10.1126/science.171.3972.701" class="uri">https://doi.org/10.1126/science.171.3972.701</a>.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="6">
<li id="fn6"><p>The following musical examples is taken from <span class="citation">Pearce (<a href="#ref-pearceStatisticalLearningProbabilistic2018a">2018</a>)</span> reflects a model where IDyOM was configured to predict pitch with an attribute linking melodic pitch interval and chromatic scale degree (pitch and scale degree) using both the short-term and long-term models, the latter trained on 903 folk songs and chorales (data sets 1, 2, and 9 from table 4.1 in <span class="citation">(Schaffrath <a href="#ref-schaffrathEssenFolkSong1995">1995</a>)</span> comprising 50,867 notes.<a href="computational-model.html#fnref6" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="experiment.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="references.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["mmd-draft-djb.pdf", "mmd-draft-djb.epub"],
"toc": {
"collapse": "subsection"
},
"toc_depth": 2
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
