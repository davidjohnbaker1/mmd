<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Chapter 3 Individual Differences | Modeling Melodic Dictation</title>
  <meta name="description" content="This dissertation explores both individual and musical features that might contribute to processes involved in melodic dictation.">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Chapter 3 Individual Differences | Modeling Melodic Dictation" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This dissertation explores both individual and musical features that might contribute to processes involved in melodic dictation." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Individual Differences | Modeling Melodic Dictation" />
  
  <meta name="twitter:description" content="This dissertation explores both individual and musical features that might contribute to processes involved in melodic dictation." />
  

<meta name="author" content="David John Baker">


<meta name="date" content="2019-01-07">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="intro.html">
<link rel="next" href="computation-chapter.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Modeling Melodic Dictation</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Significance of the Study</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#claims-about-need-to-join-the-worlds-of-theory-and-pedagogy"><i class="fa fa-check"></i><b>1.1</b> Claims about need to join the worlds of theory and pedagogy</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#chapter-overview"><i class="fa fa-check"></i><b>1.2</b> Chapter Overview</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Theoretical Background and Rationale</a><ul>
<li class="chapter" data-level="2.1" data-path="intro.html"><a href="intro.html#what-is-melodic-dictation-and-why"><i class="fa fa-check"></i><b>2.1</b> What is melodic dictation? and Why?</a><ul>
<li class="chapter" data-level="2.1.1" data-path="intro.html"><a href="intro.html#describing-melodic-dictation"><i class="fa fa-check"></i><b>2.1.1</b> Describing Melodic Dictation</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="intro.html"><a href="intro.html#individual-factors"><i class="fa fa-check"></i><b>2.2</b> Individual Factors</a><ul>
<li class="chapter" data-level="2.2.1" data-path="intro.html"><a href="intro.html#cognitive"><i class="fa fa-check"></i><b>2.2.1</b> Cognitive</a></li>
<li class="chapter" data-level="2.2.2" data-path="intro.html"><a href="intro.html#measuring-intelligence"><i class="fa fa-check"></i><b>2.2.2</b> Measuring Intelligence</a></li>
<li class="chapter" data-level="2.2.3" data-path="intro.html"><a href="intro.html#working-memory-capacity"><i class="fa fa-check"></i><b>2.2.3</b> Working Memory Capacity</a></li>
<li class="chapter" data-level="2.2.4" data-path="intro.html"><a href="intro.html#general-intelligence"><i class="fa fa-check"></i><b>2.2.4</b> General Intelligence</a></li>
<li class="chapter" data-level="2.2.5" data-path="intro.html"><a href="intro.html#environmental"><i class="fa fa-check"></i><b>2.2.5</b> Environmental</a></li>
<li class="chapter" data-level="2.2.6" data-path="intro.html"><a href="intro.html#musical-training"><i class="fa fa-check"></i><b>2.2.6</b> Musical Training</a></li>
<li class="chapter" data-level="2.2.7" data-path="intro.html"><a href="intro.html#aural-training"><i class="fa fa-check"></i><b>2.2.7</b> Aural Training</a></li>
<li class="chapter" data-level="2.2.8" data-path="intro.html"><a href="intro.html#sight-singing"><i class="fa fa-check"></i><b>2.2.8</b> Sight Singing</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="intro.html"><a href="intro.html#musical-parameters"><i class="fa fa-check"></i><b>2.3</b> Musical Parameters</a><ul>
<li class="chapter" data-level="2.3.1" data-path="intro.html"><a href="intro.html#structural"><i class="fa fa-check"></i><b>2.3.1</b> Structural</a></li>
<li class="chapter" data-level="2.3.2" data-path="intro.html"><a href="intro.html#experimental"><i class="fa fa-check"></i><b>2.3.2</b> Experimental</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="intro.html"><a href="intro.html#modeling-and-polymorphism-of-ability"><i class="fa fa-check"></i><b>2.4</b> Modeling and Polymorphism of Ability</a></li>
<li class="chapter" data-level="2.5" data-path="intro.html"><a href="intro.html#conclusions"><i class="fa fa-check"></i><b>2.5</b> Conclusions</a><ul>
<li class="chapter" data-level="2.5.1" data-path="intro.html"><a href="intro.html#add-in"><i class="fa fa-check"></i><b>2.5.1</b> Add In</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="individual-differences.html"><a href="individual-differences.html"><i class="fa fa-check"></i><b>3</b> Individual Differences</a><ul>
<li class="chapter" data-level="3.0.1" data-path="individual-differences.html"><a href="individual-differences.html#rationale"><i class="fa fa-check"></i><b>3.0.1</b> Rationale</a></li>
<li class="chapter" data-level="3.1" data-path="individual-differences.html"><a href="individual-differences.html#individual-differences-1"><i class="fa fa-check"></i><b>3.1</b> Individual Differences</a><ul>
<li class="chapter" data-level="3.1.1" data-path="individual-differences.html"><a href="individual-differences.html#musical-training-1"><i class="fa fa-check"></i><b>3.1.1</b> Musical Training</a></li>
<li class="chapter" data-level="3.1.2" data-path="individual-differences.html"><a href="individual-differences.html#dictation-without-dictation"><i class="fa fa-check"></i><b>3.1.2</b> Dictation without Dictation</a></li>
<li class="chapter" data-level="3.1.3" data-path="individual-differences.html"><a href="individual-differences.html#cognitive-measures-of-interest"><i class="fa fa-check"></i><b>3.1.3</b> Cognitive Measures of interest</a></li>
<li class="chapter" data-level="3.1.4" data-path="individual-differences.html"><a href="individual-differences.html#path-analysis-here"><i class="fa fa-check"></i><b>3.1.4</b> PATH Analysis here</a></li>
<li class="chapter" data-level="3.1.5" data-path="individual-differences.html"><a href="individual-differences.html#hypotheses"><i class="fa fa-check"></i><b>3.1.5</b> Hypotheses</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="individual-differences.html"><a href="individual-differences.html#overview-of-experiment"><i class="fa fa-check"></i><b>3.2</b> Overview of Experiment</a><ul>
<li class="chapter" data-level="3.2.1" data-path="individual-differences.html"><a href="individual-differences.html#participants"><i class="fa fa-check"></i><b>3.2.1</b> Participants</a></li>
<li class="chapter" data-level="3.2.2" data-path="individual-differences.html"><a href="individual-differences.html#materials"><i class="fa fa-check"></i><b>3.2.2</b> Materials</a></li>
<li class="chapter" data-level="3.2.3" data-path="individual-differences.html"><a href="individual-differences.html#procedure"><i class="fa fa-check"></i><b>3.2.3</b> Procedure</a></li>
<li class="chapter" data-level="3.2.4" data-path="individual-differences.html"><a href="individual-differences.html#results"><i class="fa fa-check"></i><b>3.2.4</b> Results</a></li>
<li class="chapter" data-level="3.2.5" data-path="individual-differences.html"><a href="individual-differences.html#discussion"><i class="fa fa-check"></i><b>3.2.5</b> Discussion</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="computation-chapter.html"><a href="computation-chapter.html"><i class="fa fa-check"></i><b>4</b> Computation Chapter</a><ul>
<li class="chapter" data-level="4.0.1" data-path="computation-chapter.html"><a href="computation-chapter.html#information-content-referencs"><i class="fa fa-check"></i><b>4.0.1</b> Information Content Referencs</a></li>
<li class="chapter" data-level="4.1" data-path="computation-chapter.html"><a href="computation-chapter.html#humans-like-patterns-and-are-very-good-at-picking-them-up"><i class="fa fa-check"></i><b>4.1</b> Humans like patterns and are very good at picking them up</a><ul>
<li class="chapter" data-level="4.1.1" data-path="computation-chapter.html"><a href="computation-chapter.html#we-learn-things-implicitly"><i class="fa fa-check"></i><b>4.1.1</b> We learn things implicitly</a></li>
<li class="chapter" data-level="4.1.2" data-path="computation-chapter.html"><a href="computation-chapter.html#we-can-represent-that-implicit-knowledge-with-a-corpus"><i class="fa fa-check"></i><b>4.1.2</b> We can represent that implicit knowledge with a corpus</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="computation-chapter.html"><a href="computation-chapter.html#pre-musical-corpora"><i class="fa fa-check"></i><b>4.2</b> Pre-Musical Corpora</a><ul>
<li class="chapter" data-level="4.2.1" data-path="computation-chapter.html"><a href="computation-chapter.html#information-theory"><i class="fa fa-check"></i><b>4.2.1</b> Information Theory</a></li>
<li class="chapter" data-level="4.2.2" data-path="computation-chapter.html"><a href="computation-chapter.html#computational-linguistics-as-front-runner"><i class="fa fa-check"></i><b>4.2.2</b> Computational Linguistics as front runner</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="computation-chapter.html"><a href="computation-chapter.html#musical-corpora"><i class="fa fa-check"></i><b>4.3</b> Musical Corpora</a><ul>
<li class="chapter" data-level="4.3.1" data-path="computation-chapter.html"><a href="computation-chapter.html#history-of-musical-corpora"><i class="fa fa-check"></i><b>4.3.1</b> History of Musical Corpora</a></li>
<li class="chapter" data-level="4.3.2" data-path="computation-chapter.html"><a href="computation-chapter.html#fantastic"><i class="fa fa-check"></i><b>4.3.2</b> FANTASTIC</a></li>
<li class="chapter" data-level="4.3.3" data-path="computation-chapter.html"><a href="computation-chapter.html#idyom-as-representation-of-musical-materials"><i class="fa fa-check"></i><b>4.3.3</b> IDyOM as representation of musical materials</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="computation-chapter.html"><a href="computation-chapter.html#so-what"><i class="fa fa-check"></i><b>4.4</b> So What?</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="hello-corpus.html"><a href="hello-corpus.html"><i class="fa fa-check"></i><b>5</b> Hello, Corpus</a><ul>
<li class="chapter" data-level="5.0.1" data-path="hello-corpus.html"><a href="hello-corpus.html#why-i-dont-follow-a-random-sampling-method"><i class="fa fa-check"></i><b>5.0.1</b> Why I don’t follow a random sampling method</a></li>
<li class="chapter" data-level="5.1" data-path="hello-corpus.html"><a href="hello-corpus.html#brief-review-of-chapter-4-on-corpus-language-to-reflect-journal-submission"><i class="fa fa-check"></i><b>5.1</b> Brief review of Chapter 4 on corpus (Language to reflect journal submission)</a><ul>
<li class="chapter" data-level="5.1.1" data-path="hello-corpus.html"><a href="hello-corpus.html#corpus-outside-of-music"><i class="fa fa-check"></i><b>5.1.1</b> Corpus outside of music</a></li>
<li class="chapter" data-level="5.1.2" data-path="hello-corpus.html"><a href="hello-corpus.html#corpus-in-music"><i class="fa fa-check"></i><b>5.1.2</b> Corpus in Music</a></li>
<li class="chapter" data-level="5.1.3" data-path="hello-corpus.html"><a href="hello-corpus.html#the-point-is-that-it-implicitly-represents-humand-knowledge"><i class="fa fa-check"></i><b>5.1.3</b> The point is that it implicitly represents humand knowledge</a></li>
<li class="chapter" data-level="5.1.4" data-path="hello-corpus.html"><a href="hello-corpus.html#idyom-1"><i class="fa fa-check"></i><b>5.1.4</b> IDyOM 1</a></li>
<li class="chapter" data-level="5.1.5" data-path="hello-corpus.html"><a href="hello-corpus.html#idyom-2"><i class="fa fa-check"></i><b>5.1.5</b> IDyOM 2</a></li>
<li class="chapter" data-level="5.1.6" data-path="hello-corpus.html"><a href="hello-corpus.html#idyom-3"><i class="fa fa-check"></i><b>5.1.6</b> IDyOM 3</a></li>
<li class="chapter" data-level="5.1.7" data-path="hello-corpus.html"><a href="hello-corpus.html#huron-suggestions-that-starts-of-melodies-relate-to-mental-rotaiton"><i class="fa fa-check"></i><b>5.1.7</b> Huron suggestions that starts of melodies relate to mental rotaiton</a></li>
<li class="chapter" data-level="5.1.8" data-path="hello-corpus.html"><a href="hello-corpus.html#other-huron-claims"><i class="fa fa-check"></i><b>5.1.8</b> Other Huron claims</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="hello-corpus.html"><a href="hello-corpus.html#note-problem-with-using-corpus-is-making-corpus"><i class="fa fa-check"></i><b>5.2</b> Note problem with using corpus is making corpus</a><ul>
<li class="chapter" data-level="5.2.1" data-path="hello-corpus.html"><a href="hello-corpus.html#many-are-used-on-essen"><i class="fa fa-check"></i><b>5.2.1</b> Many are used on Essen</a></li>
<li class="chapter" data-level="5.2.2" data-path="hello-corpus.html"><a href="hello-corpus.html#brinkman-says-essen-sucks"><i class="fa fa-check"></i><b>5.2.2</b> Brinkman says Essen Sucks</a></li>
<li class="chapter" data-level="5.2.3" data-path="hello-corpus.html"><a href="hello-corpus.html#if-going-to-make-generlizable-claims-need-to-always-have-new-data"><i class="fa fa-check"></i><b>5.2.3</b> If going to make generlizable claims, need to always have new data</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="hello-corpus.html"><a href="hello-corpus.html#solem-duty-to-encode-and-report-on-corpus"><i class="fa fa-check"></i><b>5.3</b> Solem duty to encode and report on corpus</a><ul>
<li class="chapter" data-level="5.3.1" data-path="hello-corpus.html"><a href="hello-corpus.html#justin-london-article-on-what-makes-it-into-a-corpsu"><i class="fa fa-check"></i><b>5.3.1</b> Justin London Article on what makes it into a corpsu</a></li>
<li class="chapter" data-level="5.3.2" data-path="hello-corpus.html"><a href="hello-corpus.html#though-i-just-encoded-the-whole-thing-because-in-my-heart-of-hearts-im-a-bayesian"><i class="fa fa-check"></i><b>5.3.2</b> Though I just encoded the whole thing because in my heart of hearts I’m a Bayesian</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="hello-corpus.html"><a href="hello-corpus.html#the-corpus"><i class="fa fa-check"></i><b>5.4</b> The Corpus</a><ul>
<li class="chapter" data-level="5.4.1" data-path="hello-corpus.html"><a href="hello-corpus.html#history-of-sight-singign-books"><i class="fa fa-check"></i><b>5.4.1</b> History of Sight Singign books</a></li>
<li class="chapter" data-level="5.4.2" data-path="hello-corpus.html"><a href="hello-corpus.html#assumed-to-be-where-long-term-store-comes-from-adumbrate-computational-model"><i class="fa fa-check"></i><b>5.4.2</b> Assumed to be where long term store comes from (adumbrate computational model)</a></li>
<li class="chapter" data-level="5.4.3" data-path="hello-corpus.html"><a href="hello-corpus.html#lots-of-melodies-in-ascending-order-of-difficulty-grouped-appropriately-though-utah-guy"><i class="fa fa-check"></i><b>5.4.3</b> Lots of melodies in ascending order of difficulty, grouped appropriately though? Utah guy</a></li>
<li class="chapter" data-level="5.4.4" data-path="hello-corpus.html"><a href="hello-corpus.html#why-i-encoded-it-in-xml"><i class="fa fa-check"></i><b>5.4.4</b> Why I encoded it in XML</a></li>
<li class="chapter" data-level="5.4.5" data-path="hello-corpus.html"><a href="hello-corpus.html#is-it-legal"><i class="fa fa-check"></i><b>5.4.5</b> Is it legal?</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="hello-corpus.html"><a href="hello-corpus.html#descriptive-stats-of-corpus"><i class="fa fa-check"></i><b>5.5</b> Descriptive Stats of Corpus</a><ul>
<li class="chapter" data-level="5.5.1" data-path="hello-corpus.html"><a href="hello-corpus.html#why"><i class="fa fa-check"></i><b>5.5.1</b> Why?</a></li>
<li class="chapter" data-level="5.5.2" data-path="hello-corpus.html"><a href="hello-corpus.html#feature-level"><i class="fa fa-check"></i><b>5.5.2</b> Feature Level</a></li>
<li class="chapter" data-level="5.5.3" data-path="hello-corpus.html"><a href="hello-corpus.html#n-gram"><i class="fa fa-check"></i><b>5.5.3</b> n-gram</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="experiments.html"><a href="experiments.html"><i class="fa fa-check"></i><b>6</b> Experiments</a><ul>
<li class="chapter" data-level="6.1" data-path="experiments.html"><a href="experiments.html#rationale-1"><i class="fa fa-check"></i><b>6.1</b> Rationale</a><ul>
<li class="chapter" data-level="6.1.1" data-path="experiments.html"><a href="experiments.html#have-done-all-this-and-have-not-actually-talked-about-dictation-yet"><i class="fa fa-check"></i><b>6.1.1</b> Have done all this and have not actually talked about dictation yet</a></li>
<li class="chapter" data-level="6.1.2" data-path="experiments.html"><a href="experiments.html#clearly-many-factors-contribte-to-this-whole-thing-and-need-to-be-taken-into-a-model"><i class="fa fa-check"></i><b>6.1.2</b> Clearly many factors contribte to this whole thing and need to be taken into a model</a></li>
<li class="chapter" data-level="6.1.3" data-path="experiments.html"><a href="experiments.html#dictation-is-basically-a-within-subjects-design-experiment"><i class="fa fa-check"></i><b>6.1.3</b> Dictation is basically a within subjects design Experiment</a></li>
<li class="chapter" data-level="6.1.4" data-path="experiments.html"><a href="experiments.html#factors"><i class="fa fa-check"></i><b>6.1.4</b> Factors</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="experiments.html"><a href="experiments.html#experiments-1"><i class="fa fa-check"></i><b>6.2</b> Experiments</a><ul>
<li class="chapter" data-level="6.2.1" data-path="experiments.html"><a href="experiments.html#experiment-i"><i class="fa fa-check"></i><b>6.2.1</b> Experiment I</a></li>
<li class="chapter" data-level="6.2.2" data-path="experiments.html"><a href="experiments.html#experiment-ii"><i class="fa fa-check"></i><b>6.2.2</b> Experiment II</a></li>
<li class="chapter" data-level="6.2.3" data-path="experiments.html"><a href="experiments.html#general-discussion"><i class="fa fa-check"></i><b>6.2.3</b> General Discussion</a></li>
<li class="chapter" data-level="6.2.4" data-path="experiments.html"><a href="experiments.html#really-what-is-needed-is-computational-model"><i class="fa fa-check"></i><b>6.2.4</b> Really what is needed is Computational Model</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="computational-model.html"><a href="computational-model.html"><i class="fa fa-check"></i><b>7</b> Computational Model</a><ul>
<li class="chapter" data-level="7.1" data-path="computational-model.html"><a href="computational-model.html#levels-of-abstraction"><i class="fa fa-check"></i><b>7.1</b> Levels of Abstraction</a></li>
<li class="chapter" data-level="7.2" data-path="computational-model.html"><a href="computational-model.html#model-overview"><i class="fa fa-check"></i><b>7.2</b> Model Overview</a></li>
<li class="chapter" data-level="7.3" data-path="computational-model.html"><a href="computational-model.html#verbal-model"><i class="fa fa-check"></i><b>7.3</b> Verbal Model</a><ul>
<li class="chapter" data-level="7.3.1" data-path="computational-model.html"><a href="computational-model.html#model-representational-assumptions"><i class="fa fa-check"></i><b>7.3.1</b> Model Representational Assumptions</a></li>
<li class="chapter" data-level="7.3.2" data-path="computational-model.html"><a href="computational-model.html#contents-of-the-prior-knowledge"><i class="fa fa-check"></i><b>7.3.2</b> Contents of the Prior Knowledge</a></li>
<li class="chapter" data-level="7.3.3" data-path="computational-model.html"><a href="computational-model.html#modeling-information-content"><i class="fa fa-check"></i><b>7.3.3</b> Modeling Information Content</a></li>
<li class="chapter" data-level="7.3.4" data-path="computational-model.html"><a href="computational-model.html#setting-limits-with-transcribe"><i class="fa fa-check"></i><b>7.3.4</b> Setting Limits with Transcribe</a></li>
<li class="chapter" data-level="7.3.5" data-path="computational-model.html"><a href="computational-model.html#pattern-matching"><i class="fa fa-check"></i><b>7.3.5</b> Pattern Matching</a></li>
<li class="chapter" data-level="7.3.6" data-path="computational-model.html"><a href="computational-model.html#dictation-re-entry"><i class="fa fa-check"></i><b>7.3.6</b> Dictation Re-Entry</a></li>
<li class="chapter" data-level="7.3.7" data-path="computational-model.html"><a href="computational-model.html#completion"><i class="fa fa-check"></i><b>7.3.7</b> Completion</a></li>
<li class="chapter" data-level="7.3.8" data-path="computational-model.html"><a href="computational-model.html#model-output"><i class="fa fa-check"></i><b>7.3.8</b> Model Output</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="computational-model.html"><a href="computational-model.html#formal-model"><i class="fa fa-check"></i><b>7.4</b> Formal Model</a><ul>
<li class="chapter" data-level="7.4.1" data-path="computational-model.html"><a href="computational-model.html#computational-model-1"><i class="fa fa-check"></i><b>7.4.1</b> Computational Model</a></li>
<li class="chapter" data-level="7.4.2" data-path="computational-model.html"><a href="computational-model.html#example"><i class="fa fa-check"></i><b>7.4.2</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="computational-model.html"><a href="computational-model.html#conclusions-1"><i class="fa fa-check"></i><b>7.5</b> Conclusions</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="reference-log.html"><a href="reference-log.html"><i class="fa fa-check"></i><b>8</b> Reference Log</a><ul>
<li class="chapter" data-level="8.1" data-path="reference-log.html"><a href="reference-log.html#to-incorporate"><i class="fa fa-check"></i><b>8.1</b> To Incorporate</a></li>
<li class="chapter" data-level="8.2" data-path="reference-log.html"><a href="reference-log.html#chapter-3"><i class="fa fa-check"></i><b>8.2</b> Chapter 3</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Modeling Melodic Dictation</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="individual-differences" class="section level1">
<h1><span class="header-section-number">Chapter 3</span> Individual Differences</h1>
<div id="rationale" class="section level3">
<h3><span class="header-section-number">3.0.1</span> Rationale</h3>
<p>The first two steps of Gary Karpinski’s model of melodic dictation <span class="citation">(Karpinski <a href="#ref-karpinskiAuralSkillsAcquisition2000">2000</a>, <a href="#ref-karpinskiModelMusicPerception1990">1990</a>)</span> rely exclusively on the mental representation of melodic information.
Karpinski conceptualizes the first stage of <em>hearing</em> as solely involving the physical motions on the tympanic membrane, as well as the listener’s attention to the musical stimulus.
This stage is distinguished from that of <em>short-term melodic memory</em> which refers to the amount of melodic information that can be represented in conscious awareness.
Given that neither stage of the first two steps of Karpinski’s model requires any sort of musical expertise, every individual with normal hearing and cognition should be able to partake in the first two steps of melodic dictation.<a href="#fn9" class="footnote-ref" id="fnref9"><sup>9</sup></a>
The ability to hear, then remember musical information is where all students of melodic dictation are presumed to start.
From this baseline students recieve explicit education in music theory and aural skills and to develop the ability to link what is heard to what can then be musically understood and then notated.</p>
<p>While the majority of beginning students of melodic dictation are assumed to start at the same ability, research from XX suggests that individual differences in musical perception exist and must be accounted for from a psychological and pedagogical standpoint.
In order to fully capture the diversity of listening abilities amongst students of melodic dictation, music pedagogy research should make sure that it has accounted for differences.</p>
<p>Attempting to investigate all four parts of melodic dictation from hearing, to short-term melodic memory, to musical understanding, to notation is both cumbersome from a theoretical perspective and practically unfeasible due to the amount of variables at play.
In order to obtain a clearer picture of what mechanisms contribute to this process, these steps must be be investigated in turn.
This chapter investigates the first two steps of the Karpinski model with an experiment investigating individual factors that contribute to an individual’s musical memory that do not depend on their knowledge of Western musical notation.
By understanding which, if any, individual factors play a role in this process, it will inform what can be reasonably expected of individuals when other musical variables are introduced.</p>
</div>
<div id="individual-differences-1" class="section level2">
<h2><span class="header-section-number">3.1</span> Individual Differences</h2>
<p>While explicit musical training is proposed to get people better at this…
Literature on individual differences says other things might contribute.
For example xyz
So while pedagogically, yes training, a full model of md needs to acknowledge the diversity of listening abilities.
This is especially relvant bc some literature does not suggest musician advantage</p>
<p>As stated above, the first two steps of Karpinski’s model of melodic dictation are not exclusive to trained musicians.
These steps involve first hearing the melody and then retaining the melody in conscious awareness.
From a pedagogical and psychological standpoint, it would be safe to assume that individuals differ in their ability to memorize musical material.
Some individuals perform well, while others do not.
Presumably these differences in performance can be attributed to a number of factors.</p>
<p>One of the first factors –and perhaps most obvious to consider– that might explain differences in ability would be an individual’s musical training.</p>
<p>Musicans tend to have better memory
they also do better with other cognitive tasks
but really this is reversed
If you look at memory for melodies, musicians do not have an advantage
Seems to be much more like domain expertise thing</p>
<p>As noted in the previous chapter, people with some degree of musical training tend to outperform their less musically trained peers on many cognitive tasks both in terms of memory and general problem solving <span class="citation">(Schellenberg <a href="#ref-schellenbergMusicNonmusicalAbilities2017">2017</a>)</span>.</p>
<p>A meta-analysis by Talamini and colleagues <span class="citation">(Talamini et al. <a href="#ref-talaminiMusiciansHaveBetter2017">2017</a>)</span> highlighted differences in memory are especially pronouced when the stimuli used in the experiments was tonal.
This finding suggests a relationship between muscial training and an increase in memory for musical material.</p>
<p>Additionally<a href="#fn10" class="footnote-ref" id="fnref10"><sup>10</sup></a>, musically trained individuals perform better on other tests of cognitive ability.
Most prominant in many of these studies are findings that link intelligence and musical training.
As reviewed in <span class="citation">Schellenberg (<a href="#ref-schellenbergMusicNonmusicalAbilities2017">2017</a>)</span>, both children and adults who engage in musical activity tend to score higher on general measurses of intelligence than their non-musical peers (Gibson, Folley and Park, 2009; Hille et al., 2011; Schellenberg, 2011a; Schellenberg and Mankarious, 2012).
The finding importantly comes with it a correlation between duration of musical training and the extent of the increases in intelligence (Degé, Kubicek and Schwarzer, 2011a; Degé, Wehrum, Stark and Schwarzer, 2015; Corrigall and Schellenberg, 2015; Corrigall, Schellenberg and Misura, 2013; Schellenberg, 2006).
While many of these studies are correlational, other researchers have further investigated this relationship in experimental settings in attempt to control for confounding variables like socio-economic status and parental involvment in out of school activities (Corrigall et al., 2013; Degé et al., 2011a; Schellenberg, 2006, 2011a, 2011b; Schellenberg and Mankarious, 2012), but findings have been mixed.</p>
<p>Schellenberg <span class="citation">(Schellenberg <a href="#ref-schellenbergMusicNonmusicalAbilities2017">2017</a>)</span> notes that in many of these studies there is a problem of too small of a sample size in his review (Corrigall and Trainor, 2011; Parbery-Clark et al., 2011; Strait, Parbery-Clark, Hittner and Kraus, 2012) in that studies that are typically smaller do not reach statistical significance.
Also referenced in Schellenberg’s review is evidence that when professional musicians are matched with non-musicians from the general population these associations are NON EXISTANT (CITE).
Interpreting the current literature Schellenberg’s review suggests researchers might consider the hypothesis that higher functioning kids that take music lessons and they tend to stay in lessons longer which leads to the observed differences in intelligence.
Additionally, Schellenberg remains skeptical of any sorts of causal factors regarding increases in IQ (e.g., François et al., 2013; Moreno et al., 2009) noting methodlogical problems like how short exposure times were in studies claiming increases in effects or researchers who not holding pre-exisiting cognitive abilitis constant (Mehr, Schachner, Katz and Spelke, 2013).</p>
<p>— there is a reverse here</p>
<p>So while the literatre above provides a body of evidence to suggest that musically trained individuals outperform their non-muscial peers– with the direction of causality yet to be firmly established– musical training is not and imporantly cannot be the sole determining factor in how well an individual remembers a melody.
Presumably other factors contribute in people’s ability to retain musical information.
One could consider a simple thought experiment where there might be people without musical training who, just due to their innate abilities, are able to outperform people with musical training based on their cognitive ability.</p>
<p>These are the first steps as well in any melodic dictation task.
this is cognitive ability, defined as XXXX to do task
But in addition to congitive ability, reason to believe that musical training will also affect this</p>
<p>So in order to get a better idea of melodic dictation, need to look at how most people do when having to do short term melody memory and eventually manipulate it.
In order to do this, need to look at variables that might be at play.</p>
<div id="musical-training-1" class="section level3">
<h3><span class="header-section-number">3.1.1</span> Musical Training</h3>
<p>Tacit in many pedagogical assumptions is that most people start out roughly with the same ability to remember musical material and that this capacity increases with musical training as noted above.
Karpinski defines this baseline limit of the amount of melodic information that can enter <em>short-term melodic memory</em> as “somewhere between five and nine notes”, referencing Miller’s magic number seven<a href="#fn11" class="footnote-ref" id="fnref11"><sup>11</sup></a> <span class="citation">(Miller <a href="#ref-millerMagicalNumberSeven1956">1956</a>)</span>.
Karpinski supports this claim with one experimental study by Marple (n.d.!!!) who corroborated Miller’s limits (p.78) and additionally noted that the binding of musical features beyond that of pitch extended the range to approximelty six to ten notes.
Similar findings were reported by Tallarico (XXX), Long (XXXX), and Pembrook (XXXX) who claimed the note limit to be between seven and eleven notes.<a href="#fn12" class="footnote-ref" id="fnref12"><sup>12</sup></a></p>
<p>In order to extend the capacity for musical memory, Karpinski puts forward two possible stragegies.
The first is extractive listening and the other is chunking.
Extrative listening is a processes described as …
Chunking on the other hand is a listener’s ability to group certain parts of sensory information together, so that more information can take up less of the finite space in short-term muscial memory.
According to Karpinkski, chunking increases with increases in musical understanding.</p>
<p>This is also backed up with a host of literature showing htat musicians tend to outperform
Especially when the task in musical
– look at these examples here</p>
<p>Taken as a whole, much of the literature could be interpreted to suggest that listeners begin as a tabula rasa, then as they engage with more musical activity, this activity affords them the ability to in perform better on musical tasks.
While this might seem like an intuitive explanation for the findings, three large problems exist with this interpretation.</p>
<p>The first is that how musicianship is measured varies from study to study, thus making interpreation of the data much harder.
In fact, music psychology in general suffers from having not standardized a way to measure musical enagement.
Talamini note this problem in the aforementioned meta-analysis saying that</p>
<blockquote>
<p>Quote</p>
</blockquote>
<p>and suggest that music psychology might consider moving towards more “catch all measures” of musicality such as the Goldsmiths Musical Sophistication Index CITE or the MUSEBAQ.
Both tools use latent variable approaches that attempt to smooth over some of the problems with measuring such a messy construct.
But one problem with adopting these tools is that it forces some sort of ontological commitment CITE FROM POT that has been critized by other CITE MEEE.</p>
<ul>
<li>but one problem is that maybe that there more domain general processes accounting for this</li>
<li>one way that other people have looked at it os OS</li>
<li>They think about it as LV with three separate sets they define as xyz</li>
<li>thing is, this is very close to WM (what they call separate)</li>
</ul>
<p>The second problem with interpreting the literature as practice driving all these effects is that this might confound the direction of causalitiy.
As noted above, most of the evidence suggesting a relationship here derrives from cross sectional designs and is limited in its ability to posit causal relationships.</p>
<p>Thirdly, these studies as a whole lack consistent control of covariates that might confound the findings (OS).
Factors such as … (Stuff from OS)</p>
<p>Considering these three confounds, it is only possible then to consider literature in one of three ways according to OS.</p>
<p>1
2
3</p>
<p>Thing that makes the mnost sense is probably what GS says that it exacerbates pre-existing differences.</p>
<p>but note a lot of this is general fluid intelligence, not WM!</p>
<p>So if this is the case, we need to look into other factors that might then also play a role</p>
<p>OS looked at this a bit ago with the Gold-MSI and the idea of executive functioning
– paper summary here</p>
<p>Here they break up EF into a few different categories,
– note here the parallels on WM and melodic dictation.</p>
<p>One of the three skills of EF is updating.
Defined as x
And has been conceptualized by some as WMC.
And if you think about it is basically MD.</p>
<p>In fact, this parallel is not even that new</p>
<p>Whether conceptualzed as the updating component of executive function (MIYAKE) or as working memory capacity (COWAN), working memory tasks share many similarities with tasks used in the music perception literature.
Berz <span class="citation">(Berz <a href="#ref-berzWorkingMemoryMusic1995">1995</a>)</span>, in his 1995 article pointed this out.</p>
<p>Looking at working memory we are not the first to point this out.
Berz did it here and noted that could just be wmc.
Berz contiues in vain of Baddely and Hitch
They solve the problem with having loops.
Berz continues in this tradiition by setting up another loop for music
But this is a problem for a few reasons.</p>
<p>if you define it as complex, need to be doing complex</p>
<p>For those reasons, decide instead to adopt the Embedded Process Model by Cowan.
It works like this.
And will eventually be more suited for what we are trying to do.
Also mention here the problem with Karpinski model that it doesnt say waht is actively rehearsed.</p>
<p>So the task now is now to investrigate how all of these factors come together.
First is to get a task that uses the first two steps of the Karpinski model that is accesible to the non-musically trained.
Then idea is to parse out the data to see which of the variables contributes to this task.</p>
<p>Having established a link between wmc tasks and md, need to now find a task that will mirror this bc it contributes
Link back to MMD!</p>
</div>
<div id="dictation-without-dictation" class="section level3">
<h3><span class="header-section-number">3.1.2</span> Dictation without Dictation</h3>
<p>One of the most popular paradigms used in music perception research, specifically the memory for melody literature, are same-different memory tasks <span class="citation">(Halpern and Bartlett <a href="#ref-halpernMemoryMelodies2010">2010</a>)</span>.
These tasks require individuals to hear a melody, retain it in memory, then hear a second melody either at the exact same pitch level, or transposed, and then make a judgment if the two melodies were the same or different.
It is important to note this type of task requires both rention of musical material and a secondary cognitive task: deciding if the melodies were identical or not.
In many ways, this type of task mirrors the first two steps of melodic dictation.
Same-different paradigms require individuals to first hear a finite amount of musical material, then while held in concious representation, perform some sort of mental action on the contents of memory by making a similarity judgment.
If one were to acknowledge these similariies in mental processes, using same-different paradigms could provide a way to investigate the first two steps of Karpinkski’s model of melodic dictation in the general population which may or may not have musical training.</p>
<p>Additionally, using a same different paradigm also enables the possibility to then look at which of the aforementioned factors best predict how individuals do in this process.
Interstingly, in contrast to results reported by <span class="citation">(Talamini et al. <a href="#ref-talaminiMusiciansHaveBetter2017">2017</a>)</span> looking at memory for more simple stimuli, <span class="citation">(Halpern and Bartlett <a href="#ref-halpernMemoryMelodies2010">2010</a>)</span> note that “although musical experts exceed nonexperts in some aspects of remembering music, frequently this outcome does not occur” when referring to more ecological settings.
The musician’s advantage dissapears.
Using this paradigm, it could then become possible to further look into what factors in addition to musical training best predict memory for melodies.</p>
</div>
<div id="cognitive-measures-of-interest" class="section level3">
<h3><span class="header-section-number">3.1.3</span> Cognitive Measures of interest</h3>
<p>Having previously established that many tests of musical ability and aptitude, may in fact be tests of working memory <span class="citation">(Berz <a href="#ref-berzWorkingMemoryMusic1995">1995</a>)</span>, one factor not yet accounted for in many of the measures of musical memory might be working memory.
If operationalized as conceptualized as Cowan above (CITE), one would have ot measure working memory using a task that uses both retention and manipulation of informaiton in memory, or a set of complex span tasks <span class="citation">(Unsworth et al. <a href="#ref-unsworthAutomatedVersionOperation2005">2005</a>)</span>.
Additionally, since general intelligence is often predictive of performance on a host of cognitive tasks LIST and has been theoretically related to working memory <span class="citation">(Kovacs and Conway <a href="#ref-kovacsProcessOverlapTheory2016">2016</a>)</span>, this measure should also be accounted for.
Finally, in response to claims made by <span class="citation">(Okada and Slevc <a href="#ref-okadaIndividualDifferencesMusical2018a">2018</a>)</span> having to need to account for specific covariates, it would also be good to keep track of things like socioeconomic status, degree of education, and OTHER VARIABLES.</p>
</div>
<div id="path-analysis-here" class="section level3">
<h3><span class="header-section-number">3.1.4</span> PATH Analysis here</h3>
<p>Given the complex nature being investigated and the theoretical concepts at play such as working memory, general fluid intelligence, and musicial sophisticaiton conceputalized as a latent variable, it follows that the most appropriate method of parsing out the variance in this covariance structure would be to use some form of path analysis.
Path analysis is a type of analysis developed by XXXX which orignally assumed a closed algerbric system which could be used to parse out causal structures amongst covariance relationships WHY.
The sets of variables presented do not make up a closed system as orignally devided by XXX in his investigation of the heretibility of guinia pig traints PEARL PAGE, path analysis using structural equation modeling does allow insight into the degree that variables of interest contribute to complex causal relationships.</p>
</div>
<div id="hypotheses" class="section level3">
<h3><span class="header-section-number">3.1.5</span> Hypotheses</h3>
<p>If I then assume that a same-different melodic memory paradigm is a stable proxy for the first two steps of Karpinski’s model of melodic dictation, then data generated from both objective tests of the Goldsmiths’ Musical Sophistication Index can serve as proxy for this measure of interst.
In this analyses, I will use a series of structural equation models in order to investigate how various individual factors contribute to an individual’s memory for melody.
Following a stepwise proceedure LIKE EE, these sets of analyses will provide a way what individual factors need to be accounted for in future research.</p>
<p>Given a robust instrument for measuring musciality, and two well established cognive measrues as specifically defined, this study analysis seeks to investigate the degree to which these individual level variables are predictive of a task that is proxy to the first two steps of melodic dictation.</p>
<p>If a large proportion of the variance of musical memory can be attributed to training, then variables related to the Goldsmiths Musical Sophisitication Index should be most predictive with the highest path coeffecients and lead to the best model fit.
If instead cognitive factors do play a role, this should be evident in the path loadings.
Not an either or, more like a both.</p>
</div>
</div>
<div id="overview-of-experiment" class="section level2">
<h2><span class="header-section-number">3.2</span> Overview of Experiment</h2>
<div id="participants" class="section level3">
<h3><span class="header-section-number">3.2.1</span> Participants</h3>
<p>Two hundred fifty-four students enrolled at Louisiana State University completed the study.
We recruited students, mainly in the Department of Psychology and the School of Music.
The criteria for inclusion in the analysis were no self-reported hearing loss, not actively taking medication that would alter cognitive performance, and univariate outliers (defined as individuals whose performance on any task was greater than 3 standard deviations from the mean score of that task).
Using these criteria, eight participants were not eligible due to self reporting hearing loss, one participant was removed for age, and six participants were eliminated as univariate outliers due to performance on one or more of the tasks of working memory capacity.
Thus, 239 participants met the criteria for inclusion. The eligible participants were between the ages of 17 and 43 (M = 19.72, SD = 2.74; 148 females).
Participants volunteered, received course credit, or were paid $20.</p>
</div>
<div id="materials" class="section level3">
<h3><span class="header-section-number">3.2.2</span> Materials</h3>
<p>Cognitive Measures</p>
<p>All variables used for modeling approximated normal distributions.
Processing errors for each task were positively skewed for the complex span tasks similar to Unsworth, Redick, Heitz, Broadway, and Engle (2009).
Positive and significant correlations were found between recall scores on the three tasks measuring working memory capacity (WMC) and the two measuring general fluid intelligence (Gf).
The WMC recall scores negatively correlated with the reported number of errors in each task, suggesting that rehearsal processes were effectively limited by the processing tasks (Unsworth et al., 2009).</p>
<div id="measures" class="section level4">
<h4><span class="header-section-number">3.2.2.1</span> Measures</h4>
<div id="goldsmiths-musical-sophistication-index-self-report-gold-msi" class="section level5">
<h5><span class="header-section-number">3.2.2.1.1</span> Goldsmiths Musical Sophistication Index Self Report (Gold-MSI)</h5>
<p>Participants completed a 38-item self-report inventory and questions consisted of free response answers or choosing a
selection on a likert scale that ranged from 1-7. (Müllensiefen, et al., 2014).
The complete survey with all questions used can be found at goo.gl/dqtSaB.</p>
</div>
<div id="tone-span-tspan" class="section level5">
<h5><span class="header-section-number">3.2.2.1.2</span> Tone Span (TSPAN)</h5>
<p>Participants completed a two-step math operation and then tried to remember three different tones in an alternating sequence (based upon Unsworth et al., 2005).
We modelled the three tones after Li, Cowan, Saults (2005) paper using frequencies outside of the equal tempered system (200Hz, 375Hz, 702Hz).
The same math operation procedure as OSPAN was used. The tones was presented aurally for 1000ms after each math operation. During tone recall, participants were presented three different options H M and L (High, Medium, and Low), each with its own check box.
Tones were recalled in serial order by clicking on each tone’s box in the appropriate order.
Tone recall was untimed.
Participants were provided practice trials and similar to OSPAN, the test procedure included three trials of each list length (3-7 tones), totalling 75 letters and 75 math operations.</p>
</div>
<div id="operation-span-ospan" class="section level5">
<h5><span class="header-section-number">3.2.2.1.3</span> Operation Span (OSPAN)</h5>
<p>Participants completed a two-step math operation and then tried to remember a letter (F, H, J, K, L, N, P, Q, R, S, T, or
Y) in an alternating sequence (Unsworth et al., 2005).
The same math operation procedure as TSPAN was used.
The letter was presented visually for 1000ms after each math
operation.
During letter recall, participants saw a 4 x 3 matrix of all possible letters, each with its own check box.
Letters were recalled in serial order by clicking on each letter’s box in the appropriate order.
Letter recall was untimed.
Participants were provided practice trials and similar to TSPAN, the test procedure included three trials of each list length (3-7 letters), totalling 75 letters and 75 math operations.</p>
</div>
<div id="symmetry-span-sspan" class="section level5">
<h5><span class="header-section-number">3.2.2.1.4</span> Symmetry Span (SSPAN)</h5>
<p>Participants completed a two-step symmetry judgment and were prompted to recall a visually-presented red square on a 4 X 4 matrix (Unsworth et al., 2005).
In the symmetry judgment, participants were shown an 8 x 8 matrix with random squares filled in black.
Participants had to decide if the black squares were symmetrical about the matrix’s vertical axis and then click the screen.
Next, they were shown a “yes” and “no” box and clicked on the appropriate box.
Participants then saw a 4 X 4 matrix for 650 ms with one red square after each symmetry judgment.
During square recall, participants recalled the location of each red square by clicking on the appropriate cell in serial order.
Participants were provided practice trials to become familiar with the procedure.
The test procedure included three trials of each list length (2-5 red squares), totalling 42 squares and 42 symmetry judgments.</p>
</div>
<div id="gold-msi-beat-perception" class="section level5">
<h5><span class="header-section-number">3.2.2.1.5</span> Gold-MSI Beat Perception</h5>
<p>Participants were presented 18 excerpts of instrumental music from rock, jazz, and classical genres (Müllensiefen et
al., 2014).
Each excerpt was presented for 10 to 16s through headphones and had a tempo ranging from 86 to 165 beats per
minute.
A metronomic beep was played over each excerpt either on or off the beat.
Half of the excerpts had a beep on the beat, and the other half had a beep off the beat.
After each excerpt was played, participants answered if the metronomic beep was on or off the beat and provided their confidence: “I
am sure”, “I am somewhat sure”, or “I am guessing”.
The final score was the proportion of correct responses on the beat judgment.</p>
</div>
<div id="gold-msi-melodic-memory-test" class="section level5">
<h5><span class="header-section-number">3.2.2.1.6</span> Gold-MSI Melodic Memory Test</h5>
<p>Participants were presented melodies between 10 to 17 notes long through headphones (Müllensiefen et al., 2014).
There were 12 trials, half with the same melody and half with different melodies.
During each trial, two versions of a melody were presented.
The second version was transposed to a different key.
In half of the second version melodies, a note was changed a step up or down from its original position in the structure of the melody.
After each trial, participants answered if the two melodies had identical pitch interval structures.</p>
</div>
<div id="number-series" class="section level5">
<h5><span class="header-section-number">3.2.2.1.7</span> Number Series</h5>
<p>Participants were presented with a series of numbers with
an underlying pattern.
After being given two example problems to solve, participants had 4.5 minutes in order to solve 15 different problems.
Each trial had 5 different options as possible answers (Thurstone, 1938).</p>
</div>
<div id="ravens-advanced-progressive-matrices" class="section level5">
<h5><span class="header-section-number">3.2.2.1.8</span> Raven’s Advanced Progressive Matrices</h5>
<p>Participants were presented a 3 x 3 matrix of geometric patterns with one pattern missing (Raven et al., 1998).
Up to eight pattern choices were given at the bottom of the screen.
Participants had to click the choice that correctly fit the pattern above.
There were three blocks of 12 problems, totalling 36 problems.
The items increased in difficulty across each block.
A maximum of 5 min was allotted for each block, totalling 15 min.
The final score was the total number of correct responses across the three blocks.</p>
</div>
</div>
</div>
<div id="procedure" class="section level3">
<h3><span class="header-section-number">3.2.3</span> Procedure</h3>
<p>Participants in this experiment completed eight different tasks, lasting about 90 minutes in duration.
The tasks consisted of the Gold-MSI self-report inventory, coupled with the Short Test of Musical Preferences, and a supplementary demographic questionnaire that included questions about socioeconomic status, aural skills history, hearing loss, and any medication that might affect their ability to perform on cognitive tests.
Following the survey they completed three WMC tasks: a novel Tonal Span, Symmetry span, and Operation span task; a battery of perceptual tests from the Gold-MSI (Melodic Memory, Beat Perception, Sound Similarity) and two tests of general fluid intelligence (Gf): Number Series and Raven’s Advanced Progressive Matrices.</p>
<p>Each task was administered in the order listed above on a desktop computer.
Sounds were presented at a comfortable listening level for the tasks that required headphones.
All participants provided informed consent and were debriefed.
Only measures used in modeling are reported below.</p>
</div>
<div id="results" class="section level3">
<h3><span class="header-section-number">3.2.4</span> Results</h3>
<div id="descriptive-data-screening-correlational" class="section level4">
<h4><span class="header-section-number">3.2.4.1</span> Descriptive, Data Screening, Correlational</h4>
<p>The goal of the analyses was to examine the relationships among the measures and constructs of WMC, general fluid intelligence, musical sophistication (operationalized as the General score from the Gold-MSI), in relation to the two objective listening tests on the Gold-MSI.
Before running any sort of modeling, we inspected our data to ensure in addition to outlier issues as mentioned above, the data exhibited normal distributions.
We report both our correlation values, as well as visually displaying our distributions in Figure 1.</p>
<p>Before running any modeling, we checked our data for assumptions of normality since violations of normality can strongly affect the covariances between items.
While some items in Figure 1 displayed a negative skew, many of the individual level items from the self report scale exhibited high
levels of Skew and Kurtosis beyond the generally accepted ± 2 (Field, Miles, &amp; Field, 2012), but none of the items with the unsatisfactory measures are used in the general factor.</p>
</div>
<div id="modeling" class="section level4">
<h4><span class="header-section-number">3.2.4.2</span> Modeling</h4>
<div id="measurement-model" class="section level5">
<h5><span class="header-section-number">3.2.4.2.1</span> Measurement Model</h5>
<p>We then fit a measurement model to examine the underlying structure of the variables of interest used to assess the latent constructs (general musical sophistication, WMC, general fluid intelligence) by performing a confirmatory factor analysis (CFA) using the lavaan package (Rosseel, 2013) using R (R Core Team, 2017).
Model fits in can be found in Table 3.
For each model, latent factors were constrained to have a mean of 0 and variance of 1 in order to allow the latent covariances to be interpreted as correlations.
Since the objective measures were on different scales, all variables were converted to z scores before running any modeling.</p>
<ul>
<li>MODEL HERE</li>
</ul>
<p>Variables are defined as follows: gen: general factor latent variable; wmc: working memory capacity; gf: general fluid intelligence; zIS: “Identify What is Special”; zHO: “Hear Once Sing Back”; zSB: “Sing Back After 2-3”; zDS: “Don’t Sing In Public”; zSH: “Sing In Harmony”; zJI:”Join In”; zNI: “Number of Instrumetns”; zRP:”Regular Practice”; zNCS: “Not Consider Self Musician”; zNcV: “Never Complimented”; zST: “Self Tonal”; zCP: “Compare Performances”; zAd: “Addiction”; zSI: “Search Internet”; zWr: “Writing About Music”; zFr: “Free Time”; zTP: “Tone Span”; zMS: “Symmetry Span”; zMO: “Operation Span”; zRA: “Ravens”; zAN: “Number Series”.</p>
</div>
</div>
<div id="structural-equation-models" class="section level4">
<h4><span class="header-section-number">3.2.4.3</span> Structural Equation Models</h4>
<p>Following the initial measurement model, we then fit a series of SEMs in order to investigate both the degree to which factor loadings changed when variables were removed from the model as well as the model fits.
We began with a model incorporating our three latent variables (general musical sophistication, WMC, general fluid intelligence) predicting our two objective measures (beat perception and melodic memory scores) and then detailed steps we took in order to improve model fit.
For each model, we calculated four model fits: χ2 , comparative fit index (CFI), root mean square error (RMSEA), and Tucker Lewis Index (TLI).
In general, a non-significant χ2 indicates good model fit, but is overly sensitive to sample size.
Comparative Fit Index (CFI) values of .95 or higher are considered to be indicative of good model fits as well as Root Mean Square Error (RMSEA) values of .06 or lower, Tucker Lewis Index (TLI) values closer to 1 indicate a better fit. (Beajean, 2014).</p>
<p>After running the first model (Model 1), we then examined the residuals between the correlation matrix the model expects and our actual correlation matrix looking for residuals
above .1.
While some variables scored near .1, two items dealing with being able to sing (“I can hear a melody once and sing it back after hearing it 2 – 3 times” and “I can hear a melody once and sing it back”) exhibited a high level of correlation amongst the residuals (.41) and were removed for Model 2 and model fit improved significantly (χ2 (41)=123.39,
p &lt; . 001).</p>
<p>After removing the poorly fitting items, we then proceeded to examine if removing the general musical sophistication self-report measures would significantly improve model fit for Model 3.
Fit measures for Model 3 can be seen in Table 3 and removing the self-report items resulted in a significantly better model fit (χ2 (171)=438.8, p &lt; . 001).
Following the rule of thumb that at least 3 variables should be used to define any latent-variable (Beajuean, 2014) we modelled WMC as latent variable and Gf as a composite average of the two tasks administered in order to improve model fit.
This model resulted in significant improvement to the model (χ2 (4)=14.37, p &lt; . 001).
Finally we examined the change in test statistics between Model 2 and a model that removed the cognitive measures– a model akin to one of the original models reported in Müllensiefen et al., (2014)– for Model 5.
Testing between the two models resulted in a significant improvement in model fit (χ2 (78)=104.75, p &lt; . 001).
Figure 3 displays Model 4, our nested model with the best fit indices.</p>
<ul>
<li><p>TABLE HERE</p></li>
<li><p>ALL FIGURES OF MODELS</p></li>
</ul>
</div>
</div>
<div id="discussion" class="section level3">
<h3><span class="header-section-number">3.2.5</span> Discussion</h3>
<div id="measurement-model-1" class="section level5">
<h5><span class="header-section-number">3.2.5.0.1</span> Measurement Model</h5>
<p>After running a confirmatory factor analysis on the variables of interest, the model fit was below the threshold of what is considered a “good model fit” as shown in Table 1 with references to above model fits.
This finding is to be expected since no clear theoretical model has been put forward that would suggest that the general musical sophistication score, when modelled with two cognitive measures should have a good model fit.
This model was run to create a baseline measurement.</p>
</div>
<div id="structural-equation-model-fitting" class="section level5">
<h5><span class="header-section-number">3.2.5.0.2</span> Structural Equation Model Fitting</h5>
<p>Following a series of nested model fits, we were able to improve model fits on a series of SEMs that incorporated both measures of WMC and measures of general fluid intelligence.
Before commenting on new models, it is worth noting that the Model 5 does not seem to align with the findings from the original 2014 paper by Müllensiefen et al.
While the correlation between the objective tasks is the same (r = .16), the factor loadings from this paper suggest lower values for both Beat Perception (.37 original, .27 this paper) as well as Melodic Memory (.28 original, .18 this paper).
Note that two items were removed dealing with melody for memory for this model; when those items were re-run with the data, the factor
loadings did not deviate from these numbers.</p>
<p>The first two models we ran resulted in minor improvements to model fit.
While difference in models was significant (χ2 (41)=123.39, p &lt; . 001 ), probably due to the
number of parameters that were now not constrained, the
relative fit indices of the models did not change drastically.
It was not until the self-report measures were removed from the model, and then manipulated according to latent variable modeling recommendations, was there a marked increase in the relative fit indices.
Fitting the model with only the cognitive measures, we were able to enter the bounds of acceptable relative fit indices that were noted above.
In order to find evidence that the cognitive models (Models 3 and 4) were indeed a better fit than using the General factor, we additionally ran a comparison between our adjusted measurement model and a model with only the self report.
While both of our nested models were significantly different, the cognitive models exhibited superior relative fit indices.
Lastly, turning to Figure 3, we note that our latent variable of WMC exhibited much larger factor loadings predicting the two objective, perceptual tests than our measure of general fluid intelligence.
We also note that the factor loading predicting the Beat Perception task (.36) was higher than that of the Melodic Memory task (.21). These rankings mirror that of the original Müllensiefen et al., (2014) paper and merit further examination in order to disentangle what processes are contributing to both tasks.</p>
<p>These results align with predictions made with Process Overlap Theory (Kovacs &amp; Conway,
2016), which predict that higher executive loads are needed
for tasks of perception.
While we failed to predict which task would load higher –we assumed that the ability to maintain and manipulate information in the Melodic Memory task would be better predicted by WMC than the Beat Perception task– this might be due to the fact that performing well in a melodic memory task demands a certain amount of musical training that is not captured by either cognitive measure.
In the future, we are interested in exploring more theoretically-driven models that use specific, task oriented predictors in order to explain the relationships between the perceptual tasks and the cognitive measures.
Given the results here that suggest that measures of cognitive ability play a significant role in tasks of musical perception, we suggest that future research should consider taking measures of cognitive ability into account, so that other variables of interest are able to be shown to contribute above and beyond baseline cognitive measures.</p>
<p>In this paper we fit a series of structural equation models in order to investigate the degree to which baseline cognitive ability was able to predict performance on a musical perception task.
Our findings suggest that measures of WMC are able to account for a large amount of variance beyond that of self report in tasks of musical perception.</p>

</div>
</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-karpinskiAuralSkillsAcquisition2000">
<p>Karpinski, Gary Steven. 2000. <em>Aural Skills Acquisition: The Development of Listening, Reading, and Performing Skills in College-Level Musicians</em>. Oxford University Press.</p>
</div>
<div id="ref-karpinskiModelMusicPerception1990">
<p>Karpinski, Gary. 1990. “A Model for Music Perception and Its Implications in Melodic Dictation.” <em>Journal of Music Theory Pedagogy</em> 4 (1): 191–229.</p>
</div>
<div id="ref-schellenbergMusicNonmusicalAbilities2017">
<p>Schellenberg, E Glenn. 2017. “Music and Nonmusical Abilities,” 17.</p>
</div>
<div id="ref-talaminiMusiciansHaveBetter2017">
<p>Talamini, Francesca, Gianmarco Altoè, Barbara Carretti, and Massimo Grassi. 2017. “Musicians Have Better Memory Than Nonmusicians: A Meta-Analysis.” Edited by Lutz Jäncke. <em>PLOS ONE</em> 12 (10): e0186773. <a href="https://doi.org/10.1371/journal.pone.0186773" class="uri">https://doi.org/10.1371/journal.pone.0186773</a>.</p>
</div>
<div id="ref-millerMagicalNumberSeven1956">
<p>Miller, George A. 1956. “The Magical Number Seven, Plus or Minus Two: Some Limits on Our Capacity for Processing Information.”</p>
</div>
<div id="ref-berzWorkingMemoryMusic1995">
<p>Berz, William L. 1995. “Working Memory in Music: A Theoretical Model.” <em>Music Perception: An Interdisciplinary Journal</em> 12 (3): 353–64. <a href="https://doi.org/10.2307/40286188" class="uri">https://doi.org/10.2307/40286188</a>.</p>
</div>
<div id="ref-halpernMemoryMelodies2010">
<p>Halpern, Andrea R., and James C. Bartlett. 2010. “Memory for Melodies.” In <em>Music Perception</em>, edited by Mari Riess Jones, Richard R. Fay, and Arthur N. Popper, 36:233–58. New York, NY: Springer New York. <a href="https://doi.org/10.1007/978-1-4419-6114-3_8" class="uri">https://doi.org/10.1007/978-1-4419-6114-3_8</a>.</p>
</div>
<div id="ref-unsworthAutomatedVersionOperation2005">
<p>Unsworth, Nash, Richard P. Heitz, Josef C. Schrock, and Randall W. Engle. 2005. “An Automated Version of the Operation Span Task.” <em>Behavior Research Methods</em> 37 (3): 498–505. <a href="https://doi.org/10.3758/BF03192720" class="uri">https://doi.org/10.3758/BF03192720</a>.</p>
</div>
<div id="ref-kovacsProcessOverlapTheory2016">
<p>Kovacs, Kristof, and Andrew R. A. Conway. 2016. “Process Overlap Theory: A Unified Account of the General Factor of Intelligence.” <em>Psychological Inquiry</em> 27 (3): 151–77. <a href="https://doi.org/10.1080/1047840X.2016.1153946" class="uri">https://doi.org/10.1080/1047840X.2016.1153946</a>.</p>
</div>
<div id="ref-okadaIndividualDifferencesMusical2018a">
<p>Okada, Brooke M., and L. Robert Slevc. 2018. “Individual Differences in Musical Training and Executive Functions: A Latent Variable Approach.” <em>Memory &amp; Cognition</em> 46 (7): 1076–92. <a href="https://doi.org/10.3758/s13421-018-0822-8" class="uri">https://doi.org/10.3758/s13421-018-0822-8</a>.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="9">
<li id="fn9"><p>This whole model needs critique under the WMC literature. It’s kind of strange to think that the act of something hitting your ear is different than attention (the way that Cowan thinks about WMC and again that you can split up the representation in memory from that of what the characteristics are of the melody like the meter and scale degrees, which have been argued to be part of intrinsic qualia) also there is a big problem here about stuff being actively rehearsed or not<a href="individual-differences.html#fnref9" class="footnote-back">↩</a></p></li>
<li id="fn10"><p>Could drop the stuff on intelligence<a href="individual-differences.html#fnref10" class="footnote-back">↩</a></p></li>
<li id="fn11"><p>Should I footnote here saying why this was not meant to be taken literally?<a href="individual-differences.html#fnref11" class="footnote-back">↩</a></p></li>
<li id="fn12"><p>For reasons noted in previous chapters (SECTION), it is important to stress the maximum about of notes able to be memorized should not be direclty interpreted as a one to one mapping of the amount of items that can be held in short term memory.
Breifly reviewing earlier points, the first reason is that Miller’s number 7 was never meant to be taken literally as a number used to investigate the items of memory, but rather a rheatorical device for a keynote address he was asked to give.
Secondly, lieterature on the capacity limits of memory need to account for chunking mechanisms, most of which are bountiful in the musical domain as noted in SECTION.<a href="individual-differences.html#fnref12" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="intro.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="computation-chapter.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/02-individual-parameters.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": ["mmd-draft-djb.pdf", "mmd-draft-djb.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
