<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Chapter 6 Experiment | MODELING MELODIC DICTATION</title>
  <meta name="description" content="This dissertation models both individual and musical features that contribute to processes involved in melodic dictation.">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Chapter 6 Experiment | MODELING MELODIC DICTATION" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This dissertation models both individual and musical features that contribute to processes involved in melodic dictation." />
  <meta name="github-repo" content="davidjohnbaker1/document" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 6 Experiment | MODELING MELODIC DICTATION" />
  
  <meta name="twitter:description" content="This dissertation models both individual and musical features that contribute to processes involved in melodic dictation." />
  

<meta name="author" content="A Dissertation">
<meta name="author" content="Submitted to the Graduate Faculty of the Louisiana State University and Agricultural and Mechanical College in partial fulfillment of the requirements for the degree of Doctor of Philosophy">
<meta name="author" content="in">
<meta name="author" content="The School of Music">
<meta name="author" content="by David John Baker">
<meta name="author" content="B.M., Baldwin Wallace University, 2012">
<meta name="author" content="MSc., Goldsmiths, University of London, 2015">
<meta name="author" content="May 2019">


<meta name="date" content="2019-04-15">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="chapterfour.html">
<link rel="next" href="computational-model.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Modeling Melodic Dictation</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Significance of the Study</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#rationale"><i class="fa fa-check"></i><b>1.1</b> Rationale</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#chapter-overview"><i class="fa fa-check"></i><b>1.2</b> Chapter Overview</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Context, Literature, and Rationale</a><ul>
<li class="chapter" data-level="2.1" data-path="intro.html"><a href="intro.html#melodic-dictation"><i class="fa fa-check"></i><b>2.1</b> Melodic Dictation</a><ul>
<li class="chapter" data-level="2.1.1" data-path="intro.html"><a href="intro.html#describing-melodic-dictation"><i class="fa fa-check"></i><b>2.1.1</b> Describing Melodic Dictation</a></li>
<li class="chapter" data-level="2.1.2" data-path="intro.html"><a href="intro.html#taxonomizing"><i class="fa fa-check"></i><b>2.1.2</b> Taxonomizing</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="intro.html"><a href="intro.html#individual-factors"><i class="fa fa-check"></i><b>2.2</b> Individual Factors</a><ul>
<li class="chapter" data-level="2.2.1" data-path="intro.html"><a href="intro.html#cognitive"><i class="fa fa-check"></i><b>2.2.1</b> Cognitive</a></li>
<li class="chapter" data-level="2.2.2" data-path="intro.html"><a href="intro.html#environmental"><i class="fa fa-check"></i><b>2.2.2</b> Environmental</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="intro.html"><a href="intro.html#musical-factors"><i class="fa fa-check"></i><b>2.3</b> Musical Factors</a><ul>
<li class="chapter" data-level="2.3.1" data-path="intro.html"><a href="intro.html#the-notes"><i class="fa fa-check"></i><b>2.3.1</b> “The Notes”</a></li>
<li class="chapter" data-level="2.3.2" data-path="intro.html"><a href="intro.html#abstracted-features"><i class="fa fa-check"></i><b>2.3.2</b> Abstracted Features</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="intro.html"><a href="intro.html#polymorphism-of-ability"><i class="fa fa-check"></i><b>2.4</b> Polymorphism of Ability</a></li>
<li class="chapter" data-level="2.5" data-path="intro.html"><a href="intro.html#conclusions"><i class="fa fa-check"></i><b>2.5</b> Conclusions</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="individual-differences.html"><a href="individual-differences.html"><i class="fa fa-check"></i><b>3</b> Individual Differences</a><ul>
<li class="chapter" data-level="3.1" data-path="individual-differences.html"><a href="individual-differences.html#rationale-1"><i class="fa fa-check"></i><b>3.1</b> Rationale</a></li>
<li class="chapter" data-level="3.2" data-path="individual-differences.html"><a href="individual-differences.html#individual-differences-1"><i class="fa fa-check"></i><b>3.2</b> Individual Differences</a><ul>
<li class="chapter" data-level="3.2.1" data-path="individual-differences.html"><a href="individual-differences.html#improving-musical-memory"><i class="fa fa-check"></i><b>3.2.1</b> Improving Musical Memory</a></li>
<li class="chapter" data-level="3.2.2" data-path="individual-differences.html"><a href="individual-differences.html#memory-for-melodies"><i class="fa fa-check"></i><b>3.2.2</b> Memory for Melodies</a></li>
<li class="chapter" data-level="3.2.3" data-path="individual-differences.html"><a href="individual-differences.html#musicians-cognitive-advantage"><i class="fa fa-check"></i><b>3.2.3</b> Musician’s Cognitive Advantage</a></li>
<li class="chapter" data-level="3.2.4" data-path="individual-differences.html"><a href="individual-differences.html#relationship-established"><i class="fa fa-check"></i><b>3.2.4</b> Relationship Established</a></li>
<li class="chapter" data-level="3.2.5" data-path="individual-differences.html"><a href="individual-differences.html#dictation-without-dictation"><i class="fa fa-check"></i><b>3.2.5</b> Dictation Without Dictation</a></li>
<li class="chapter" data-level="3.2.6" data-path="individual-differences.html"><a href="individual-differences.html#cognitive-measures-of-interest"><i class="fa fa-check"></i><b>3.2.6</b> Cognitive Measures of Interest</a></li>
<li class="chapter" data-level="3.2.7" data-path="individual-differences.html"><a href="individual-differences.html#structural-equation-modeling"><i class="fa fa-check"></i><b>3.2.7</b> Structural Equation Modeling</a></li>
<li class="chapter" data-level="3.2.8" data-path="individual-differences.html"><a href="individual-differences.html#hypotheses"><i class="fa fa-check"></i><b>3.2.8</b> Hypotheses</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="individual-differences.html"><a href="individual-differences.html#overview-of-experiment"><i class="fa fa-check"></i><b>3.3</b> Overview of Experiment</a><ul>
<li class="chapter" data-level="3.3.1" data-path="individual-differences.html"><a href="individual-differences.html#participants"><i class="fa fa-check"></i><b>3.3.1</b> Participants</a></li>
<li class="chapter" data-level="3.3.2" data-path="individual-differences.html"><a href="individual-differences.html#materials"><i class="fa fa-check"></i><b>3.3.2</b> Materials</a></li>
<li class="chapter" data-level="3.3.3" data-path="individual-differences.html"><a href="individual-differences.html#procedure"><i class="fa fa-check"></i><b>3.3.3</b> Procedure</a></li>
<li class="chapter" data-level="3.3.4" data-path="individual-differences.html"><a href="individual-differences.html#results"><i class="fa fa-check"></i><b>3.3.4</b> Results</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="individual-differences.html"><a href="individual-differences.html#discussion"><i class="fa fa-check"></i><b>3.4</b> Discussion</a><ul>
<li class="chapter" data-level="3.4.1" data-path="individual-differences.html"><a href="individual-differences.html#model-fits"><i class="fa fa-check"></i><b>3.4.1</b> Model Fits</a></li>
<li class="chapter" data-level="3.4.2" data-path="individual-differences.html"><a href="individual-differences.html#relating-to-melodic-dictation"><i class="fa fa-check"></i><b>3.4.2</b> Relating to Melodic Dictation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="computation-chapter.html"><a href="computation-chapter.html"><i class="fa fa-check"></i><b>4</b> Computation Chapter</a><ul>
<li class="chapter" data-level="4.1" data-path="computation-chapter.html"><a href="computation-chapter.html#rationale-2"><i class="fa fa-check"></i><b>4.1</b> Rationale</a></li>
<li class="chapter" data-level="4.2" data-path="computation-chapter.html"><a href="computation-chapter.html#agreeing-on-complexity"><i class="fa fa-check"></i><b>4.2</b> Agreeing on Complexity</a><ul>
<li class="chapter" data-level="4.2.1" data-path="computation-chapter.html"><a href="computation-chapter.html#methods"><i class="fa fa-check"></i><b>4.2.1</b> Methods</a></li>
<li class="chapter" data-level="4.2.2" data-path="computation-chapter.html"><a href="computation-chapter.html#agreement-among-peagogues"><i class="fa fa-check"></i><b>4.2.2</b> Agreement Among Peagogues</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="computation-chapter.html"><a href="computation-chapter.html#modeling-complexity"><i class="fa fa-check"></i><b>4.3</b> Modeling Complexity</a><ul>
<li class="chapter" data-level="4.3.1" data-path="computation-chapter.html"><a href="computation-chapter.html#what-are-features"><i class="fa fa-check"></i><b>4.3.1</b> What Are Features?</a></li>
<li class="chapter" data-level="4.3.2" data-path="computation-chapter.html"><a href="computation-chapter.html#back-to-the-classroom"><i class="fa fa-check"></i><b>4.3.2</b> Back to the Classroom</a></li>
<li class="chapter" data-level="4.3.3" data-path="computation-chapter.html"><a href="computation-chapter.html#dynamic"><i class="fa fa-check"></i><b>4.3.3</b> Dynamic</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="computation-chapter.html"><a href="computation-chapter.html#frequency-facilitation-hypothesis"><i class="fa fa-check"></i><b>4.4</b> Frequency Facilitation Hypothesis</a><ul>
<li class="chapter" data-level="4.4.1" data-path="computation-chapter.html"><a href="computation-chapter.html#corpus-analysis"><i class="fa fa-check"></i><b>4.4.1</b> Corpus Analysis</a></li>
<li class="chapter" data-level="4.4.2" data-path="computation-chapter.html"><a href="computation-chapter.html#implications"><i class="fa fa-check"></i><b>4.4.2</b> Implications</a></li>
<li class="chapter" data-level="4.4.3" data-path="computation-chapter.html"><a href="computation-chapter.html#limitations-of-frequency-facilitation-hypothesis"><i class="fa fa-check"></i><b>4.4.3</b> Limitations of Frequency Facilitation Hypothesis</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="computation-chapter.html"><a href="computation-chapter.html#conclusions-1"><i class="fa fa-check"></i><b>4.5</b> Conclusions</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="chapterfour.html"><a href="chapterfour.html"><i class="fa fa-check"></i><b>5</b> Hello, Corpus</a><ul>
<li class="chapter" data-level="5.1" data-path="chapterfour.html"><a href="chapterfour.html#rationale-3"><i class="fa fa-check"></i><b>5.1</b> Rationale</a></li>
<li class="chapter" data-level="5.2" data-path="chapterfour.html"><a href="chapterfour.html#history"><i class="fa fa-check"></i><b>5.2</b> History</a></li>
<li class="chapter" data-level="5.3" data-path="chapterfour.html"><a href="chapterfour.html#melosol-corpus"><i class="fa fa-check"></i><b>5.3</b> MeloSol Corpus</a></li>
<li class="chapter" data-level="5.4" data-path="chapterfour.html"><a href="chapterfour.html#comparison-of-corpora"><i class="fa fa-check"></i><b>5.4</b> Comparison of Corpora</a><ul>
<li class="chapter" data-level="5.4.1" data-path="chapterfour.html"><a href="chapterfour.html#corpus-analysis-1"><i class="fa fa-check"></i><b>5.4.1</b> Corpus Analysis</a></li>
<li class="chapter" data-level="5.4.2" data-path="chapterfour.html"><a href="chapterfour.html#discussion-1"><i class="fa fa-check"></i><b>5.4.2</b> Discussion</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="experiment.html"><a href="experiment.html"><i class="fa fa-check"></i><b>6</b> Experiment</a><ul>
<li class="chapter" data-level="6.1" data-path="experiment.html"><a href="experiment.html#rationale-4"><i class="fa fa-check"></i><b>6.1</b> Rationale</a></li>
<li class="chapter" data-level="6.2" data-path="experiment.html"><a href="experiment.html#introduction"><i class="fa fa-check"></i><b>6.2</b> Introduction</a><ul>
<li class="chapter" data-level="6.2.1" data-path="experiment.html"><a href="experiment.html#memory-for-melodies-1"><i class="fa fa-check"></i><b>6.2.1</b> Memory for Melodies</a></li>
<li class="chapter" data-level="6.2.2" data-path="experiment.html"><a href="experiment.html#musical-factors-1"><i class="fa fa-check"></i><b>6.2.2</b> Musical Factors</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="experiment.html"><a href="experiment.html#methods-1"><i class="fa fa-check"></i><b>6.3</b> Methods</a><ul>
<li class="chapter" data-level="6.3.1" data-path="experiment.html"><a href="experiment.html#participants-1"><i class="fa fa-check"></i><b>6.3.1</b> Participants</a></li>
<li class="chapter" data-level="6.3.2" data-path="experiment.html"><a href="experiment.html#materials-1"><i class="fa fa-check"></i><b>6.3.2</b> Materials</a></li>
<li class="chapter" data-level="6.3.3" data-path="experiment.html"><a href="experiment.html#procedure-1"><i class="fa fa-check"></i><b>6.3.3</b> Procedure</a></li>
<li class="chapter" data-level="6.3.4" data-path="experiment.html"><a href="experiment.html#scoring-melodies"><i class="fa fa-check"></i><b>6.3.4</b> Scoring Melodies</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="experiment.html"><a href="experiment.html#results-1"><i class="fa fa-check"></i><b>6.4</b> Results</a><ul>
<li class="chapter" data-level="6.4.1" data-path="experiment.html"><a href="experiment.html#data-screening"><i class="fa fa-check"></i><b>6.4.1</b> Data Screening</a></li>
<li class="chapter" data-level="6.4.2" data-path="experiment.html"><a href="experiment.html#modeling-1"><i class="fa fa-check"></i><b>6.4.2</b> Modeling</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="experiment.html"><a href="experiment.html#discussion-2"><i class="fa fa-check"></i><b>6.5</b> Discussion</a></li>
<li class="chapter" data-level="6.6" data-path="experiment.html"><a href="experiment.html#conclusions-2"><i class="fa fa-check"></i><b>6.6</b> Conclusions</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="computational-model.html"><a href="computational-model.html"><i class="fa fa-check"></i><b>7</b> Computational Model</a><ul>
<li class="chapter" data-level="7.1" data-path="computational-model.html"><a href="computational-model.html#levels-of-abstraction"><i class="fa fa-check"></i><b>7.1</b> Levels of Abstraction</a></li>
<li class="chapter" data-level="7.2" data-path="computational-model.html"><a href="computational-model.html#model-overview"><i class="fa fa-check"></i><b>7.2</b> Model Overview</a></li>
<li class="chapter" data-level="7.3" data-path="computational-model.html"><a href="computational-model.html#verbal-model"><i class="fa fa-check"></i><b>7.3</b> Verbal Model</a><ul>
<li class="chapter" data-level="7.3.1" data-path="computational-model.html"><a href="computational-model.html#model-representational-assumptions"><i class="fa fa-check"></i><b>7.3.1</b> Model Representational Assumptions</a></li>
<li class="chapter" data-level="7.3.2" data-path="computational-model.html"><a href="computational-model.html#contents-of-the-prior-knowledge"><i class="fa fa-check"></i><b>7.3.2</b> Contents of the Prior Knowledge</a></li>
<li class="chapter" data-level="7.3.3" data-path="computational-model.html"><a href="computational-model.html#modeling-information-content"><i class="fa fa-check"></i><b>7.3.3</b> Modeling Information Content</a></li>
<li class="chapter" data-level="7.3.4" data-path="computational-model.html"><a href="computational-model.html#setting-limits-with-transcribe"><i class="fa fa-check"></i><b>7.3.4</b> Setting Limits with Transcribe</a></li>
<li class="chapter" data-level="7.3.5" data-path="computational-model.html"><a href="computational-model.html#pattern-matching"><i class="fa fa-check"></i><b>7.3.5</b> Pattern Matching</a></li>
<li class="chapter" data-level="7.3.6" data-path="computational-model.html"><a href="computational-model.html#dictation-re-entry"><i class="fa fa-check"></i><b>7.3.6</b> Dictation Re-Entry</a></li>
<li class="chapter" data-level="7.3.7" data-path="computational-model.html"><a href="computational-model.html#completion"><i class="fa fa-check"></i><b>7.3.7</b> Completion</a></li>
<li class="chapter" data-level="7.3.8" data-path="computational-model.html"><a href="computational-model.html#model-output"><i class="fa fa-check"></i><b>7.3.8</b> Model Output</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="computational-model.html"><a href="computational-model.html#formal-model"><i class="fa fa-check"></i><b>7.4</b> Formal Model</a><ul>
<li class="chapter" data-level="7.4.1" data-path="computational-model.html"><a href="computational-model.html#computational-model-1"><i class="fa fa-check"></i><b>7.4.1</b> Computational Model</a></li>
<li class="chapter" data-level="7.4.2" data-path="computational-model.html"><a href="computational-model.html#example"><i class="fa fa-check"></i><b>7.4.2</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="computational-model.html"><a href="computational-model.html#conclusions-3"><i class="fa fa-check"></i><b>7.5</b> Conclusions</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/davidjohnbaker1/document" target="blank">Modeling Melodic Dictation</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">MODELING MELODIC DICTATION</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="experiment" class="section level1">
<h1><span class="header-section-number">Chapter 6</span> Experiment</h1>
<div id="rationale-4" class="section level2">
<h2><span class="header-section-number">6.1</span> Rationale</h2>
<p>Using experiments to understand factors that contribute to an individual’s ability to remember melodic material are by no means new <span class="citation">(Ortmann <a href="#ref-ortmannTonalDeterminantsMelodic1933">1933</a>)</span>.
This is not a simple problem as noted in the <a href="intro.html#intro">Context, Literature, and Rationale</a>, as both individual differences as well as musical features are difficult to quantify and subsequently model.
Capturing variability at both the individual and item level is not only riddled with measurement problems, but this variability problem is exacerbated when realizing many of the statistical ramifications of measuring so many variables in a single experiment.
Many variables leads to many tests, which leads to inflated type I error rates, as well as massive resources needed in order to detect even small effects.</p>
<p>Fortunately, dealing with high levels of variability at both the individual and item level is not a problem exclusive to work on melodic dictation.
Work from from the field of linguistics has developed more sophisticated methodologies that are able to accommodate the above challenges and provide a more elegant way of handling these types of problems <span class="citation">(Baayen, Davidson, and Bates <a href="#ref-baayenMixedeffectsModelingCrossed2008">2008</a>)</span>.
In this chapter, I synthesize work from the previous chapters of this dissertation in an experiment investigating melodic dictation.
Unlike work in the past literature, I take advantage of statistical methodologies that are able to better accommodate problems in experimental design using paradigms that accommodate for both individual and item level differences.
By using mixed effects modeling, I put forward a more principled way of modeling data that more ecologically reflects melodic dictation.
I show how it is possible to combine both tests of individual ability and as well as musical features in order to predict performance.
Additionally, I discuss the intricacies associated with scoring and relate these practices back to the classroom.</p>
</div>
<div id="introduction" class="section level2">
<h2><span class="header-section-number">6.2</span> Introduction</h2>
<p>Despite its near ubiquity in Conservatory and School of Music curricula, research surrounding topics concerning aural skills is not well understood.
This is peculiar since almost any individual seeking to earn a degree in music usually must enroll in multiple aural skills classes that cover a wide array of topics from sight-singing melodies, to melodic and harmonic dictation– all of which are presumed to be fundamental to any musician’s formal training.
Skills acquired in these classes are meant to hone the musician’s ear and enable them not only to think about music, but to borrow Karpinski’s phrase, to “think in music” <span class="citation">(Karpinski <a href="#ref-karpinskiAuralSkillsAcquisition2000">2000</a>, <a href="mailto:p.4;@bestMusicCurriculaFuture1992">p.4;@bestMusicCurriculaFuture1992</a>)</span>.
The tacit assumption behind these tasks is that once one learns to think in music, these abilities should transfer to other aspects of the musician’s playing in a deep and profound way.
The skills that make up an individual’s aural skills encompass many abilities, and are thought to be reflective of some sort of core skill.
This logic is evident in early attempts to model performance in aural skills classes where <span class="citation">Harrison, Asmus, and Serpe (<a href="#ref-harrisonEffectsMusicalAptitude1994">1994</a>)</span> created a latent variable model to predict an individual’s success in aural skills classes based on musical aptitude, musical experience, motivation, and academic ability.
While their model was able to predict a large amount of variance (73%), modeling at this high, conceptual level does not provide any sort of specific insights into the mental processes that are required for completing aural skills related tasks.
This trend can also be seen in more recent research that has explored the relationship between how well entrance exams at the university level are able to predict success later on in the degree program.</p>
<p><span class="citation">Wolf and Kopiez (<a href="#ref-wolfGradesReflectDevelopment2014">2014</a>)</span> noted multiple confounds in their study attempting to asses ability level in university musicians such as inflated grading, which led to ceiling effects, as well as a broad lack of consistency in how schools are assessing the success of their students.
But even if the results at the larger level were to be clearer, this again says nothing about the processes that contribute to tasks like melodic dictation.
Rather than taking a bird’s eye view of the subject, this chapter will primarily focus on descriptive factors that might contribute to an individual’s ability dictate a melody.</p>
<p>Melodic dictation is one of the central activities in an aural skills class.
The activity normally consists of the instructor of the class playing a monophonic melody a limited number of times and the students must use both their <em>ear</em>, as well as their understanding of Western Music theory and notation, in order to transcribe the melody without any sort of external reference.
No definitive method is taught across universities, but many schools of thought exist on the topic and a wealth of resources and materials have been suggested that might help students better complete these tasks <span class="citation">(Berkowitz <a href="#ref-berkowitzNewApproachSight2011">2011</a>; Cleland and Dobrea-Grindahl <a href="#ref-clelandDevelopingMusicianshipAural2010">2010</a>; Karpinski <a href="#ref-karpinskiManualEarTraining2007">2007</a>; Ottman and Rogers <a href="#ref-ottmanMusicSightSinging2014">2014</a>)</span>
The lack of consistency could be attributed to the fact that there are so many variables at play during this process.
Prior to listening, the student needs to have an understanding of Western music notation at least to the level of understanding the melody being played.
This understanding must to be readily accessible, since as new musical information is heard, it is the student’s responsibility, in that moment, to essentially follow the Karpinski model and encode the melody in short term memory or pattern-match to long term memory <span class="citation">(Oura <a href="#ref-ouraConstructingRepresentationMelody1991a">1991</a>)</span> so that they can identify what they are hearing and transcribe it moments later into Western notation <span class="citation">(Karpinski <a href="#ref-karpinskiAuralSkillsAcquisition2000">2000</a>, <a href="#ref-karpinskiModelMusicPerception1990">1990</a>)</span>.
Regardless, performing some sort of aural skills task requires both long term memory and knowledge for comprehension, as well as the ability to actively manipulate differing degrees of complex musical information in real time while concurrently writing it down.</p>
<p>Given the complexity of this task, as well as the difficulty in quantifying attributes of melodies, it is not surprising that scant research exists for describing these tasks.
Fortunately, a fair amount of research exists in related literature which can generate theories and hypotheses explaining how individuals dictate melodies.
Beginning first with factors that are less malleable from person to person would be individual differences in cognitive ability.
While dictating melodies is something that is learned, a growing body of literature suggests that other factors can explain
unique amounts of variance in performance via differences in cognitive ability.
For example, <span class="citation">Meinz and Hambrick (<a href="#ref-meinzDeliberatePracticeNecessary2010">2010</a>)</span> found that measures of working memory capacity (WMC)
were able to explain variance in an individual’s ability to sight read above and beyond that of sight reading experience and
musical training.
<span class="citation">Colley, Keller, and Halpern (<a href="#ref-colleyWorkingMemoryAuditory2017">2017</a>)</span> recently suggested an individual’s WMC could also help explain differences beyond musical training in tasks related to tapping along to expressive timing in music.
These issues become more confounded when considering other recent work by <span class="citation">Swaminathan, Schellenberg, and Khalil (<a href="#ref-swaminathanRevisitingAssociationMusic2017">2017</a>)</span> that suggests factors such as musical aptitude, when considered in the modeling process, can better explain individual differences in intelligence between musicians and nonmusicians implying that there are selection biases at play within the musical population.
They claim there is a selection bias that “smarter” people tend to gravitate towards studying music, which may explain some of the differences in memory thought to be caused by music study <span class="citation">(Talamini et al. <a href="#ref-talaminiMusiciansHaveBetter2017">2017</a>)</span>.
Knowing that these cognitive factors can play a role warrants attention from future researchers on controlling for variables that might contribute to this process, but are not directly intuitive and have not been considered in much of the past
research.
This is especially important given recent critique of models that purport to measure cognitive ability but are not grounded in an explanatory theoretical model <span class="citation">(Kovacs and Conway <a href="#ref-kovacsProcessOverlapTheory2016">2016</a>)</span>.</p>
<div id="memory-for-melodies-1" class="section level3">
<h3><span class="header-section-number">6.2.1</span> Memory for Melodies</h3>
<p>The ability to understand how individuals encode melodies is at the heart of much of the music perception literature.
Largely stemming from the work of Bregman <span class="citation">(Bregman <a href="#ref-bregmanAuditorySceneAnalysis2006">2006</a>)</span>, Deutsch and Feroe, <span class="citation">(Deutsch and Feroe <a href="#ref-deutschInternalRepresentationPitch1981">1981</a>)</span>, and Dowling’s <span class="citation">(Bartlett and Dowling <a href="#ref-bartlettRecognitionTransposedMelodies1980">1980</a>; Dowling <a href="#ref-dowlingExpectancyAttentionMelody1990">1990</a>, <a href="#ref-dowlingPerceptionInterleavedMelodies1973">1973</a>, <a href="#ref-dowlingScaleContourTwo1978">1978</a>)</span> work on memory for melodies has sugested that both key and contour information play a central role in the perception and memory of novel melodies.
Memory for melodies tends to be much worse than memory for other stimuli, such as pictures or faces, noting that the average area under the ROC curve tends to be at about .7 in many of the studies they reviewed, with .5 meaning chance and 1 being a perfect performance <span class="citation">(Halpern and Bartlett <a href="#ref-halpernMemoryMelodies2010">2010</a>)</span>.
Halpern and Bartlett also note that much of the literature on memory for melodies primarily used same-different experimental paradigms to investigate individuals’ melodic perception ability similar to the paradigm used in <span class="citation">(Halpern and Müllensiefen <a href="#ref-halpernEffectsTimbreTempo2008">2008</a>)</span>.</p>
</div>
<div id="musical-factors-1" class="section level3">
<h3><span class="header-section-number">6.2.2</span> Musical Factors</h3>
<p>Not nearly as much is known about how an individual learns melodies, especially in dictation settings.
The last, and possibly most obvious, variable that would contribute to an individual’s ability to learn and dictate a melody would be the amount of previous exposure to the melody and the complexity of the melody itself.
A fair amount of research from the music education literature examines melodic dictation in a more ecological setting <span class="citation">(N. Buonviri <a href="#ref-buonviriEffectsMusicNotation2015">2015</a>; Nathan O. Buonviri <a href="#ref-buonviriEffectsPreparatorySinging2015">2015</a>; Buonviri <a href="#ref-buonviriEffectsTwoListening2017">2017</a>, <a href="#ref-buonviriExplorationUndergraduateMusic2014">2014</a>; Nathan O Buonviri and Paney <a href="#ref-buonviriMelodicDictationInstruction2015">2015</a>; Unsworth et al. <a href="#ref-unsworthAutomatedVersionOperation2005">2005</a>)</span>, but most take a descriptive approach to modeling the results using between-subject manipulations.
Some rules of thumb regarding how many times a melody should be played in a dictation setting have been proposed by Karpinski <span class="citation">(Karpinski <a href="#ref-karpinskiAuralSkillsAcquisition2000">2000</a>, 99)</span> that account for chunking as well as the idea that more exposure would lead to more complete encoding.
For example, he suggests using the formula <span class="math inline">\(P = (Ch/L) + 1\)</span> where <span class="math inline">\(P\)</span> is the number of playings, <span class="math inline">\(Ch\)</span> is the number of chunks in the dictation (with chunk defined as a single memorable unit), and <span class="math inline">\(L\)</span> = the limit of a listener’s short term memory in terms of chunks, a number between 6 and 10.
This definition requires expert selection of what a chunk is, and does not take into account any of the Experimental factors put forward in the taxnomy presented in this dissertation’s <a href="intro.html#intro">Context, Literature, and Rationale</a>.</p>
<p>Recently, tools have been developed in the field of computational musicology to help with operationalizing the complexity of melodies.
Both simple and more complex features have been used to model performance in behavioral tasks.
For example <span class="citation">Eerola et al. (<a href="#ref-eerolaPerceivedComplexityWestern2006">2006</a>)</span> found that note density, though not consciously apparent to the participants, predicted judgments of human similarity between melodies not familiar to the participants.
Both <span class="citation">Harrison, Musil, and Müllensiefen (<a href="#ref-harrisonModellingMelodicDiscrimination2016">2016</a>)</span> and <span class="citation">Baker and Müllensiefen (<a href="#ref-bakerPerceptionLeitmotivesRichard2017">2017</a>)</span> used measures of melodic complexity created from data reductive techniques to sucessfuly predict difficulty on melodic memory tasks.</p>
<p>Note density would be an ideal candidate to investigate because it is both easily measured and the amount of information that can be currently held in memory as measured by bits of information has a long history in cognitive psychology <span class="citation">(Cowan <a href="#ref-cowanWorkingMemoryCapacity2005">2005</a>; George A. Miller <a href="#ref-millerInformationMemory1956">1956</a>; Pearce <a href="#ref-pearceStatisticalLearningProbabilistic2018a">2018</a>)</span>.
In terms of more complex features, much of this work largely stems from the work of Müllensiefen and his development of the FANTASTIC Toolbox <span class="citation">(<a href="#ref-mullensiefenFantasticFeatureANalysis2009">2009</a>)</span>, and a few papers have claimed to be able to predict various behavioral outcomes based on the structural characteristics of melodies.
For example, <span class="citation">(Kopiez and Mullensiefen <a href="#ref-kopiezAufSucheNach2011">2011</a>)</span> claimed to have been able to predict how well songs from The Beatles’ album Revolver did on popularity charts based on structural characteristic of the melodies using a data driven approach.
Expanding on an earlier study, <span class="citation">(Müllensiefen and Halpern <a href="#ref-mullensiefenRoleFeaturesContext2014">2014</a>)</span> found that the degree of distinctiveness of a melody when compared to its parent corpus could be used to predict how participants in an old/new memory paradigm were able to recognize melodies.
These abstracted features also have been used in various corpus studies <span class="citation">(Jakubowski et al. <a href="#ref-jakubowskiDissectingEarwormMelodic2017">2017</a>; Janssen, Burgoyne, and Honing <a href="#ref-janssenPredictingVariationFolk2017">2017</a>; Rainsford, Palmer, and Sauer <a href="#ref-rainsfordDistinctivenessEffectRecognition2019">2019</a>; Rainsford, Palmer, and Paine <a href="#ref-rainsfordMUSOSMUsicSOftware2018">2018</a>)</span>
that again use data driven approaches in order to explain which of the 38 features that FANTASTIC calculates can predict real-world behavior.</p>
<p>While helpful and somewhat explanatory, the problem with either data reductive or data driven approaches to this modeling is that they take a post-hoc approach with the assumption that listeners are even able to abstract and perceive these features.
Doing this does not allow for any sort of controlled approach.
Without experimentally manipulating the parameters, which is then further confounded when using some sort of data reduction technique.
This is understandable seeing as it is very difficult to manipulate certain qualities of a melody without disturbing
other features <span class="citation">(Taylor and Pembrook <a href="#ref-taylorStrategiesMemoryShort1983">1983</a>)</span>.
For example, if you wanted to decrease the “tonalness” of a melody by adding in a few more chromatic pitches, you inevitably will increase other measures of pitch and interval entropy.
In order to truly understand if these features are driving changes in behaviour, each needs to be altered in some sort of controlled and systematic way while simultaneously considering differences in training and cognitive ability.</p>
<p>In order to accomplish this, I put forward findings from an experiment modeling performance on melodic dictation tasks using both individual and musical features.
A pilot study was run (N=11) in order to assess musical confounds that might be present in modeling melodic dictation.
Results of that pilot study are not reported here.
Based on the results of this pilot data, a follow up experiment was conducted to better investigate the features in question.</p>
<p>The study sought to answer three main hypotheses:</p>
<ol style="list-style-type: decimal">
<li>Are all experimental melodies used equally difficult to
dictate?</li>
<li>To what extent do the musical features of Note Density
and Tonalness play a role in difficulty of dictation?</li>
<li>Do individual factors at the cognitive level play a role
in the melodic dictation process above and beyond musical
factors?</li>
</ol>
</div>
</div>
<div id="methods-1" class="section level2">
<h2><span class="header-section-number">6.3</span> Methods</h2>
<div id="participants-1" class="section level3">
<h3><span class="header-section-number">6.3.1</span> Participants</h3>
<p>Forty-three students enrolled at Louisiana State University School of Music completed the study.
The inclusion criteria in the analysis included reporting no hearing loss, not actively taking medication that would alter cognitive performance, and individuals whose performance on any task performed greater than three standard deviations from the mean score of that task.
Using these criteria, two participants were dropped for not completing the entire experiment.
Thus, 41 participants met the criteria for inclusion.
The eligible participants were between the ages of 17 and 26 (M = 19.81, SD = 1.93; 15 women).
Participants volunteered, received course credit, or were paid $10.</p>
</div>
<div id="materials-1" class="section level3">
<h3><span class="header-section-number">6.3.2</span> Materials</h3>
<p>Four melodies for the dictation were selected from a corpus of N=115 melodies derived from the A New Approach to Sight Singing aural skills textbook <span class="citation">(Berkowitz <a href="#ref-berkowitzNewApproachSight2011">2011</a>)</span>.
Melodies were chosen based on their musical features as extracted via the FANTASTIC Toolbox <span class="citation">(Mullensiefen <a href="#ref-mullensiefenFantasticFeatureANalysis2009">2009</a>)</span>.
After abstracting the full set of features of the melodies, possible melodies were first narrowed down by limiting the corpus to melodies lasting between 9 and 12 seconds and then indexed to select four melodies that were chosen as part of a 2 x 2 repeated measures design including a high and low tonalness and note density condition.
Melodies, as well as a table of their abstracted features can be seen in <a href="#expfeaturetable"><strong>??</strong></a> and <a href="experiment.html#fig:b34">6.1</a>, <a href="experiment.html#fig:b95">6.2</a>,<a href="#fig:112"><strong>??</strong></a>, and <a href="experiment.html#fig:b9">6.4</a> .
Melodies and other sounds used were encoded using MuseScore 2 using the standard piano timbre and all set to a tempo of quarter = 120 beats per minute and adjusted accordingly based on time signature to ensure they all were same absolute time duration.
The experiment was then coded in jsPsych <span class="citation">(de Leeuw <a href="#ref-deleeuwJsPsychJavaScriptLibrary2015">2015</a>)</span> and played through a browser offline with high quality headphones.</p>
<table>
<caption><span id="tab:expfeaturetable">Table 6.1: </span>Feature Table</caption>
<thead>
<tr class="header">
<th align="left">Melodies</th>
<th align="right">Tonalness</th>
<th align="right">Note.Density</th>
<th align="left">Design</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">34</td>
<td align="right">0.95</td>
<td align="right">1.67</td>
<td align="left">High Tonal, Low Note Density</td>
</tr>
<tr class="even">
<td align="left">112</td>
<td align="right">0.98</td>
<td align="right">3.73</td>
<td align="left">High Tonal, High Note Density</td>
</tr>
<tr class="odd">
<td align="left">9</td>
<td align="right">0.71</td>
<td align="right">1.75</td>
<td align="left">Low Tonal, Low Note Density</td>
</tr>
<tr class="even">
<td align="left">95</td>
<td align="right">0.76</td>
<td align="right">3.91</td>
<td align="left">Low Tonal, High Note Density</td>
</tr>
</tbody>
</table>
<div class="figure" style="text-align: center"><span id="fig:b34"></span>
<img src="img/Berkowitz34.png" alt="Melody 34" width="100%" />
<p class="caption">
Figure 6.1: Melody 34
</p>
</div>
<div class="figure" style="text-align: center"><span id="fig:b95"></span>
<img src="img/Berkowitz95.png" alt="Melody 95" width="100%" />
<p class="caption">
Figure 6.2: Melody 95
</p>
</div>
<div class="figure" style="text-align: center"><span id="fig:b112"></span>
<img src="img/Berkowitz112.png" alt="Melody 112" width="100%" />
<p class="caption">
Figure 6.3: Melody 112
</p>
</div>
<div class="figure" style="text-align: center"><span id="fig:b9"></span>
<img src="img/BerkowitzNo9.png" alt="Melody 9" width="100%" />
<p class="caption">
Figure 6.4: Melody 9
</p>
</div>
</div>
<div id="procedure-1" class="section level3">
<h3><span class="header-section-number">6.3.3</span> Procedure</h3>
<p>Upon arrival, participants sat down in a lab at their own personal computer.
Multiple individuals were tested simultaneously although individually.
Each participant was given a test packet that contained all information needed for the experiment.
After obtaining written consent, participants navigated a series of instructions explaining the nature of the experiment and were given an opportunity to adjust the volume to a comfortable level.
The first portion of the experiment that participants completed was the melodic dictation.
In order to alleviate any anxiety in performance, participants were explicitly told that “unlike dictations performed in class, they were not expected to get perfect scores on their dictations”.
Each melody was played five times with 20 seconds between hearings and 120 seconds after the last hearing <span class="citation">(Paney <a href="#ref-paneyEffectDirectingAttention2016">2016</a>)</span>.<br />
After the dictation portion of the experiment, participants completed a small survey on their Aural Skills background, as well as the Bucknell Auditory Imagery Scale C <span class="citation">(Halpern <a href="#ref-halpernDifferencesAuditoryImagery2015">2015</a>)</span>.
After completing the Aural Skills portion of the experiment, participants completed one block of two different tests of working memory capacity <span class="citation">(Unsworth et al. <a href="#ref-unsworthAutomatedVersionOperation2005">2005</a>)</span> and Raven’s Advanced Progressive Matrices, and a Number Series task as two tests of general fluid intelligence (Gf) <span class="citation">(Raven <a href="#ref-ravenManualRavenProgressive1994">1994</a>; Thurstone <a href="#ref-thurstonePrimaryMentalAbilities1938">1938</a>)</span> resulting in four total scores.</p>
<p>Exact details of the design can be found in Chapter 3.
After completing the cognitive battery, participants finished the experiment by compiling the self-report version of the Goldsmiths Musical Sophistication Index <span class="citation">(Müllensiefen et al. <a href="#ref-mullensiefenMusicalityNonMusiciansIndex2014">2014</a>)</span>, the Short Test of Musical Preferences <span class="citation">(Rentfrow and Gosling <a href="#ref-rentfrowReMiEveryday2003">2003</a>)</span>, as well as questions pertaining to the participant’s socio-economic status, and any other information we needed to control for (Hearing Loss, Medication). Exact materials for the experiment can be found at <a href="https://github.com/davidjohnbaker1/modelingMelodicDictation" class="uri">https://github.com/davidjohnbaker1/modelingMelodicDictation</a>).</p>
</div>
<div id="scoring-melodies" class="section level3">
<h3><span class="header-section-number">6.3.4</span> Scoring Melodies</h3>
<p>Melodies were scored by counting the amount of notes in the melody and multiplying that number by two.
Half the points were attributed to rhythmic accuracy and the other
half to pitch accuracy.
Points were not deducted for notating the melody in the incorrect octave.
Points for pitch could only be given if the participant correctly notated the rhythm.
For example, in Melody 34 in Figure <a href="experiment.html#fig:b34">6.1</a> there were 40 points possible (20 notes * 2).
If a participant was to have put a quarter note on the second beat of the third measure, and have everything else correct, they would have scored a 19/20.
Only if the correct rhythms of the measures were accurate could pitch points be
awarded.
In cases where there were more serious errors, for example if the second half of the second bar was not notated, points would have been deducted in both the pitch and rhythm sub-scores.
Dictations were scored all melodies independently by two raters and then cross referenced for inter rater reliability (<span class="math inline">\(\kappa\)</span> = .96) whic suggests a high degree of inter-rater reliability <span class="citation">(Koo and Li <a href="#ref-kooGuidelineSelectingReporting2016">2016</a>)</span>.</p>
</div>
</div>
<div id="results-1" class="section level2">
<h2><span class="header-section-number">6.4</span> Results</h2>
<div id="data-screening" class="section level3">
<h3><span class="header-section-number">6.4.1</span> Data Screening</h3>
<p>Before conducting any analyses, data was screened for quality.
List-wise deletion was used to remove any participants where not all variables were present.
This process resulted in removing four participants: two did not complete any of the survey materials and two did not have any measures of working memory capacity due to computer error.
After list-wise deletion, thirty-nine participants remained.</p>
</div>
<div id="modeling-1" class="section level3">
<h3><span class="header-section-number">6.4.2</span> Modeling</h3>
<p>In order to model the data and investigate the hypotheses, I fit a series of linear mixed effects models using the R programming langauge <span class="citation">(R Core Team <a href="#ref-R-base">2017</a>)</span> and the <em>lme4</em> and <em>LMERTEST</em> <span class="citation">(Bates et al. <a href="#ref-batesFittingLinearMixedEffects2015">2015</a>)</span> packages to predict scores from the dictation exercise using both individual and musical variables.
Variables are added at each step and tests of significant model improvement are presented in Table <a href="#modeltable"><strong>??</strong></a>.
P-values were obtained by likelihood ratio tests between models.
I also report AIC and BIC measures for each model ran.
Models were ran sequentially moving from a null model to a theoretical, statistical model predicted by variables discussed above.</p>
<p>After establishing a null model (Model 1, <code>null_model</code>), I then added individual level predictors (<code>wmc_model</code> (Model 2) and <code>gf_model</code>, and <code>cognitive_model</code>) carrying forward only measures of working memory capacity.
No cognitive variables resulted in a significant increase in model fit.
Next, I modeled musical level features.
Each musical model that I ran allowed musical features to have random slopes since presumably individuals would perform differently on each melody.
The first muscial model (Model 3) treated each melody as a categorical fixed effect.
The second musical model (Model 4) used the categorical features as predictors, which included an interaction effect.
The third musical model (Model 5) used the continous FANTASTIC measures as fixed effects, which included an interaction effect.
In order to investigate claims put forward in the third chapter of this dissertation, I also ran a model (Model 6) using only interval entropy as calculated by FANTASTIC.
Finally, I ran two models that accounted for both musical and indiviual level predictors.
Model 7 exteneds Model 6, with the addition of working memory capacity as fixed effect, allowing for random slopes.
Model 8 extends Model 5, with the addition of working memory capacity as a fixed effect, allowing for random slopes.
I depict the fixed effects for all models in <a href="experiment.html#fig:megrid">6.5</a>.
Tables for each model are presented in Tables <a href="#fig:metable1"><strong>??</strong></a>, @(ref:metable2), @(ref:metable3), and <a href="#metable4"><strong>??</strong></a>.</p>
<table>
<caption><span id="tab:modeltable">Table 6.2: </span>Series of Model Fits</caption>
<thead>
<tr class="header">
<th></th>
<th align="right">Df</th>
<th align="right">AIC</th>
<th align="right">BIC</th>
<th align="right">logLik</th>
<th align="right">deviance</th>
<th align="right">Chisq</th>
<th align="right">Chi Df</th>
<th align="right">Pr(&gt;Chisq)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>null_model</td>
<td align="right">3</td>
<td align="right">62.73561</td>
<td align="right">71.885181</td>
<td align="right">-28.36781</td>
<td align="right">56.73561</td>
<td align="right">NA</td>
<td align="right">NA</td>
<td align="right">NA</td>
</tr>
<tr class="even">
<td>wmc_model</td>
<td align="right">4</td>
<td align="right">63.44881</td>
<td align="right">75.648235</td>
<td align="right">-27.72441</td>
<td align="right">55.44881</td>
<td align="right">1.286802</td>
<td align="right">1</td>
<td align="right">0.2566381</td>
</tr>
<tr class="odd">
<td>gf_model</td>
<td align="right">4</td>
<td align="right">64.73129</td>
<td align="right">76.930718</td>
<td align="right">-28.36565</td>
<td align="right">56.73129</td>
<td align="right">0.000000</td>
<td align="right">0</td>
<td align="right">1.0000000</td>
</tr>
<tr class="even">
<td>cognitive_model</td>
<td align="right">5</td>
<td align="right">65.38048</td>
<td align="right">80.629759</td>
<td align="right">-27.69024</td>
<td align="right">55.38048</td>
<td align="right">1.350815</td>
<td align="right">1</td>
<td align="right">0.2451357</td>
</tr>
<tr class="odd">
<td>melody_model</td>
<td align="right">6</td>
<td align="right">-59.76827</td>
<td align="right">-41.469130</td>
<td align="right">35.88413</td>
<td align="right">-71.76827</td>
<td align="right">127.148745</td>
<td align="right">1</td>
<td align="right">0.0000000</td>
</tr>
<tr class="even">
<td>feature_category_model</td>
<td align="right">6</td>
<td align="right">-59.76827</td>
<td align="right">-41.469130</td>
<td align="right">35.88413</td>
<td align="right">-71.76827</td>
<td align="right">0.000000</td>
<td align="right">0</td>
<td align="right">1.0000000</td>
</tr>
<tr class="odd">
<td>feature_ientropy_model</td>
<td align="right">6</td>
<td align="right">-28.80330</td>
<td align="right">-10.504165</td>
<td align="right">20.40165</td>
<td align="right">-40.80330</td>
<td align="right">0.000000</td>
<td align="right">0</td>
<td align="right">1.0000000</td>
</tr>
<tr class="even">
<td>total_model_ientropy</td>
<td align="right">7</td>
<td align="right">-22.83439</td>
<td align="right">-1.485395</td>
<td align="right">18.41719</td>
<td align="right">-36.83439</td>
<td align="right">0.000000</td>
<td align="right">1</td>
<td align="right">1.0000000</td>
</tr>
<tr class="odd">
<td>feature_cont_model</td>
<td align="right">8</td>
<td align="right">-68.07246</td>
<td align="right">-43.673611</td>
<td align="right">42.03623</td>
<td align="right">-84.07246</td>
<td align="right">47.238072</td>
<td align="right">1</td>
<td align="right">0.0000000</td>
</tr>
<tr class="even">
<td>total_model_experimental</td>
<td align="right">9</td>
<td align="right">-56.97263</td>
<td align="right">-29.523924</td>
<td align="right">37.48631</td>
<td align="right">-74.97263</td>
<td align="right">0.000000</td>
<td align="right">1</td>
<td align="right">1.0000000</td>
</tr>
</tbody>
</table>
<p><img src="img/metable1.png" width="100%" style="display: block; margin: auto;" /></p>
<p><img src="img/metable2.png" width="100%" style="display: block; margin: auto;" /></p>
<p><img src="img/metable3.png" width="100%" style="display: block; margin: auto;" /></p>
<p><img src="img/metable4.png" width="100%" style="display: block; margin: auto;" /></p>
<div class="figure" style="text-align: center"><span id="fig:megrid"></span>
<img src="img/me_grid.png" alt="Fixed Effects of Models" width="100%" />
<p class="caption">
Figure 6.5: Fixed Effects of Models
</p>
</div>
<p>Distributions for average scores of the melodies are found in <a href="experiment.html#fig:eboxplot">6.6</a> and density scores of these items are foudn in <a href="experiment.html#fig:edistribution">6.7</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:eboxplot"></span>
<img src="img/melody_difficulty.png" alt="Melody Difficulty" width="100%" />
<p class="caption">
Figure 6.6: Melody Difficulty
</p>
</div>
<div class="figure" style="text-align: center"><span id="fig:edistribution"></span>
<img src="img/melody_differences.png" alt="Melodic Differences" width="100%" />
<p class="caption">
Figure 6.7: Melodic Differences
</p>
</div>
</div>
</div>
<div id="discussion-2" class="section level2">
<h2><span class="header-section-number">6.5</span> Discussion</h2>
<p>In this chapter, I investigated the extent to which both individual differences and abstracted musical features could be used to model results in melodic dictations.
In order to examine <span class="math inline">\(H1\)</span>, I ran a linear mixed effects model in order discern any differences in melody difficulty.
As noted in <a href="#HERE"><strong>??</strong></a>, both a significant main effect of Tonalness and Note Density was found, as well as a small interaction between the two variables suggesting evidence supporting rejecting <span class="math inline">\(H2\)</span>’s null hypothesis.
The interaction emerged from differences in melody means in the low density conditions with the melody with higher tonalness actually scoring higher in terms of number of errors.
Subsequent adding of individual level predictors did result in a better fitting model.</p>
<p>While I expected to find an interaction, this condition (Melody 34) was hypothesized to be the easiest of the four
conditions.
With Melody 9 there was a clear floor effect, which was also to be expected as when we chose the melodies, I had no previous experimental data explicitly looking at melodic dictation to rely on.
Future experiments should use abstracted features from Melody 9 as a baseline in order to avoid floor effects.</p>
<p>The main effect of note density was expected and exhibited a large effect size. (<span class="math inline">\(\eta\)</span> = .46).
While it would be tempting to attribute this finding exactly to the Note Density feature extracted by FANTASTIC, the high and low density conditions could also be operationalized as having compound versus simple meter.
Given the large effect of note density, I plan on taking more careful steps in the selection of our next melodies in order to control for any effects of meter and keep the effects limited to one meter if at all possible.</p>
<p>Somewhat surprisingly, the analysis incorporating the cognitive measures of covariance did not yield any significant
results.
While other researchers and the third Chapter of this disseratation have noted the importance of baseline cognitive ability <span class="citation">(Schellenberg and W. Weiss <a href="#ref-schellenbergMusicCognitiveAbilities2013">2013</a>)</span>, the task specificity of doing melodic dictation as we designed the experiment might not be well suited to capture the variability needed for any effects.
Hence, this chapter would not be able to reject H3’s null hypothesis.
Considering that other researchers have found constructs like working memory capacity and general fluid intelligence to be important factors of tasks of musical perception, a more refined design might be considered in the future to find any sort of effects.
Taken as a whole, these findings suggest that aural skills pedagogues should consider exploring the extent to which computationally extracted features can guide the difficulty expected of melodic dictation exercises.</p>
</div>
<div id="conclusions-2" class="section level2">
<h2><span class="header-section-number">6.6</span> Conclusions</h2>
<p>This chapter demonstrated that abstracted musical features such as tonalness and note density can play a role in predicting how well students do in tasks of melodic dictation.
While the experiment failed to yield any significant differences in cognitive ability predicting success at the task, future research plans should not fail to take these effects into consideration.</p>
<p>One important caveat in this modeling is that the models reported here are subject to change given other scoring procedures.
There is a high degree of variability in how melodic dictations are scored <span class="citation">(Gillespie <a href="#ref-gillespieMelodicDictationScoring2001">2001</a>)</span>, and modeling how different scoring procedures lead to differing results should be considered for future research.
Importantly, if a researcher adopted this paradigm for investigating melodic dictation, what is most important is that there is some sort of external reference a single scorer can compare themselves to.
Without having a pre-defined metric, scoring and thus grading will be subjected to the scorer’s explicit or implicit biases.
Given all that has been put forward here, the research thus far still does not explain the underlying processes for melodic dictation.
In this chapter, I have put forward two factors that help describe what contributes to this process using a principled experimental framework.
– sentence on principle.</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-ortmannTonalDeterminantsMelodic1933">
<p>Ortmann, Otto. 1933. “Some Tonal Determinants of Melodic Memory.” <em>Journal of Educational Psychology</em> 24 (6): 454–67. <a href="https://doi.org/10.1037/h0075218" class="uri">https://doi.org/10.1037/h0075218</a>.</p>
</div>
<div id="ref-baayenMixedeffectsModelingCrossed2008">
<p>Baayen, R.H., D.J. Davidson, and D.M. Bates. 2008. “Mixed-Effects Modeling with Crossed Random Effects for Subjects and Items.” <em>Journal of Memory and Language</em> 59 (4): 390–412. <a href="https://doi.org/10.1016/j.jml.2007.12.005" class="uri">https://doi.org/10.1016/j.jml.2007.12.005</a>.</p>
</div>
<div id="ref-karpinskiAuralSkillsAcquisition2000">
<p>Karpinski, Gary Steven. 2000. <em>Aural Skills Acquisition: The Development of Listening, Reading, and Performing Skills in College-Level Musicians</em>. Oxford University Press.</p>
</div>
<div id="ref-harrisonEffectsMusicalAptitude1994">
<p>Harrison, Carole S., Edward P. Asmus, and Richard T. Serpe. 1994. “Effects of Musical Aptitude, Academic Ability, Music Experience, and Motivation on Aural Skills.” <em>Journal of Research in Music Education</em> 42 (2): 131. <a href="https://doi.org/10.2307/3345497" class="uri">https://doi.org/10.2307/3345497</a>.</p>
</div>
<div id="ref-wolfGradesReflectDevelopment2014">
<p>Wolf, Anna, and Reinhard Kopiez. 2014. “Do Grades Reflect the Development of Excellence in Music Students? The Prognostic Validity of Entrance Exams at Universities of Music.” <em>Musicae Scientiae</em> 18 (2): 232–48. <a href="https://doi.org/10.1177/1029864914530394" class="uri">https://doi.org/10.1177/1029864914530394</a>.</p>
</div>
<div id="ref-berkowitzNewApproachSight2011">
<p>Berkowitz, Sol, ed. 2011. <em>A New Approach to Sight Singing</em>. 5th ed. New York: W.W. Norton.</p>
</div>
<div id="ref-clelandDevelopingMusicianshipAural2010">
<p>Cleland, Kent D., and Mary Dobrea-Grindahl. 2010. <em>Developing Musicianship Through Aural Skills: A Holisitic Approach to Sight Singing and Ear Training</em>. New York: Routledge.</p>
</div>
<div id="ref-karpinskiManualEarTraining2007">
<p>Karpinski, Gary S. 2007. <em>Manual for Ear Training and Sight Singing</em>. New York: Norton.</p>
</div>
<div id="ref-ottmanMusicSightSinging2014">
<p>Ottman, Robert W., and Nancy Rogers. 2014. <em>Music for Sight Singing</em>. 9th ed. Upper Saddle River, NJ: Pearson.</p>
</div>
<div id="ref-ouraConstructingRepresentationMelody1991a">
<p>Oura, Yoko. 1991. “Constructing a Representation of a Melody: Transforming Melodic Segments into Reduced Pitch Patterns Operated on by Modifiers.” <em>Music Perception: An Interdisciplinary Journal</em> 9 (2): 251–65. <a href="https://doi.org/10.2307/40285531" class="uri">https://doi.org/10.2307/40285531</a>.</p>
</div>
<div id="ref-karpinskiModelMusicPerception1990">
<p>Karpinski, Gary. 1990. “A Model for Music Perception and Its Implications in Melodic Dictation.” <em>Journal of Music Theory Pedagogy</em> 4 (1): 191–229.</p>
</div>
<div id="ref-meinzDeliberatePracticeNecessary2010">
<p>Meinz, Elizabeth J., and David Z. Hambrick. 2010. “Deliberate Practice Is Necessary but Not Sufficient to Explain Individual Differences in Piano Sight-Reading Skill: The Role of Working Memory Capacity.” <em>Psychological Science</em> 21 (7): 914–19. <a href="https://doi.org/10.1177/0956797610373933" class="uri">https://doi.org/10.1177/0956797610373933</a>.</p>
</div>
<div id="ref-colleyWorkingMemoryAuditory2017">
<p>Colley, Ian D, Peter E Keller, and Andrea R Halpern. 2017. “Working Memory and Auditory Imagery Predict Sensorimotor Synchronisation with Expressively Timed Music.” <em>Quarterly Journal of Experimental Psychology</em> 71 (8): 1781–96. <a href="https://doi.org/10.1080/17470218.2017.1366531" class="uri">https://doi.org/10.1080/17470218.2017.1366531</a>.</p>
</div>
<div id="ref-swaminathanRevisitingAssociationMusic2017">
<p>Swaminathan, Swathi, E. Glenn Schellenberg, and Safia Khalil. 2017. “Revisiting the Association Between Music Lessons and Intelligence: Training Effects or Music Aptitude?” <em>Intelligence</em> 62 (May): 119–24. <a href="https://doi.org/10.1016/j.intell.2017.03.005" class="uri">https://doi.org/10.1016/j.intell.2017.03.005</a>.</p>
</div>
<div id="ref-talaminiMusiciansHaveBetter2017">
<p>Talamini, Francesca, Gianmarco Altoè, Barbara Carretti, and Massimo Grassi. 2017. “Musicians Have Better Memory Than Nonmusicians: A Meta-Analysis.” Edited by Lutz Jäncke. <em>PLOS ONE</em> 12 (10): e0186773. <a href="https://doi.org/10.1371/journal.pone.0186773" class="uri">https://doi.org/10.1371/journal.pone.0186773</a>.</p>
</div>
<div id="ref-kovacsProcessOverlapTheory2016">
<p>Kovacs, Kristof, and Andrew R. A. Conway. 2016. “Process Overlap Theory: A Unified Account of the General Factor of Intelligence.” <em>Psychological Inquiry</em> 27 (3): 151–77. <a href="https://doi.org/10.1080/1047840X.2016.1153946" class="uri">https://doi.org/10.1080/1047840X.2016.1153946</a>.</p>
</div>
<div id="ref-bregmanAuditorySceneAnalysis2006">
<p>Bregman, Albert S. 2006. <em>Auditory Scene Analysis: The Perceptual Organization of Sound</em>. 2. paperback ed., repr. A Bradford Book. Cambridge, Mass.: MIT Press.</p>
</div>
<div id="ref-deutschInternalRepresentationPitch1981">
<p>Deutsch, Diana, and John Feroe. 1981. “The Internal Representation of Pitch Sequences in Tonal Music.” <em>Psychological Review</em> 88 (6): 503–22.</p>
</div>
<div id="ref-bartlettRecognitionTransposedMelodies1980">
<p>Bartlett, James C, and W Jay Dowling. 1980. “Recognition of Transposed Melodies: A Key-Distance Effect in Developmental Perspective,” 15.</p>
</div>
<div id="ref-dowlingExpectancyAttentionMelody1990">
<p>Dowling, W. 1990. “Expectancy and Attention in Melody Perception.” <em>Psychomusicology: A Journal of Research in Music Cognition</em> 9 (2): 148–60. <a href="https://doi.org/10.1037/h0094150" class="uri">https://doi.org/10.1037/h0094150</a>.</p>
</div>
<div id="ref-dowlingPerceptionInterleavedMelodies1973">
<p>Dowling, W.J. 1973. “The Perception of Interleaved Melodies.” <em>Cognitive Psychology</em> 5 (3): 322–37. <a href="https://doi.org/10.1016/0010-0285(73)90040-6" class="uri">https://doi.org/10.1016/0010-0285(73)90040-6</a>.</p>
</div>
<div id="ref-dowlingScaleContourTwo1978">
<p>Dowling, W. Jay. 1978. “Scale and Contour: Two Components of a Theory of Memory for Melodies.” <em>Psychological Review</em> 84 (4): 341–54.</p>
</div>
<div id="ref-halpernMemoryMelodies2010">
<p>Halpern, Andrea R., and James C. Bartlett. 2010. “Memory for Melodies.” In <em>Music Perception</em>, edited by Mari Riess Jones, Richard R. Fay, and Arthur N. Popper, 36:233–58. New York, NY: Springer New York. <a href="https://doi.org/10.1007/978-1-4419-6114-3\_8" class="uri">https://doi.org/10.1007/978-1-4419-6114-3\_8</a>.</p>
</div>
<div id="ref-halpernEffectsTimbreTempo2008">
<p>Halpern, Andrea R., and Daniel Müllensiefen. 2008. “Effects of Timbre and Tempo Change on Memory for Music.” <em>Quarterly Journal of Experimental Psychology</em> 61 (9): 1371–84. <a href="https://doi.org/10.1080/17470210701508038" class="uri">https://doi.org/10.1080/17470210701508038</a>.</p>
</div>
<div id="ref-buonviriEffectsMusicNotation2015">
<p>Buonviri, Nathan. 2015. “Effects of Music Notation Reinforcement on Aural Memory for Melodies.” <em>International Journal of Music Education</em> 33 (4): 442–50. <a href="https://doi.org/10.1177/0255761415582345" class="uri">https://doi.org/10.1177/0255761415582345</a>.</p>
</div>
<div id="ref-buonviriEffectsPreparatorySinging2015">
<p>Buonviri, Nathan O. 2015. “Effects of a Preparatory Singing Pattern on Melodic Dictation Success.” <em>Journal of Research in Music Education</em> 63 (1): 102–13. <a href="https://doi.org/10.1177/0022429415570754" class="uri">https://doi.org/10.1177/0022429415570754</a>.</p>
</div>
<div id="ref-buonviriEffectsTwoListening2017">
<p>Buonviri, Nathan O. 2017. “Effects of Two Listening Strategies for Melodic Dictation.” <em>Journal of Research in Music Education</em> 65 (3): 347–59. <a href="https://doi.org/10.1177/0022429417728925" class="uri">https://doi.org/10.1177/0022429417728925</a>.</p>
</div>
<div id="ref-buonviriExplorationUndergraduateMusic2014">
<p>Buonviri, Nathan O. 2014. “An Exploration of Undergraduate Music Majors’ Melodic Dictation Strategies.” <em>Update: Applications of Research in Music Education</em> 33 (1): 21–30. <a href="https://doi.org/10.1177/8755123314521036" class="uri">https://doi.org/10.1177/8755123314521036</a>.</p>
</div>
<div id="ref-buonviriMelodicDictationInstruction2015">
<p>Buonviri, Nathan O, and Andrew S Paney. 2015. “Melodic Dictation Instruction.” <em>Journal of Research in Music Education</em> 62 (2): 224–37.</p>
</div>
<div id="ref-unsworthAutomatedVersionOperation2005">
<p>Unsworth, Nash, Richard P. Heitz, Josef C. Schrock, and Randall W. Engle. 2005. “An Automated Version of the Operation Span Task.” <em>Behavior Research Methods</em> 37 (3): 498–505. <a href="https://doi.org/10.3758/BF03192720" class="uri">https://doi.org/10.3758/BF03192720</a>.</p>
</div>
<div id="ref-eerolaPerceivedComplexityWestern2006">
<p>Eerola, Tuomas, Tommi Himberg, Petri Toiviainen, and Jukka Louhivuori. 2006. “Perceived Complexity of Western and African Folk Melodies by Western and African Listeners.” <em>Psychology of Music</em> 34 (3): 337–71. <a href="https://doi.org/10.1177/0305735606064842" class="uri">https://doi.org/10.1177/0305735606064842</a>.</p>
</div>
<div id="ref-harrisonModellingMelodicDiscrimination2016">
<p>Harrison, Peter M.C., Jason Jiří Musil, and Daniel Müllensiefen. 2016. “Modelling Melodic Discrimination Tests: Descriptive and Explanatory Approaches.” <em>Journal of New Music Research</em> 45 (3): 265–80. <a href="https://doi.org/10.1080/09298215.2016.1197953" class="uri">https://doi.org/10.1080/09298215.2016.1197953</a>.</p>
</div>
<div id="ref-bakerPerceptionLeitmotivesRichard2017">
<p>Baker, David J., and Daniel Müllensiefen. 2017. “Perception of Leitmotives in Richard Wagner’s Der Ring Des Nibelungen.” <em>Frontiers in Psychology</em> 8 (May). <a href="https://doi.org/10.3389/fpsyg.2017.00662" class="uri">https://doi.org/10.3389/fpsyg.2017.00662</a>.</p>
</div>
<div id="ref-cowanWorkingMemoryCapacity2005">
<p>Cowan, Nelson. 2005. <em>Working Memory Capacity</em>. Working Memory Capacity. New York, NY, US: Psychology Press. <a href="https://doi.org/10.4324/9780203342398" class="uri">https://doi.org/10.4324/9780203342398</a>.</p>
</div>
<div id="ref-millerInformationMemory1956">
<p>Miller, George A. 1956. “Information and Memory.” <em>Scientific American</em> 195 (2): 42–46. <a href="https://doi.org/10.1038/scientificamerican0856-42" class="uri">https://doi.org/10.1038/scientificamerican0856-42</a>.</p>
</div>
<div id="ref-pearceStatisticalLearningProbabilistic2018a">
<p>Pearce, Marcus T. 2018. “Statistical Learning and Probabilistic Prediction in Music Cognition: Mechanisms of Stylistic Enculturation: Enculturation: Statistical Learning and Prediction.” <em>Annals of the New York Academy of Sciences</em> 1423 (1): 378–95. <a href="https://doi.org/10.1111/nyas.13654" class="uri">https://doi.org/10.1111/nyas.13654</a>.</p>
</div>
<div id="ref-mullensiefenFantasticFeatureANalysis2009">
<p>Mullensiefen, Daniel. 2009. “Fantastic: Feature ANalysis Technology Accessing STatistics (in a Corpus): Technical Report V1.5.”</p>
</div>
<div id="ref-kopiezAufSucheNach2011">
<p>Kopiez, Reinhard, and Daniel Mullensiefen. 2011. “Auf Der Suche Nach Den ‘Popularitätsfaktoren’ in Den Song-Melodien Des Beatles-Albums Revolver: Eine Computergestützte Feature-Analyse [in Search of Features Explaining the Popularity of the Tunes from the Beatles Album Revolver: A Computer-Assisted Feature Analysis].” <em>Musik Und Popularitat. Beitrage Zu Einer Kulturgeschichte Zwischen</em>, 207–25.</p>
</div>
<div id="ref-mullensiefenRoleFeaturesContext2014">
<p>Müllensiefen, Daniel, and Andrea R. Halpern. 2014. “The Role of Features and Context in Recognition of Novel Melodies.” <em>Music Perception: An Interdisciplinary Journal</em> 31 (5): 418–35. <a href="https://doi.org/10.1525/mp.2014.31.5.418" class="uri">https://doi.org/10.1525/mp.2014.31.5.418</a>.</p>
</div>
<div id="ref-jakubowskiDissectingEarwormMelodic2017">
<p>Jakubowski, Kelly, Sebastian Finkel, Lauren Stewart, and Daniel Müllensiefen. 2017. “Dissecting an Earworm: Melodic Features and Song Popularity Predict Involuntary Musical Imagery.” <em>Journal of Aesthetics, Creativity, and the Arts</em> 11 (2): 112–35.</p>
</div>
<div id="ref-janssenPredictingVariationFolk2017">
<p>Janssen, Berit, John A. Burgoyne, and Henkjan Honing. 2017. “Predicting Variation of Folk Songs: A Corpus Analysis Study on the Memorability of Melodies.” <em>Frontiers in Psychology</em> 8 (April). <a href="https://doi.org/10.3389/fpsyg.2017.00621" class="uri">https://doi.org/10.3389/fpsyg.2017.00621</a>.</p>
</div>
<div id="ref-rainsfordDistinctivenessEffectRecognition2019">
<p>Rainsford, Miriam, Matthew A. Palmer, and James D. Sauer. 2019. “The Distinctiveness Effect in the Recognition of Whole Melodies.” <em>Music Perception: An Interdisciplinary Journal</em> 36 (3): 253–72. <a href="https://doi.org/10.1525/mp.2019.36.3.253" class="uri">https://doi.org/10.1525/mp.2019.36.3.253</a>.</p>
</div>
<div id="ref-rainsfordMUSOSMUsicSOftware2018">
<p>Rainsford, M., M. A. Palmer, and G. Paine. 2018. “The MUSOS (MUsic SOftware System) Toolkit: A Computer-Based, Open Source Application for Testing Memory for Melodies.” <em>Behavior Research Methods</em> 50 (2): 684–702. <a href="https://doi.org/10.3758/s13428-017-0894-6" class="uri">https://doi.org/10.3758/s13428-017-0894-6</a>.</p>
</div>
<div id="ref-taylorStrategiesMemoryShort1983">
<p>Taylor, Jack A., and Randall G. Pembrook. 1983. “Strategies in Memory for Short Melodies: An Extension of Otto Ortmann’s 1933 Study.” <em>Psychomusicology: A Journal of Research in Music Cognition</em> 3 (1): 16–35. <a href="https://doi.org/10.1037/h0094258" class="uri">https://doi.org/10.1037/h0094258</a>.</p>
</div>
<div id="ref-deleeuwJsPsychJavaScriptLibrary2015">
<p>Leeuw, Joshua R. de. 2015. “jsPsych: A JavaScript Library for Creating Behavioral Experiments in a Web Browser.” <em>Behavior Research Methods</em> 47 (1): 1–12. <a href="https://doi.org/10.3758/s13428-014-0458-y" class="uri">https://doi.org/10.3758/s13428-014-0458-y</a>.</p>
</div>
<div id="ref-paneyEffectDirectingAttention2016">
<p>Paney, Andrew S. 2016. “The Effect of Directing Attention on Melodic Dictation Testing.” <em>Psychology of Music</em> 44 (1): 15–24. <a href="https://doi.org/10.1177/0305735614547409" class="uri">https://doi.org/10.1177/0305735614547409</a>.</p>
</div>
<div id="ref-halpernDifferencesAuditoryImagery2015">
<p>Halpern, Andrea R. 2015. “Differences in Auditory Imagery Self-Report Predict Neural and Behavioral Outcomes.” <em>Psychomusicology: Music, Mind, and Brain</em> 25 (1): 37–47. <a href="https://doi.org/10.1037/pmu0000081" class="uri">https://doi.org/10.1037/pmu0000081</a>.</p>
</div>
<div id="ref-ravenManualRavenProgressive1994">
<p>Raven, J. 1994. <em>Manual for Raven’s Progressive Matrices and Mill Hill Vocabulary Scales.</em></p>
</div>
<div id="ref-thurstonePrimaryMentalAbilities1938">
<p>Thurstone, L. L. 1938. <em>Primary Mental Abilities</em>. Chicago: University of Chicago Press.</p>
</div>
<div id="ref-mullensiefenMusicalityNonMusiciansIndex2014">
<p>Müllensiefen, Daniel, Bruno Gingras, Jason Musil, and Lauren Stewart. 2014. “The Musicality of Non-Musicians: An Index for Assessing Musical Sophistication in the General Population.” Edited by Joel Snyder. <em>PLoS ONE</em> 9 (2): e89642. <a href="https://doi.org/10.1371/journal.pone.0089642" class="uri">https://doi.org/10.1371/journal.pone.0089642</a>.</p>
</div>
<div id="ref-rentfrowReMiEveryday2003">
<p>Rentfrow, Peter J., and Samuel D. Gosling. 2003. “The Do Re Mi’s of Everyday Life: The Structure and Personality Correlates of Music Preferences.” <em>Journal of Personality and Social Psychology</em> 84 (6): 1236–56. <a href="https://doi.org/10.1037/0022-3514.84.6.1236" class="uri">https://doi.org/10.1037/0022-3514.84.6.1236</a>.</p>
</div>
<div id="ref-kooGuidelineSelectingReporting2016">
<p>Koo, Terry K., and Mae Y. Li. 2016. “A Guideline of Selecting and Reporting Intraclass Correlation Coefficients for Reliability Research.” <em>Journal of Chiropractic Medicine</em> 15 (2): 155–63. <a href="https://doi.org/10.1016/j.jcm.2016.02.012" class="uri">https://doi.org/10.1016/j.jcm.2016.02.012</a>.</p>
</div>
<div id="ref-R-base">
<p>R Core Team. 2017. <em>R: A Language and Environment for Statistical Computing</em>. Vienna, Austria: R Foundation for Statistical Computing. <a href="https://www.R-project.org/" class="uri">https://www.R-project.org/</a>.</p>
</div>
<div id="ref-batesFittingLinearMixedEffects2015">
<p>Bates, Douglas, Martin Mächler, Ben Bolker, and Steve Walker. 2015. “Fitting Linear Mixed-Effects Models Using Lme4.” <em>Journal of Statistical Software</em> 67 (1). <a href="https://doi.org/10.18637/jss.v067.i01" class="uri">https://doi.org/10.18637/jss.v067.i01</a>.</p>
</div>
<div id="ref-schellenbergMusicCognitiveAbilities2013">
<p>Schellenberg, E. Glenn, and Michael W. Weiss. 2013. “Music and Cognitive Abilities.” In <em>The Psychology of Music</em>, 499–550. Elsevier. <a href="https://doi.org/10.1016/B978-0-12-381460-9.00012-2" class="uri">https://doi.org/10.1016/B978-0-12-381460-9.00012-2</a>.</p>
</div>
<div id="ref-gillespieMelodicDictationScoring2001">
<p>Gillespie, Jeffrey L. 2001. “Melodic Dictation Scoring Methods: An Exploratory Study.” <em>Journal for Music Theory Pedagogy</em> 15.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="chapterfour.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="computational-model.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["mmd-draft-djb.pdf", "mmd-draft-djb.epub"],
"toc": {
"collapse": "subsection"
},
"toc_depth": 2
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
