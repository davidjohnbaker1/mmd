---
title: "What is happening in a student’s mind when they do melodic dictation?"
author: "David John Baker"
bibliography: book.bib
output:
  pdf_document: default
  html_document: default
  word_document: default
subtitle: Aubrey Hickman Award Application
biblio-style: apalike
---

## Models of Melodic Dictation

Teaching melodic dictation involves instructing students on what and where to direct their attention in order to improve their abilities. 
This process has been formalized by Gary Karpinski into four discrete steps of hearing, memorizing, understanding, and notating. 
These steps help students break down the overwhelming amount of mental processes they need to coordinate in order to successfully complete a melodic dictation [@karpinskiAuralSkillsAcquisition2000]. 
As students’ experience increases, they are able memorize larger chunks of music and often find themselves able to dictate music they once found difficult. 
But what is happening in the student’s mind over the course of aural skills instructions that allows for this growth? 

This paper puts forward a computational, cognitive model of melodic dictation with the goal of helping explain how students improve at melodic dictation. 
The model is based in research from both cognitive psychology [@cowanMagicalMysteryFour2010] and computational musicology [@pearceStatisticalLearningProbabilistic2018a] and incorporates relevant theoretical aspects such as working memory and the structure of the melody itself. 
In this paper I demonstrate how modeling the cognitive decision process during melodic dictation helps provide a precise framework for pedagogues to understand student’s inner cognition during melodic dictation and can help inform teaching practice. 

Using a cadential passage from Schubert’s Octet in D Major (D. 803), I walk through an iteration of the model and show how the model’s choices aligns with intuitions of aural skills pedagogues to establish the model’s verisimilitude. 
The model consists of three main modules:  Prior Knowledge, Selective Attention, and Transcription. 
First, the model is trained using a computational model of auditory cognition [@pearceConstructionEvaluationStatistical2005] that derives measures of expectancy based on prior listening experience. 
Second, the melody is “heard” by the computer and the incoming musical is chunked based on the information content of the melody as shown in Figure 2. 
Third, the model searches for a match within the Prior Experience and if found, the contents of Selective Attention are successfully notated. 
If not, the model truncates the chunk and recursively repeats the process. 
An entire iteration of the model is shown in Figure 3 and the entire model written in pseudocode is given in Figure 4. 

In addition to quantifying each step, the model incorporates flexible parameters that could be adjusted in order to accommodate  individual differences, while still relying on a domain general process.
By relying on cognitive mechanisms based in statistical learning, rather than a rule based system for music analysis [@lerdahlGenerativeTheoryTonal1986; @narmourAnalysisCognitionBasic1990; @narmourAnalysisCognitionMelodic1992; @temperleyCognitionBasicMusical2004] this model allows for the heterogeneity of musical experience among a diversity of music listeners.

After giving a brief overview of the model, I then argue the model’s implications for teaching melodic dictation and suggest how combining research from music cognition and music theory can help create a more linear path to success among students. 
Presenting a computational model additionally demonstrates every ontological commitment, thus making it completely amenable to criticism allowing it serve as a point of conversational departure in discussions of best practice for melodic dictation pedagogy. 
This paper directly address the recurring call [@davidbutlerWhyGulfMusic1997a; @karpinskiAuralSkillsAcquisition2000; @klonoskiImprovingDictationAuralSkills2006] to address the chasm in research between music cognition and music theory pedagogy. 

\newpage

## Levels of Abstraction

In his 2007 article _Models of Music Similarity_, Geraint Wiggins distinguishes between _descriptive_ and _explanatory_ models in describing the modeling of human behavior [@wigginsModelsMusicalSimilarity2007]. 
Descriptive models assert what will happen in response to an event.
For example, as the note density of a melody increases and the tonalness of a melody decreases, a melody may become harder to dictate (Baker, Monzingo, & Shanahan, 2018).
While the increase in note density is assumed to drive the decrease in dictation scores, merely stating that there is an established relationship between one variable and the other says nothing about the causal mechanisms involved in this process.
An explanatory model on the other hand not only describes what will happen, but additionally notes why and how this process occurs.
For example, work in musical expectation demonstrates that as an individual's exposure to a musical style increases, so does their ability to predict specific events within a given musical texture [@pearceStatisticalLearningProbabilistic2018a].

Not only does more exposure predict more accurate responses, but many of these models of musical expectation derive their underlying predictive power from the brain's ability to implicitly track statistical regularities in their auditory environment [@saffranStatisticalLearningTone1999;@margulisRepeatHowMusic2014].
The _how_ derives from the tracking of statistical regularities in musical information and the _why_ derives from evolutionary demands.
Organisms that adapt abilities to make more accurate predictions about their environment are more likely to survive and pass on their genes [@huronSweetAnticipation2006]. 

Wiggins writes that although there can be both explanatory and descriptive theories, depending on the level of abstraction, a theory may be explanatory at one level, yet descriptive at another.
Using the mind-brain dichotomy, he asserts that the example of a theory of musical expectation could be explanatory at the level of behavior as noted above, but says nothing about what is happening at the neural level.
Both descriptive and explanatory theories are needed: descriptive theories are used to test explanatory theories and by stringing together different layers of abstraction, we can arrive at a better understanding of how the world works.

Under Wiggins' framework the Karpinski model of melodic dictation [@karpinskiAuralSkillsAcquisition2000; @karpinskiModelMusicPerception1990] qualifies as a descriptive model.
The model says what happens over the time course of a melodic dictation-- specifying four discrete stages discussed above-- but does not explicitly state _how_ or _why_ this process happens. 
In order to have a more complete understanding of melodic dictation, an explanatory model is needed. 

## Model Overview

The model consists of three main modules, each with its own set of parameters:

1. _Prior Knowledge_
2. _Selective Attention_
3. _Transcription and Re-entry_

Inspired by Bayesian computational modeling, the _Prior Knowledge_ module reflects the previous knowledge an individual brings to the melodic dictation.
The _Selective Attention_-- somewhat akin to Karpinski's extractive listening-- segments incoming musical information by using the window of attention as conceptualized as the limits of working memory capacity as a sensory bottleneck to constrict the size of musical chunk that an individual could to transcribe.
Once musical material is in the focus of attention, the _Transcription_ function pattern matches against the _Prior Knowledge's_ corpus of information in order to find a match of explicitly known musical information.
The _Transcription_ function will recursively truncate what musical information is in _Selective Attention_ if no match is found. 
In addition to _Transcription_, there is also a _Re-entry_ function that will restart the entire loop.
This process reflects, but does not actually mirror the exact cognitive process used in melodic dictation, yet seems to be phenomenologically similar to the decision making process used when attempting notate novel melodies.
Based on both the prior knowledge and individual differences of the individual, the model will scale in ability, with the general retrieval mechanisms in place.
The exact details of the assumptions, parameters, and complete formula of the model are discussed below.

### Contents of the Prior Knowledge

The _Prior Knowledge_ consists of a corpus of digitally represented melodies taken to reflect the implicitly understood structural patterns in a musical style that the listener has been exposed to.
The logic of representing an individual's prior knowledge follows the assumptions of both the Statistical Learning Hypothesis (SLH) and the Probabilistic Prediction Hypothesis (PPH), both core theoretical assumptions of the Information Dynamic of Music (IDyOM) model of Marcus Pearce [@pearceConstructionEvaluationStatistical2005; @pearceStatisticalLearningProbabilistic2018a].
Using a corpus of melodies to represent an individual's prior knowledge relies on the Statistical Learning Hypothesis which states:

> musical enculturation is a process of implicit statistical learning in which listeners progressively acquire internal models of the statistical and structural regularities present in the musical styles to which they are exposed, over short (e.g., an individual piece of music) and long time scales (e.g., an entire lifetime of listening).  (Pearce, 2018, pp.2)

The logic here is that the more an individual is exposed musical material, the more they will implicitly understand it which leads the corroborating probabilistic prediction hypothesis which states: 

> while listening to new music, an enculturated listener applies models learned via the SLH to generate probabilistic predictions that enable them to organize and process their mental representations of the music and generate culturally appropriate responses. (Pearce, 2018, pp.2).

Taken together and then quantified using Shannon information content [@shannonMathematicalTheoryCommunication1948], it then becomes possible using the IDyOM framework to have a quantifiable measure that reliably predicts the amount of perceived unexpectedness in a musical melody that can change pending on the musical corpus that the model is trained on.
As a model IDyOM has been successful mirroring human behavior in melodies in various styles [@pearceStatisticalLearningProbabilistic2018a], harmony-- outperforming [@harrisonDissociatingSensoryCognitive2018] sensory models of harmony [@bigandEmpiricalEvidenceMusical2014]--, and is also being developed to handle polyphonic materials [@sauvePredictionPolyphonyModelling2017].

This model assumes that IDyOM's calculations of information content, used in past research to quantify a measure of unexpectedness, can be used as a proxy for the actual amount of information that can be held in working memory.
I make this assumption based one of the consequential outcomes of the Perceptual Fluency hypothesis [@bornsteinAttributionDiscountingPerceptual1994] which posits that stimuli an individual has been exposed to more frequently-- higher occurring n-grams-- will be processed more efficiently and thus be less of a tax on attention.

Stepping beyond the assumptions of IDyOM, the prior knowledge also needs to have a implicit/explicitly known parameter which indicates whether or not an pattern of music-- or n-gram^[n-grams refer to the amount of musical objects in a string. For example a bi-gram or 2-gram, would be an interval. Tri-grams or 3-grams would consist of two intervals and so on.] pattern-- is explicitly learned.
This threshold can be set relative to the entire distribution of all n-grams in the corpus. 

Having established that the models' first parameters to be decided are the representation of strings and the implicit/explicit threshold, the next decision that has to be made is how the model decides segmentation for the second stage of _Selective Attention_.
Although there has been a large amount of work on different ways to segment the musical surface using rule based methods [@lerdahlGenerativeTheoryTonal1986; @margulisModelMelodicExpectation2005; @narmourAnalysisCognitionBasic1990; @narmourAnalysisCognitionMelodic1992], which rely on matching a music theorist's intuition with a set of descriptive rules somewhat like the boundary formation rules put forward in _A Generative Theory of Tonal Music_, as noted by Pearce [@pearceStatisticalLearningProbabilistic2018a], rule based models often fail at when applied to music outside the Western art music canon.
Additionally, since melodic dictation is an active memory process, rather than a semi-passive process of listening, this model needs to be able to quantify musical information on two conditions.
The first is that it must be dependent on prior musical experience.
The second is that it should allow for a movable boundary for selective attention so that musical information that is in memory can be actively maintained while carrying out another cognitive process, that of notating the melody. 
In order to create this metric, I rely on IDyOM's use of information content [@shannonMathematicalTheoryCommunication1948] which quantifies the information content of melodies based on corpus of materials.

For example, when trained against a corpus of melodies, this excerpt in Figure 1 from the fourth movement of Schubert's _Octet in F Major_ (D.803) lists the information content of the excerpt calculated for each note atop the notation^[The following musical examples is taken from @pearceStatisticalLearningProbabilistic2018a reflects a model where IDyOM was configured to predict pitch with an attribute linking melodic pitch interval and chromatic scale degree (pitch and scale degree) using both the short-term and long-term models, the latter trained on 903 folk songs and chorales (data sets 1, 2, and 9 from table 4.1 in [@schaffrathEssenFolkSong1995] comprising 50,867 notes.]
Appearing in Figure 2, I plot the cumulative information content of the melody, along with both an arbitrary threshold for the limits of working memory capacity and where the subsequent segmentation boundary for musical material to be put in the _Selective Attention_ buffer would be.
These values chosen show a small example of how the _Selective Attention_ module works.
The advantage of operationalizing how an individual hears a melody like this is that melodies with lower information content, derived from an understanding of having more predictable patterns from the corpus, will allow for larger chunks to be put inside of the selective attention buffer. 
Additionally, individuals with higher working memory capacity would be able to take in more musical information.

```{r, echo=FALSE, fig.cap="Cadential Excerpt from Schubert's Octet in F Major",fig.align='center', out.width="100%"}
knitr::include_graphics("img/SchubertF.pdf")
```

```{r, echo=FALSE, fig.cap="Cumulative Information in Schubert Octet Excerpt",fig.align='center', out.width="75%"}
knitr::include_graphics("img/cumSchubert.jpeg")
```

It is important to highlight that the notes above the melody here are dependent on what is current in the _Prior Knowledge_ module.
A corpus of _Prior Knowledge_ with less melodies would theoretically lead to higher information content measures for each set of notes, while a prior knowledge that has extensive tracking of the patterns would lead to lower information content in line with the Probabilistic Prediction Hypothesis. 
This increase in predictive accuracy mathematically reflects the intuition that those with more listening experience can process greater chunks of musical information. 

### Setting Limits with Transcribe

With each note then quantified with a measure of information content, it then becomes possible to set a limit on the maximum amount of information that the individual would be able to hold in memory as defined by the _Selective Attention_ module.
A higher threshold would allow for more musical material to be put in the attentional buffer, and a lower threshold would restrict the amount of information held in an attentional buffer.
By putting a threshold on this value, this serves as something akin to a perceptual bottleneck based on the assumption that there is a capacity limit to that of working memory [@cowanEvolvingConceptionsMemory1988; @cowanMagicalMysteryFour2010].
Modulating this boundary will help provide insights into the degree to which melodic material can be retained between high and low working memory span individuals.

### Pattern Matching

With a subset of notes of the melody represented in the attentional buffer, whether or not the melody becomes notated depends on whether or not the melody or string in the buffer can be matched with a string that is explicitly known in the corpus.
Mirroring a search pattern akin to Cowan's Embedded Process model [@cowanEvolvingConceptionsMemory1988; @cowanMagicalMysteryFour2010], the individual would search across their long term memory, or _Prior Knowledge_ for anything close to or resembling the pattern in the _Selective Attention_ buffer.
Cowan's model differs from other more module based models of working memory like those of @baddeleyWorkingMemory1974 by positing that working memory should be conceptualized as a small window of conscious attention.
As an individual directs their attention to concepts represented in their long term memory, they can only spotlight a finite amount of information.
Related categorical information that is similar to the contents of the window of attention are not far from retrieval.

When searching for a pattern match, the _Transcription_ module is at work.
If a pattern match that has been moved to _Selective Attention_ is immediately found, the contents of _Selective Attention_ would be considered to be notated.
The model would register that a loop had taken place and document the n-gram match.
Of course, finding an immediate pattern match each time is highly unlikely and the model needs to be able to compensate if that happens.

If a pattern is not found in the initial search that is _explicitly_ known, one token of the n-gram would be dropped off the string and the search would happen again.
This recursive search would happen until an explicit long term memory match is made.
Like humans taking melodic dictation, the computer would have the best luck finding patterns that fall within the largest density of a corpus of intervals distribution.
Additionally, like students performing a dictation, if a student does not explicitly know an interval, or a 2-gram, the dictation would not be able to be completed. 
If this happens, both the model and student would have to move on to the next segment via the _Re-entry_ function.

Eventually there would be a successful explicit match of a string in the _Transcription_ module and that section of the melody would be considered to be dictated.
The model here would register that one iteration of the function has been run and the chunk transcribed would then be recorded.
After recording this history, the process would happen again starting at either the next note from where the model left off, the note in the entire string with the lowest information content, or n-gram left in the melody with that is most represented in the corpus.
This parameter is defined before the model is run and the question of dictation re-entry certainly warrants further research and investigation. 

Upon the successful pattern match of a string, the _Selective Attention_ and _Transcription_ module would need to then be run again.
This process is done via the _Re-entry_ function. 

## Formal Model 

Below I present the computational model in psudeocode as described in Figure 3.
First listed are the defined inputs, the functions needed to run the algorithm, and then the sequence the model runs.
To aid distinguishing between functions and objects, I put functions in italics and objects in bold.


```{r, echo=FALSE, fig.cap="Formal Model",fig.align='center', out.width="100%"}
knitr::include_graphics("img/Model.pdf")
```

```{r, echo=FALSE, fig.cap="Model Example",fig.align='center', out.width="100%"}
knitr::include_graphics("img/Model Graphic.pdf")
```
