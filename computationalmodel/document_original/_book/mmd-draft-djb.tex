\documentclass[]{book}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={Modeling Melodic Dictation},
            pdfauthor={David John Baker},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{natbib}
\bibliographystyle{apalike}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\newcommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}

  \title{Modeling Melodic Dictation}
    \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
    \author{David John Baker}
    \preauthor{\centering\large\emph}
  \postauthor{\par}
      \predate{\centering\large\emph}
  \postdate{\par}
    \date{2019-01-15}

\usepackage{booktabs}
\usepackage{amsthm}
\makeatletter
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}
\makeatother

\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## -- Attaching packages ---------------------------------- tidyverse 1.2.1 --
\end{verbatim}

\begin{verbatim}
## v ggplot2 3.1.0     v purrr   0.2.5
## v tibble  1.4.2     v dplyr   0.7.8
## v tidyr   0.8.2     v stringr 1.3.1
## v readr   1.3.1     v forcats 0.3.0
\end{verbatim}

\begin{verbatim}
## -- Conflicts ------------------------------------- tidyverse_conflicts() --
## x dplyr::filter() masks stats::filter()
## x dplyr::lag()    masks stats::lag()
\end{verbatim}

\hypertarget{significance-of-the-study}{%
\chapter{Significance of the Study}\label{significance-of-the-study}}

All students pursing a Bachelor's degree in Music from universities accredited by the National Association of Schools of Music must learn to take melodic dictation \citep[ Section VIII.6.B.2.A]{NationalAssociationSchools2018}.
Melodic dictation is a cognitively demanding process that requires students to listen to a melody, retain it in memory, and then use their knowledge of Western musical notation in order to recreate the mental image of the melody on paper in a limited time frame.
As of 2018 there are 647 Schools of Music belonging to National Association of Schools of Music (NASM) CITE WEBSITE, meaning that hundereds of students every year will be expected to learn this challenging task as part of their Aural Skills education.
The logic being that as one improves in their abiliy to take melodic dictation, this practice of critical and active listening develops as a means to improve one's ability to ``think in music'' and thus become a more compotent musician.
While learning Aural Skills has been a hallmark of being educated within the Western conservatory tradition, the rationale behind both the how and why of aural skills is often thought of as being esoteric.
Throughout the past century, people have disagreed on exactly how one does go about learning a melody with different areas of research each attacking the problem from a different angle.

Despite its ubiqiquity in curricula within School of Music settings, research on topics pertain to how aural skills are acquired is limited at best.
{[}Citations here about the cosntant calls butler, klondoski, pembrook{]}
The fields of music theory and cognitive psychology are best positioned to make progress on this question, but often the skills required to be well versed ein ither of these subjects are disparate, published in other journals, and the research with overlap is scarce.
This problem is not new and there have been repeated attempts to bridge the gap between practioners of aural skills and people in cognitive psycholgy CITES.
Literature from music theory has establisehd conceptual frameworks regarding aural skills \citet{karpinskiAuralSkillsAcquisition2000} and the relavint cognitive psychology literature has explored factors that might contribute to melodic perception (SCHMUKLER SYNERR 2016 2016), and there exists applied literature from the world of music education (CITES).

However, despite these siloed areas of research, we as music researchers do not have an a concrete understanding of exaclty what contributes to HOW individuals learn melodies (HALPERNBARLETT2010).
This is peculiar since ``how does one learn a melody'' seems to be one of the fundamental questions to the fields of music theory, music psychology, as well as music education.
Given this lack of understanding, it becomes even more peculiar that this lack of convergence of evidence is then unable to provide a solid baseline as to what student in their aural skills classrooms can be expected to do. (Also something about we should really know this if we are going to grade people on this ability).
While no single dissertation can solve any problem completely, this dissertation aims to fill the gap in the literature between aural skills practitioners (theorists and educators) and music psychologists in order to reach conclusion that can be applied systematically in pedagogical contexts.
In order to do this I draw both literatures (music and science) in order to demonstrate how tools from both cognitive psychology as well as computational musicology can help move both fields forward.
Some line here about if we really want to understand what is happening we need to know about causal factors going on here and have experimental manipulation and things like making models of the whole thing or talk about what Judea Pearl thinks about the ability to do some sort of causal modeling with diagrams.
Great to rely on some sort of anecdoatal evidence, but if we are going to put things on the line with our education then we need to be able to make some sort of falsifiable claims about what we are doing.
Can only do that through the lens of science.

\hypertarget{claims-about-need-to-join-the-worlds-of-theory-and-pedagogy}{%
\section{Claims about need to join the worlds of theory and pedagogy}\label{claims-about-need-to-join-the-worlds-of-theory-and-pedagogy}}

\begin{itemize}
\tightlist
\item
  \citep{davidbutlerWhyGulfMusic1997a}
\item
  \citep{klonoskiPerceptualLearningHierarchy2000} - perceptual hierarchy, not enough info from aural skills training
\item
  \citep{karpinskiAuralSkillsAcquisition2000} - ``There is indeed a gap between the disciples of music cognition and aural skills training'', GK says that one of his goals is to bridge that gap, and he does.
\end{itemize}

\hypertarget{chapter-overview}{%
\section{Chapter Overview}\label{chapter-overview}}

In this first chapter, I introduce the process of melodic dictation and discuss factors that would presumably could play a role in taking melodic dictation.
The chapter introduces both a theoretical backgorund and rationale for using method form both computational musicology and congitive psychology in order ot answr quesitona bout how individuals learn melodies.
I argue that tools for understanding this best because as we currently understand it, I see us operating in a Kuhnian normal science where much can be learned by just using the tools in front of us.
This chapter will clearly outline the factors hypothesized to contribute to an individual's abilit to learn melodies, incorporating both individual and musical parameters.
The chapter ends with a discussion some of the philosophical/theoretical problems with attempting to measure thigns like this (is it just a party trick?) and establishes that I will be taking a more polymorphic view of musicianship in order to answer this question.

The second chapter of my dissertation focuses on the history and current state of aural skills pedagogy.

Tracing back its origins to the practical need to teach musical skills back with Guido d'Arezzo, I compare
and contrast the different methodological approaches that have been used, along with their goals.

The third chapter discusses previous work that examines individual factors thought to contribute to one's
ability to perform an aural skills task, and it will discuss results from an experiment contributing to a discussion of how individual differences could contribute to how a person learns melodies.

Turning away from individual differences and focusing on musical features, in the fourth chapter I plan to
discuss how music researchers can use tools from computational musicology as predictive features of
melodies.
Inspired by work from computational linguistics and information theory, recent work in
computational musicology has developed software capable of abstracting features thought to be
important to learning melodies, such as note density and `tonalness' (Müllensiefen, 2009).
Talk a bit about how this has been also looked at before in the music education community.

While these features have been used in large scale, exploratory studies, work in this chapter will discuss how these features could be used in controlled, experimental studies as a stand-in for the intuition many music pedagogues have when determining difficulty of a melody in a classroom setting.

In my fifth chapter, I introduce a novel corpus of over 600 digitized melodies encoded in a queryable
format.
This dataset will also serve as a valuable resource for future researchers in music, psychology,
and the digital humanities.
This chapter begins with a discussion of the history of corpus studies, noting their origin outside of music, their current state in music, and their limitations.
This chapter, encapsulating the encoding process, the sampling criteria, and the situation of corpus methodologies within the broader research area, will go over summary data and also talk about how it could be used to generate hypotheses for future experiemnts (n-gram stuff based on patterns) .

Lastly, in the final chapter, I will synthesize the previous research in a series of melodic dictation
experiments.
Stimuli for the experiments are selected based on the abstracted features of the melodies
and are manipulated as independent variables based on the previous theoretical literature.
I then model responses from the experiments using both individual factors and musical features in order to predict how well an individual performs in behavioral tasks similar to some of my previously published research (Baker \& Müllensiefen, 2017).
Here I also note important caveats in scoring melodic dictation, referencing some other of my own work on using metrics, such as edit distance (Baker \& Shanahan, 2018), to discuss similarities between the correct answer and an individual's attempts at dictation.
Results from the final chapter will be discussed with reference to how findings are applicable to
pedagoges in aural skills settings.
Recommendations will be made building on current conceptual frameworks (Karpinski, 2000).

\hypertarget{intro}{%
\chapter{Theoretical Background and Rationale}\label{intro}}

\hypertarget{what-is-melodic-dictation-and-why}{%
\section{What is melodic dictation? and Why?}\label{what-is-melodic-dictation-and-why}}

Melodic dictation is the process in which an individual hears a melody, retains it in memory, and then uses their knowledge of Western musical notation to recreate the mental image of the melody on paper in a limited time frame.
For many, becoming proficient at this task is at the core of developing one's aural skills \citep{karpinskiModelMusicPerception1990}.
For over a century, music pedagogues have valued melodic dictation\footnote{In his highly influential book \emph{Aural Skills Acquisition: The Development of Listening, Reading, and Performing Skills in College-Level Musicians}, \citet{karpinskiAuralSkillsAcquisition2000} documents this sentiment in music pedagogy circles by highlighting poetic adages from Romantic composer Robert Schumann in the mid 19th century through 21st century music educator Charles Elliott in the opening of his book, thus providing concrete examples of the belief that improving one's aural skills, or \emph{ear}, is a highly sought after advanced skill.} which is evident from the fact that most aural skills texts with content devoted to honing one's listening skills have sections on melodic dictation \citep{karpinskiAuralSkillsAcquisition2000}.
Additionally, any school accredited by the National Association of Schools of Music in North America requires students to learn this skill \citep[ §VIII.6.B.2.A]{NationalAssociationSchools2018}.

Yet despite this tradition and ubiquity, the rationales as to \emph{why} it is important for students to learn this ability often comes from some sort of appeal to tradition or underwhelming anecdotal evidence.
The argument tends to go that time spent learning to take melodic dictation results in increases in near transfer abilities after an individual acquires a certain degree of proficiency learning to take melodic dictation.
Rationales given for why students should learn melodic dictation has even been described by Gary Karpinski as being based on ``comparatively vague aphorisms about mental relationships and intelligent listening'' \citep[p.192]{karpinskiModelMusicPerception1990}, thus leaving the evidence for the argument for learning to take melodic dictation not being well supported.

Some researchers have taken a more skeptical stance and asserted that the rationale for why we teach melodic dictation deserves more critique.
For example, Klonoski in writing about aural skills education aptly questions ``What specific deficiency is revealed with an incorrect response in melodic dictation settings?'' \citep{klonoskiImprovingDictationAuralSkills2006}.
Earlier researchers like Potter, in their own publications, have noted how they have been baffled that many musicians do not actually keep up with their melodic dictation abilities after the class ends \citep{potterIdentifyingSucessfulDictation1990}, but presumably go on to have successful and fulfilling musical lives.
Additionally, suggesting that people who can hear music and then are unable to write it down, thus are unable to think \emph{in} music \citep{karpinskiAuralSkillsAcquisition2000}, seems somewhat exclusionary to musical cultures that do not depend on any sort of written notation.

Though despite this skepticism towards the topic, melodic dictation remains at the forefront of many aural skills classrooms.
The act of becoming better at this skill may or may not lead to large in increases in far transfer of ability, but used as a pedagogical tool, teaching students to take melodic dictation brings with it concepts that have been deemed relevant to the core of undergraduate music training.
While there has not been extensive research on melodic dictation research in recent years-- in fact \citet{paneyEffectDirectingAttention2016} notes that since 2000, only four studies were published that directly examined melodic dictation-- this skill set sits on the border between literature on learning, melodic perception, memory, and music theory pedagogy.
Understanding and modeling exactly how melodic dictation works remains as a untapped watershed of knowledge for the field of music theory, music education, and music perception and is deserving of much more attention.

In this chapter I examine literature both directly and indirectly related to melodic dictation by first reviewing the prominent four step model put forth by Karpinski in order to establish and describe what melodic dictation is.
After describing his model, I then critique what this model lacks and clarify what is missing by providing a taxonomy of parameters that presumably would contribute to an individual's ability to take melodic dictation.
Using this taxonomy, I then review relevant literature and assert that the next steps forward in understanding how melodic dictation works come from examing the process both experimentally and computationally.
It has been nearly two decades since \emph{Aural Skills Acquistion} was first published as the first major step to finally build a bridge between the field of music cognition and music theory pedagogy \citep{davidbutlerWhyGulfMusic1997a, karpinskiAuralSkillsAcquisition2000, klonoskiPerceptualLearningHierarchy2000} and as with all public works, they need to be maintained?\footnote{Yes I know this is an awful metaphor and I will change it eventually}

\hypertarget{describing-melodic-dictation}{%
\subsection{Describing Melodic Dictation}\label{describing-melodic-dictation}}

Much of the foundational theoretical work on the topic of melodic dictation comes from Gary Karpinski.
Summarized most recently in his \emph{Aural Skills Acquisition} \citep{karpinskiAuralSkillsAcquisition2000}-- though first presented in an earlier article \citep{karpinskiModelMusicPerception1990}-- Karpinski proposes a four-step model of melodic dictation.\footnote{This four stage process sythesizes earlier research where in Karpinski 1990 he notes two other models of melodic dictation, one from Rogers where he says there are 2 processes, another from Thomas who says there are 15}

The four steps of Karpinski's model include

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Hearing
\item
  Short Term Melodic Memory
\item
  Musical Understanding
\item
  Notation
\end{enumerate}

and occur as a looping process depicted in Figure \ref{fig:flowchart}.
The model is discussed extensively in both this original article \citep{karpinskiModelMusicPerception1990} and throughout the third chapter in his book \citep{karpinskiAuralSkillsAcquisition2000}.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/karpinski31} 

}

\caption{Karpinski Idealized Flowchart of Melodic Dictation}\label{fig:flowchart}
\end{figure}

Karpinski's \textbf{hearing} stage involves the initial perceptions of the sound at the psychoacoustical level and the listener's \emph{attention} to the incoming musical information.
If the listener is not actively engaging in the task because of factors such as ``boredom, lack of discipline, test anxiety, attention deficit disorder, or any number of other causes'' then any further processes later down the model will be detrimentally effected.
Karpinski notes that these types of interferences are normally ``beyond the traditional jurisdiction of aural skills instruction'', but I will later argue that the concept of willful attention, when re-conceptualized as working memory, may actually play a larger role in the melodic dictation process than is claimed here.

The \textbf{short-term melodic memory} stage in his process is where musical material is held in active memory.
From Figure 1 it appears that the stage is not conceptualized as an active process where something like active rehearsal would occur, but rather just consists soley of passive mental representation.
Though Karpinski does not posit any sort of active process in the short term melodic memory stage, he does suggest there are two separate memory encoding mechanisms, one for contour, and one for pitch.
He arrives at these two mechanisms by using both empirical qualitative interview evidece as well as noting literature from music perception that supports this claim for contour \citep{dowlingScaleContourTwo1978, dewittRecognitionNovelMelodies1986} and literature suggesting that memory for melodic material is dependant on enculturation \citep{ouraMemoryMelodiesSubjects1988, handelListeningIntroductionPerception1989, dowlingExpectancyAttentionMelody1990}.
Since its publication in 2000, this area of research has expanded with other reearchers also demonstrating the effects of musical acculturation via exposure \citep{eerolaExpectancySamiYoiks2009, stevensMusicPerceptionCognition2012, pearceAuditoryExpectationInformation2012}.

In describing the short term melodic memory stage, Karpinksi also details two processes that he believes to be nesscary for this part of melodic dictation: extractive listening and chunking.
Noting that there is probably some sort of capacity limit to the perception of musical material, citing Miller \citeyearpar{millerMagicalNumberSeven1956}, Karpinski explains how each strategy might be used.
Extractive listening is the process in which someone dictating the melody will selectivly remember only a small part of the melody in order to lessen the load on memory.
Chunking is the process in which smaller musical elements can be fused together in order to expand how much information can be actively held in memory and manipulated.
The concept of chunking is very helpful as a pedagogical tool, but as detailed below, is a complicated concept to pin down how it works.

After some musical material is extracted, then represented in memory, the next step in the process is \textbf{musical understanding}.
At this point in the dictation the dictator needs to take the extracted musical material that is represented in memory and the use their music theoretic knowledge in order to comprehend any sort of hierarchical relationships between notes, common rhythmic groupings, or any sorts of tonal functions.
This is the point in the process where solimization of either or both pitch and rhythm, and musical material might be understood in terms of relative pitch.
In the model solimization takes place later, but it is worth questioning if it is possible to dissacociate relative pitch relations from the qualia of the tones themselves \citep{arthurPerceptualStudyScaledegree2018}.
For Karpinski, the more quickly what is represented in musical memory can be understood, the more quickly it can then be tranlated at the final step of notation.

\textbf{Notation}, the final step of the dictation loop, requires that the individual taking the notation have sufficient knowledge of Western musical notation so that they are able to translate their musical understanding into written notation.
This last step is ripe for errors and has proved problematic for researchers attempting to study dictation \citep{taylorStrategiesMemoryShort1983, klonoskiImprovingDictationAuralSkills2006}.
It is also worth highlighting is that it is difficult to notate musical material if the individual who is dictating does not have the requisite musical category and knowledge for the sounds.
Lack of this knowledge will limit an individual's ability to translate what is in their short term melodic memory into notation, even if it is perfectly represented in memory!

The final parts of the chapter, Karpinski notes that other factors like \textbf{tempo}, the \textbf{length and number of playings}, and the \textbf{duration between playings} will also play a role in determining how an individual will perform on a melodic dictation.
While this framework can help illuminate this cognitive process and help pedagogues understand how to best help their students, presumably there are many more factors that contribute to this process.
The model as it stands is not detailed enough for explanatory purposes and lacks in two areas that would need to be expanded if this model were to be explored experimentally and computationally.

First, having a single model for melodic dictation assumes that all individuals are likely to engage in this sequential ordering of events.
This could in fact be the case\footnote{And in his Figure 3.1 he does caption it as an \emph{idealized} dictation process}, but there is research from music perception \citep{goldmanImprovisationExperiencePredicts2018a} and other areas of memory psychology such as work on expert chess players \citep{laneChessKnowledgePredicts2018} that suggests that as individuals gain more expertise, their processing and categorization of information changes.
Additionally, different individuals will most likely have different experiences dictating melodies based on their own past listening experience, an area that Karpinski refers to when citing literature on musical enculturation based on statistical exposure.
The model does not have any flexibility in terms of individual differences.

Second, the model presumes the same sequence of events for every melody.
As a general heurstic for communicating the process, this process is generalizable, but intuition would suggest that treating all melodies the same is not going to lead to having a robust model.
For example, on page 103, Karpinski suggest that two listenings should be adequate for a listener with few to no chunking skills to listen to be able to dictate a melody of twelve to twenty notes.
This process might generalize to many tonal melodies, but presumably different strategies in recognition would be involved in dictating the two melodies of equal length shown in Figure \ref{fig:shortmelody1} and \ref{fig:shortmelody2}.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img/musicalexamples/MMD_Figure2-1} 

}

\caption{Tonal Melody}\label{fig:shortmelody1}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img/musicalexamples/MMD_Figure2-1} 

}

\caption{Atonal Melody}\label{fig:shortmelody2}
\end{figure}

Presumably different people with different levels of abilities will perform differently on different melodies and while helpful as a pedaogogical tool, this one size-fits-all approach to melodic dictation is not robust.
This agnosticism for both variability for melodic and individual differences serves as a stepping off point for this study.
In order to have a more through understanding of melodic dictation, there needs to be a model that is able to accomodate the exhaustive differences at both the individual and musical levels.
Additionally, the model should be able to be operationalized so that it can be explored in both experimental and computational settings.
By explicitly stating variables thought to contribute and noting how melodic dictation works, it will give the community a better sense of the melodic dictation process, which will then enable a more through understanding of melodic perception and subsequently allow for better teaching practices in aural skills classrooms.

At this point, it is worth stepping back and noting that the sheer amount of variables at play here is cumbersome and almost haphazard.
In order to better understand and organize factors thought to contribute to this process, it would advantagous for future research to taxonomize the multitude of features thought to contribute to melodic dictation.
In doing this, it wil allow for a clearer picture of what factors might contribute and what literatures to explore in order to learn more about them.

The taxonomy that I propose appears in Figure \ref{fig:taxonomy} and bifucates the possible factors thought to affect an individual's ability to take melodic dictation into two categories: \textbf{individual} parameters and \textbf{musical} parameters.
Each of these two categories can then be split again into \textbf{cognitive} and \textbf{environmental} parameters as well as \textbf{structural} and \textbf{experimental factors} respectively.
Below I expand on what these categories entail, then explore each in depth.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img/taxonomy4} 

}

\caption{Taxonomy of Factors Contributing to Aural Skills}\label{fig:taxonomy}
\end{figure}

The \textbf{individual} parameters split broadly into \textbf{cognitive factors}, or factors of people that are relatively consistent with people over time and could be understood as largely being governed by \emph{nature}.
The other category of this division consist of factors that change with training and exposure and could be understand as largely being governed by \emph{nurture}.
This second set of parameters are the \textbf{environmental} factors.
These categories are not deterministic, nor exclusive, and almost inevitably interact with one another.\footnote{Could footnote this interaction and talk about how AP people generally need to have genetic predisposition, fixed pitch instrumetn in house, and start at early age CITATION FOR THAT?}

For example, it would be possible to imagine an individual with higher cognitive ability, the oppertunity to have a high degree of training early on in their musical career, and personality traits that are associated with higher learning aptitudes.
This individual's musical perception abilities might be markedly different than someone with lower cognitive abilities, no opertunity for individualized training, come from a lower socio-economic status, and not have a general inclination to even take music lessons.
This variability at the individual level might then lead to differences in their ability to take melodic dictation.

Complementing the individual differences, there would also be differences at the \textbf{musical} level which in turn divides into two categories.
On one hand exists the \textbf{structural aspects} of the melody itself.
These are aspects of the melody that would remain invariant when written down on a score.
Parameters in this category would include features generated by the interval structure of the pitches over time that allow the melody to be categorically distinct from other melodies.
These structural features are then complimented by the \textbf{experimental features} which are emergent properties of the structrual relation of the pitches over time based on performance pratice choices.
Examples of these parameters would include, key, tempo, note density, timbral qualities, and the amount of times a melody is played during a melodic dictation or emergent properties like a melody's tonalness as computed through various metrics.
This division is not an exhaustive, categorical divide.
One could imagine exceptions to these rules where a melody is tranformed to the minor key, ornamented, and then played with extenive rubato and experienced as a phenomenologically similiar experience.

Given all of these parameters that could contribute to the melodic dictation process, the remainder of this chapter will exploring literature using this taxonomy as a guide.
The chapter conculdes with a reflection on operationalizing each of these factors and problems that can arise in modeling and reminds the reader about the dangers of statistical reification.
These are important to note since from an empirical standpoint, both the task as well as the process of melodic dictation as depicted by Karpinksi resemble something that could be operationalized as both an experiment, as well as a computational model and if understood this way will be subjectd to the same types of critique.

\hypertarget{individual-factors}{%
\section{Individual Factors}\label{individual-factors}}

\hypertarget{cognitive}{%
\subsection{Cognitive}\label{cognitive}}

Research from cognitive psychology suggests that individuals differ in their perceptual and cognitive abilities in ways that are both stable throughout a lifetime and are not easily influened by short term traning.
When investigated on a large scale, these abilities-- such as general intelligence or working memory capacity-- predict a wealth of human behavior on a large scale ranging from longevity, annual income, ability to deal with stressful life events, and even the onset of Alzheimer's disease \citep{ritchieIntelligenceAllThat2015, unsworthAutomatedVersionOperation2005}.
Given the strength and generality of these predictors, it is worth investigating the extent that these abilities might contribute when investigating any modeling of melodic dictation.
It is imporant to understand the degree to which these cognitive factors might influence aural skills abilities in order to ensure that the types of assesments that are given in music schools validly measure abilities that individuals have the ability to improve upon.
If it is the case that much of the variance in a student's aural skills grades can be attributed to something the student has little control over, this would call for a serious upheaval of the current model of aural skills teaching and assesment.

Recently there has been a surge of interest in this area \footnote{DO I CITE THINGS HERE LIKE THAT WORK OF NANCY ROGERS, LEIGH VAN HANDLE, THE UTAH GUY, GARY KARPINSKIS ICMPC, THE FORM AT SMPC} which could be attributed to the fact that educators are picking up on the fact that cognitive abilities are powerful predictors and need to be understood since they inevitably will play a role in pedagogical settings.

Before diving into a discussion regarding differences in cognitive ability, I should note that sometimes ideas regarding differences in cognitive ability been hostily received (citation against people talking about IQ) and for good reasons.
Research in this area can and has been taken advantage to further specious ideologies, but often arguments that assert meaningful differences in cognitive abilities between groups are founded on statistical misunderstandings and have been debunked in other literature \citep{gouldMismeasureMan1996}.
Considering that, it then becomes very difficult to maintain a scientific commitment to the theory of evolution \citep{darwinOriginSpecies1859} and not expect variation in all aspects of human behavior, with cognition falling within that umbrella.
Even given this statement, measuring a theoretical construct such as an aspect of cognition desereves to be examined since the ability to validly and reliably measure an individual's cognitive ability is a fundmental assumption of this study.

\hypertarget{measuring-intelligence}{%
\subsection{Measuring Intelligence}\label{measuring-intelligence}}

Attempting to measure and quantify aspects of cognition go back over a century.
Even before concepts of intelligence were posited by Charles Spearman and his conception of \emph{g} \citep{spearmanGeneralIntelligenceObjectively1904}, scientists were interested in establishing links between an individual's mental capacities and some sort of physical manifestation.
The origins of this area of research have been critiqued on the basis that the early work implicitly tended to validate preconceptualized beliefs on the superiority of certain groups of peoples and used methodlogies that today would be considered risible.

\begin{itemize}
\tightlist
\item
  For example, BROCA thought he could get at intelligence by measuring skulls AND MORTON
\item
  Or Spitzka who post hoc measured eminance and brain size page 127
\end{itemize}

While not immediatly relevant to current thinking in cognitive psychology, work from both Broca and XXX was continued by the American herediterian school of IQ (page 187 in Gould) and the early research done by Alfred Binet on IQ took inspiration from Broca.
This lineage of ideas has often been used to tarnish systematic investigations into differences in cognitive ability, which from their outset were to initially funded by the French governement to identify children struggling in the classroom so that they could be given special attention.

Bient was the initial developer of the idea of an intelligence quotient or IQ\footnote{divide mental age by chronological age then multiply by 100} and provided one of the first ways to attempt to quantify a theoretical concept that was not capable of being manifested in the physical world.
It was also around the same time that researchers like Cyril Burt and Charles Spearmean began developing their new theories of intelligence founded on the reification of factor analysis.
In developing a battry of tests whose performance on one subtest could often reliabily predict performance on another-- a manifestation referred to as the postive manifold-- Spearman and Burt put forth a separate conception of intelligence based on the ability to solve problems without any sort of background information and referred to this ability a \emph{g} for general intellgience.

Though seemingly unrelated to the current state of thinking about cognitive abilities, Binet's and Spearman's ideologies about what intelligence is and how to measure it still represent two of the larger schools on cognitive ability.
On one hand their idea that cognitive abilities are based upon a steady growth of incoming information that someone is able to manipulate once they retrieve from long term memor; on the other hand there is a school of thought that there is some sort of measureable construct, \emph{g} that aids in the process of solving problems that do not depend on any sort of contextuxal information.
Conceptualizing cognitive ability as these two different constructs inevitably leads to different types of measurements and subsequently what these constructs are then able to predict in terms of human behavior.

Without detailing entire histories of both lines of thought, Binet's conceptulization manifested into an argument for general crystallized intelligence or \emph{Gc}, or the ability to solve problems based on previously acquired skills.
Spearman and Burt's ideas about \emph{g} school reflect a belief that individuals have some sort of latent cognitive ability to draw on to perform mental tasks.
The cognitive psychology literature has noted that \emph{g} often shares a statistically equivalent relationship to idea conceptualized as general fluid intelligence \emph{Gf}, or the ability to solve problems in novel situations Cattell, 1971; Horn, 1994).
This distinction between \emph{Gf} and \emph{Gc} is different than that of \emph{g}, but again it should be noted that \emph{Gf} and \emph{g} share a statistically identical relationship Matzke, Dolan, and Molenaar (2010).
These conceptions of intelligenec and cognitive ability aslo differ from more current theories that sythesize these previous areas of research \citep{kovacsProcessOverlapTheory2016}.

Even though both of these constructs are powerful predictors on a large scale and do predict things like educational success, income, and even life expectancy \citep{ritchieIntelligenceAllThat2015}-- even when other variables like socioeconomic status are held constant.
Yet despite this, only conceptualizing cognitive abilities in terms of intelligence does not fully explain the diversity of human cognition.

Another large area in the field of cognitive psychology is the area of working memory capacity.
In addition to concepts of intelligence, be it \emph{Gf} or \emph{Gc}, the working memory capacity literature also is directly relevant to work on melodic dictation for reasons discussed below.

\hypertarget{working-memory-capacity}{%
\subsection{Working Memory Capacity}\label{working-memory-capacity}}

Working memory is one of the most investigated concepts in the cognitive psychology literature.
According to Nelson Cowan, the term working memory generally refers to

\begin{quote}
the relatively small amount of information that one can hold in mind, attend to, or, technically speaking, maintain in a rapidly accessible state at one time. The term working is mean to indicate that mental work requires the use of such information. (p.1) \citep{cowanWorkingMemoryCapacity2005}
\end{quote}

The term, like most concepts in science, does not have an exact definition, nor does it have a definitive method of measurement.
While there is no universally recognized first use of the term, researchers began to postulate that there was some sort of system that mediated incoming sensory information with the world with the information in long term storage using modular models of memory in the mid-twentieth century.
Summarized in \citep{cowanWorkingMemoryCapacity2005}, one of the first modal models of memory was proposed by \citep{broadbentPerceptionCommunication1958} and later expanded by \citep{atkinsonHUMANMEMORYPROPOSED1968}.
As seen in Figure \ref{fig:wmmodels}, both models here posit incoming information that is then put into some sort of limited capacity store.
These modal models were then expanded on by Baddeley and Hitch \citep{baddeleyWorkingMemory1974} in their 1974 chapter with the name \emph{Working Memory}, where they proposed a system with an central executive module that was able to carry out active maintenance and rehearsal of information that could be stored in either a phonological store for sounds or a visual sketchpad for images.

\begin{figure}

{\centering \includegraphics[width=9.81in]{img/wm_models} 

}

\caption{Schematics of Models of Working Memory taken from Cowan, 2005}\label{fig:wmmodels}
\end{figure}

Later revisions of their model also incorporated an episodic buffer \citep{baddeleyEpisodicBufferNew2000} where the modules were explicitly depicted as being able to interface with long term memory in the rehearsal processes.
The model has even been expanded upon by other researchers throughout its lifetime.
The most relevant to this study is by \citep{berzWorkingMemoryMusic1995}, who postulated adding a musical rehearsal loop to the already established phonological loop and visual spatial sketchpad.
While Berz is most likely correct in asserting that the nature of storing and processing musical information is different to that of words or pictures and there has been experimental evidence to suggest this \citep{williamsonMusiciansNonmusiciansShortterm2010} that has been interpreted in favor of multiple loops \citep{wollnerAttentionalFlexibilityMemory2016} , it does introduce the theoretical problem of multiple stores which has been addressed by other researchers.

In addressing the problem of explicitly stating which rehearsal loops do and do not exist, Nelson Cowan proposed a separate model \citep{cowanEvolvingConceptionsMemory1988, cowanWorkingMemoryCapacity2005} dubbed the Embedded Process Model which do not claim the existence of any domain specific module (e.g.~positing a phonological loop, visual spatial sketchpad) but is rather based on an exhaustive model that did away with the problem of asserting specific buffers for new types of information.

In Cowan's own words comparing his model from that of Baddeley:

\begin{quote}
The aim was to see if the description of the processing structure could be exhaustive, even if not complete, in detail. By analogy, consider two descriptions of a house that has not been explored completely. Perhaps it has only been examined from the outside. Baddeley's (1986) approach to modeling can be compared with hypothesizing that there is a kitchen, a bathroom, two equal-size square bedrooms, and a living room. This is not a bad guess, but it does not rule out the possibllity that there actually are extra bedrooms or bathroom, that the bedroom space is apportioned into two rooms very different in size, or that other rooms exist in the house. Cowan's (1988) apporach, on the other hand, can be compared with hypothesizing that the house includes food preparating quarters, sleeping quarters, batroom/toilet quarters, and other living quarters. It is meant to be exhaustive in that nothing was left out, even though it is noncommital on the details of some of the rooms. p.42 Cowan, 2005.
\end{quote}

The system is depicted in the bottom tier of TABLE X, and conceptualizes the limited amount of information that is readily available as being in the focus of attention, with activated sensory and categorical features of what is in the focus of attention to be accessible nearby.
Moving further from the locus of attention is long term memory, whose content can be turned to by using the central executive to access non-immediately available information.
In contrast to the modular approaches, Cowan's framework does not require the researchers to specify exactly how and where each the incoming information is being stored which makes it advantageous for studying complex stimuli such as music and melodies.

In addition to having multiple frameworks for studying working memory capacity, there is also the problem of limits to the working memory system, often referred to as the working memory capacity.
Most popularized by Miller in his famous \citep{millerMagicalNumberSeven1956} speech turned article, Miller suggests out of jest that the number 7 might be worth investigating, which has been used as a point of reference for many researchers since then.
It is worth nothing that Miller has since gone on record as noting that using 7 (plus or minus 2) was a rhetorical device used to string together his speech \citep{millerHistoryPsychologyAutobiography1989}.
Nevertheless, while the number seven is most likely a red herring, it did inspire a large amount of research on capacity limits.
In the decades since the number 7 has been reduced to about 4 \citep{cowanMagicalMysteryFour2010} and research around capacity limits has been investigated using a variety of novel tasks, most noteable the complex span task CITATION.\\
When used as predictors in both higher and lower cognitive tasks, measures of working memory capacity predict performance well and additionally tend to be stable across a lifetime \citep{unsworthAutomatedVersionOperation2005}.

Given its predictive strength as well as its direct similarity to tasks of melodic dictation, a in depth look at the literature is warranted.\\
Clearly an individual's ability to take in sensory information, maintain it in memory, actively carry out other tasks (like notating said melody) are almost identical to tasks of working memory capacity.
Before venturing onward from this striking parallel, tasks investigating working memory capacity differ from melodic dictation tasks in a few key ways.
The first is that musical information is always sequential: a melodic dictation task would never require the student to recall the pitches back in scrambled orders.
Serial order recall is an important characteristic in the scoring and analyzing of working memory tasks \citep{conwayWorkingMemorySpan2005}, but musical tones do not appear in random order and are normally in discernable chunks as discussed by Karpinski\citep{karpinskiAuralSkillsAcquisition2000}.
The use of chunks is pervasive in much of the memory literature, but often is used as more of a heuristic to help explain that information in the environment and why it is often grouped together.
Of the problems with chunking, most are related to music and have relevance to melodic dictation.
Below I review the problems with chunking noted by Cowan \citep{cowanWorkingMemoryCapacity2005}, and any pertitant music psychology literature.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \emph{Chunks may have a hierarchical organization.} Tonal music has historically been understood to be hierarchical \citep{krumhanslCognitiveFoundationsMusical2001, meyerEmotionMeaningMusic1956, schenkerFreieSatz1935} with the study for memory for tones being confounded by some pitches being undestood by their relation to structurally more important tones.
\item
  \emph{The working memory load may be reduced as working memory shifts between levels in hierarchy.} If an individual understands a chunk to be something such as a major triad, the load on working memory would be less since it that information could be understood as a singular chunk.
\item
  \emph{Chunks may be the endpoints of a continuum of associations.} Pairing a group of tones together that might be functionally anomolous like\ldots{}
\item
  \emph{Chunks may include asymmetrical information.} More tonal possibilities are possible from a stable note like tonic or dominant, whereas in a tonal context, a raised scale degree \#\(\hat{4}\) when understood in a functional context would be taken as having stricter transitional probabilities (\#\(\hat{4} \rightarrow \hat{5}\)).
\item
  \emph{There may be a complex network of associations.} If a set of pitches sounds like a similar set of pitches from long term memory , the information coming in can not be understood as being separate units of working memory.
\item
  \emph{Chunks may increase in size rapidly over time.} Three tones that are seemingly unrelated when incoming like E4, G5, C5 might enter sensory perception as three different tones, but then be fused together when they are understood as one chunk-- a first inverstion major triad.
\item
  \emph{Information in working memory may benefit from rapid storage in long term memory.} Given the amount of patterns that an individual learns and can understand, as soon as something is fused, it could be encoded in long term memory, especially if there is a salient feature in the incoming melodic information such as the immediate recognition of a mode or cadence.
\end{enumerate}

The points by Cowan are important to acknowledge in that it it not possible to directly lift work and paradigms from working memory capcacity to work in music perception.
That said, the enormous amount of theoretical frameworks put forward by the working memory liteature when understood in conjunction with theories in music psychology such as implicit statistical learning \citep{saffranStatisticalLearningTone1999} can provide for new, fruitul theories.
Past reserachers have noted the strength and predictive abilities of literature from the working memory capacity as aiding research in music perception.
In ending his article positing a musical memory loop to be annexed to the Baddley and Hitch modular model of working memory, Berz \citep{berzWorkingMemoryMusic1995} captures the power of this concept in the last sentence of his article and warns future reserachers that

\begin{quote}
Individual differences portrayed in some music aptitude tests may {[}sic{]} represent not talent or musical intelligence but ability, reflecting differences in working memory capacity. p.~362
\end{quote}

Berz's assertion has not been exhausivly tested since first published in 1995, but the subject of music, memory, and cognitive abilities has been the focus of research of both psychologists and musicologists alike.
Below I survey literature bordering on both music, as well as cognitive abiltity.

\hypertarget{working-memory-capacity-and-music}{%
\subsubsection{Working Memory Capacity and Music}\label{working-memory-capacity-and-music}}

Of the papers in the music science literature that specifically investigates working memory, each uses different measures, though but all tend to converge on two general findings.
The first is that there are some sort of enhanced memory capabilities in individuals with musical training.
The second is that working memory capacity, however it is measured, often plays a significant role in musical tasks.
Evidence for the first point appears most convincingly in a recent meta analyses by Talamini and colleagues \citep{talaminiMusiciansHaveBetter2017} who demonstrated via three separate meta-analyses that musicians outperform their non-musical counterparts on tasks dealing with long-term memory, short-term memory, as well as working memory.
The authors also noted that the effects were the strongest in working memory tasks where the stimuli were tonal, which again suggests an advantage of exposure and understanding of the hierarchical organization of musical materials.
In this meta-analyss and others investigating music and cognitive ability, it is important to be reminded that the direction of causality still from these studies cannot be determined using these statistical methodologies.
While it might seem that musical training tends to lead to these increases, it is also possible that higher functioning individuals will self select into musical activities.
Even if there is no seletion bias in engaging with musical activity it also remains a possiblity that of the people that do engage with musical activity, the higher functioning individuals will be less likely to quit over a lifetime.

In terms of musical performance abilities, working memory capacity has also been shown to be a significant predictor.
Kopiez and Lee suggested that working memory capacity should to contribute to sight reading tasks based on research where they found measures of working memory capacity, as measured by a matrix span task, to significantly correlated with many of their measures hypothesized to be related to sight reading ability in pianists at lower difficulty grading \citep{kopiezDynamicModelSkills2006, kopiezGeneralModelSkills2008}.

Following up on this work on sight reading, Meinz and Hambrick \citep{meinzDeliberatePracticeNecessary2010} found that working memory capacity, as measured by an operation span task, a reading span task, rotation span task, and a matrix span task was able to predict a small amount of variance \(R^2=.074(0.067)\) above and beyond that of deliberate practice alone \(R^2=.451(.441)\) in a sight-reading task.
More recently, two studies looking at specific sub groups of musicians have shown working memory capacity to significantly contribute to models of performances on musical tasks related to novel stimuli.
\citep{wollnerAttentionalFlexibilityMemory2016} found that although no differences were found between pianists and conductors in measures of working memory capacity as measured via a set of span tasks, conductors showed superior performance in their attention flexibility.
Following up on this line of research \citep{nicholsScoreOneJazz2018} used the same battery of working memory tasks and found that jazz musicians excelled over their classically trained counterparts in a task which required them to hear notes and reproduce them on the piano.
The authors also noted that of their working memory battery, based on standard operation span methods \citep{engleWorkingMemoryCapacity2002}, that the auditory dictation condition scored surprisingly low and further research might consider further work on dictation abilities.
Additionally \citep{colleyWorkingMemoryAuditory2018} found that working memory capacity, as measured by a backwards digit span and operation span, to be successful predictors in a tapping task requiring sensory motor prediction abilities.
As mentioned above, each of these tasks where working memory was a significant predictor of performance occured where the task involved active enagement with novel musical material.

The growing evidence in this field suggests that the advantage of working memory capacity to be greatest in both musically trained people, dealing with novel information, using tonal materials.
Since all three of these factors are related to melodic dictation, it would seem sensible to continue to include these measures in tasks of musical perception and continue Berz's assertion that research in music percetion could inadvertently be picking up on individual differences in working memory abilities.

\hypertarget{general-intelligence}{%
\subsection{General Intelligence}\label{general-intelligence}}

As discussed above, the idea of IQ or intelligence has a long history that is bothgood and bad.
When used as a predictor in models it often serves to predict traits that society values like longevity and general income so given its ability to predict in more domain general settings, surveying literature where it applies to musical activity is a worthy task.
Below I use the term intelligence as a catch all term to avoid the historical context of IQ and specify where availible which measure was actually used.
Before surveying the literature here it is also worth noting that reserach on music and intelligence is not as developed as some of the larger studies looking at intelligence which provides problems for both establishing causal directionality, as well as controlling for other factors like self theories of ability, socioeconomic status, and personality \citep{mullensiefenInvestigatingImportanceSelftheories2015}.

\hypertarget{papers-that-suggest-gf-plays-a-role}{%
\subsubsection{Papers that suggest GF plays a role}\label{papers-that-suggest-gf-plays-a-role}}

\begin{itemize}
\tightlist
\item
  Redo this structure
\end{itemize}

As reviewed in \citet{schellenbergMusicNonmusicalAbilities2017}, both children and adults who engage in musical activity tend to score higher on general measurses of intelligence than their non-musical peers (Gibson, Folley and Park, 2009; Hille et al., 2011; Schellenberg, 2011a; Schellenberg and Mankarious, 2012) with the duration of training sharing a relationship with the extent of the increases in IQ (Degé, Kubicek and Schwarzer, 2011a; Degé, Wehrum, Stark and Schwarzer, 2015; Corrigall and Schellenberg, 2015; Corrigall, Schellenberg and Misura, 2013; Schellenberg, 2006).
Though many of these studies are correlational, they also have made attempts to control for confounding variables like socio-economic status and parental involvment in out of school activities. (Corrigall et al., 2013; Degé et al., 2011a; Schellenberg, 2006, 2011a, 2011b; Schellenberg and Mankarious, 2012).
Schellenberg notes the problem of smaller sample sizes in his review (Corrigall and Trainor, 2011; Parbery-Clark et al., 2011; Strait, Parbery-Clark, Hittner and Kraus, 2012) in that studies that are typically smaller do not reach statistical significance.
Schellenberg also references evidence that when professional musicians are matched with non-musicians from the general population there do not seem to be these associations (CITE).
Schellenberg's review suggests the current state of the literature points to support the hypothesis that higher functioning kids that take music lessons and they tend to stay in lessons longer.
Additionally, Schellenberg remains skeptical of any sorts of causal factors regarding increases in IQ (e.g., François et al., 2013; Moreno et al., 2009) noting methodlogical problems like how short exposure times were in studies claiming increases in effects or researchers who not holding pre-exisiting cognitive abilitis constant (Mehr, Schachner, Katz and Spelke, 2013).

\begin{itemize}
\tightlist
\item
  \citep{corrigallMusicTrainingCognition2013}
\item
  \citep{corrigallMusicTrainingCognition2013a}
\item
  \citep{swaminathanRevisitingAssociationMusic2017}
\end{itemize}

\hypertarget{environmental}{%
\subsection{Environmental}\label{environmental}}

Standing in contrast to factors that individuals do not have a much control over such as the size of their working memory capacity or factors related to their general fluid intelligence, most of the factors we believe contribute to someone's ability to take melodic dictation have to deal with factors related to training and the environment.
In fact, one of the tacit assumptions of getting a music degree revolves around the belief that with deliberate and attentive practice, that an individual is able to move from novice to expertise in their chosen domain.
The idea that time invested results in beneficial returns is probably best exemplified by work produced by \citet{ericssonRoleDeliberatePractice1993} that suggests that performance at more elite levels results from deliberate practice.
Below I review literature that supports this argument, since it's no doubt that someone has to engage in something to be good at it.

\hypertarget{musical-training}{%
\subsection{Musical Training}\label{musical-training}}

\begin{itemize}
\item
  Papers that suggest practicing makes you better?
\item
  It almost seems redundant to review literature in support of music practice leading to better results.
\item
  List of those papers here
\end{itemize}

\hypertarget{aural-training}{%
\subsection{Aural Training}\label{aural-training}}

In additon to individuals differeng in their general musical abilities-- however they are defined-- individuals also differ in their abilities at the level of their aural skills.
Though not heavily researched in the past few decades \citep{furbyEffectsPeerTutoring2016}, there has been specific research looking at explaining how people do in aural skills examinatons.
\citet{harrisonEffectsMusicalAptitude1994} examined the effect of aural skills training on undergraduate students by creating a latent variable model investigating musical aptitude, academic ability, musical expertise, and motivation to study music in a sample of 142 undergraduate students and claimed to be able to explain 73\% of the variance in aural skills abilities using the variables measured.
Work from Colin Wright's dissertation found that \ldots{} \citep{wrightInvestigatingAuralCase2016}

These are things that people have suggested people trying to do :
As noted in \citet{furbyEffectsPeerTutoring2016}, researchers in the past have suggested a variety of techniques for improving their abilities in melodic dictation by isolating rhythm and melody {[}\citet{bantonRoleVisualAuditory1995}; \citet{blandSightSingingMelodic1984}; \citet{rootMethodicalSightSingingLessons1931}; WILSON{]}, listening attentively to the melody before writing \citep{bantonRoleVisualAuditory1995}, recognizing patterns \citep{bantonRoleVisualAuditory1995, blandSightSingingMelodic1984, rootMethodicalSightSingingLessons1931} and silently vocalizing while dictating \citep{klonoskiImprovingDictationAuralSkills2006}.

\hypertarget{sight-singing}{%
\subsection{Sight Singing}\label{sight-singing}}

Often described as the other side of the same coin of melodic dictation, sight singing is an area of music pedagogy research that has had sparse attention paid to it given its prevelance in school of music curricula.
Recently \citep{fournierCognitiveStrategiesSightsinging2017a} catalouged and categorized 14 different sub categories into four larger main cateogories while also providing commenatary on some of the current state of aural skills.
Of the four large categories, they group them into reading mechanisms, sight singing, readings skills acquiskiton, and learnign support.

FIX HERE

The authors note a line of research that has documented that university students are often unprepared to sight-read single lines of music (Asmus, 2004; Davidson, Scripp \& Welsh, 1988; Fournier, 2015; Thompson, 2004; Vujović \& Bogunović, 2012) even though it is, like dictation, thought of as a means for deeper musical understanding. (DeBellis, 2005; Karpinski, 2000; Ottman, 1956; Rogers, 2004; Scripp, 1995; Scripp \& Davidson, 1994)
The authors of Fournier et. al also note that sight-reading has been an active area of research due to the often reported relationship that performance on sight reading often predicts several studies have shown links between academic success in sight-singing and predictors such as entrance tests (Harrison, 1987, 1990, 1991; Ottman, 1956; Rodeheaver, 1972; Schleuter, 1983), academic ability (Chadwick, 1933; Harrison, 1990, 1991; Harrison, Asmus, \& Serpe, 1994; Rodeheaver,1972), and musical experience (Brown, 2001; Dean, 1937; Furby, 2008; Harrison, 1990, 1991; Harrison et al., 1994; Thostenson, 1967).

Taken as a whole, the research tends to suggesting that learning to be a fluid and compotent sight reader helps musicians hone their skills by bootstrapping other musical skills since the skills needed for sightreading touch on many of the skills used in musical performance like pattern matching and listening for small changes in intonnation.
Each of these individual factors contributes in a small and significant way and additionally will interact with the other half of the taxonomy: musical parameters.

\hypertarget{musical-parameters}{%
\section{Musical Parameters}\label{musical-parameters}}

Transitioning to the other half of the taxonomy on figure X, the other main source of variation on any study looking at melodic perception, and consequently studies of melodic dictation, is the effect of the melody itself.
I find it safe to assume that not all melodies are equally difficult to dictate and assert that variance in the difficulty the melody can partitioned between both \textbf{structural} and \textbf{experimental} aspects of a melody.
As noted above, there is not a strict deliniation between these two categories since once could imagine drastic maniuplations in experimental parameters in order to result in a phenomenologically different experience of melody.
Questions of transformations of melodies and musical similarity fall have been addressed in ohter research \citep{cambouropoulosHowSimilarSimilar2009, wigginsModelsMusicalSimilarity2007} but are beyond the scope of this study.

\hypertarget{structural}{%
\subsection{Structural}\label{structural}}

The notion that the music as represented by a score is able to provide insights towards understanding aspects about music is not new to music theory and anlaysis.
Heinrich Schenker argued for an undertanding of tonal music \citep{schenkerFreieSatz1935} that asserted hierarchical relationships of notes on the musical surface in the early 20th century and has since been expanded upon by theorists over the past century SALTZER, SCHAKTER, ROTHSTEIN.
Leonard Meyer in his \emph{Emotion and Meaning in Music} \citep{meyerEmotionMeaningMusic1956} continued some of these lines of thought and was the first who put forward the idea that tethered the struture that earlier theorists wrote about and suggested that in addition to this struture being fundamental to the perception of the piece that there were also responsible for some aspects of the emotion and meaning listeners found in the piece.

Meyer's work since inspired a line of reserach investigating the perception of the structural aspects of music could be understood with the work of Eugene Narmor \citep{narmourAnalysisCognitionBasic1990, narmourAnalysisCognitionMelodic1992}, Glenn Schellenberg \citep{schellenbergSimplifyingImplicationRealizationModel1997}, Elizabeth Hellmuth Margulis \citep{margulisModelMelodicExpectation2005}, David Huron \citep{huronSweetAnticipation2006} and have inpsired computational, machine learning approaches to expectational frameworks in with work by Marcus Pearce \citep{pearceAuditoryExpectationInformation2012}.

\begin{itemize}
\tightlist
\item
  WHAT IS THE POINT OF ALL THIS RESERACH SETNECE
\end{itemize}

These general models of melodic perception tend suggest that Meyer was correct in his assertion that computational methodologies could be used to better understand question of melodic perception and strucuture and that there are links between the structure of the music and its perception.\footnote{This whole section sucks}

Turning to studies examining melodic dictation with a focus on musical structure, the first study to examine it extensively was Ortmann in 1933 \citep{ortmannTonalDeterminantsMelodic1933}.
Ortamnn used a series of twenty five-note melodies in order to examine the effects of repetition, pitch direction, conjunct-disjunct motion (contour), interval size, order, and chord structure, all of which he deems to be the \emph{determinants} of an individual's ability to dictate melodic material.
Though Ortmann did not use any statistical methods to model his data, he did assert that each of his determinants contributed to an individual's ability to dictate musical material.
This work was extended by \citep{taylorStrategiesMemoryShort1983} which additionally incoporated using musical skill as a predictor and additionally found evidence that these factors contributed to individual dictation abilities in a sample of 122 undergraduate students.

Although the literature is generally sparce compared to other areas of music cognition, literature exploring the effects of structural characteristics on memory does exist.
Long found that length, tonal structure, contour, and individual traits all contribute to performance on melodic dictation examinations and found that structure and tonalness to have significant, albeit small predictive powers in modeling \citep{longRelationshipsPitchMemory1977}.
One problem with studies such as \citep{longRelationshipsPitchMemory1977} is that they sometimes would make conspicuous methodloogical decisions such as elimnating individuals who were bad singers for the example.
Not only does this reduce the spectrum of ability levels (assuming that singing ability correlates with dictation ability, a finding since which has been established \citep{norrisRelationshipSightSinging2003}), but is additinally flawed in that it is at odds both with the intuition that an individual's singing ability cannot be taken as a direct representation of their mental image of the melody and is probably more related to the ability to have motor control over the vocal tract \citep{pfordresherPoorPitchSingingAbsence2007}.

Other researchers have also put forward other paramters thought to contribute like tempo \citep{hofstetterComputerBaesedRecognitionPerceptual1981}
tonality \citep{dowlingScaleContourTwo1978} \citep{longRelationshipsPitchMemory1977} \citep{pembrookInterferenceTranscriptionProcess1986} \citep{ouraMemoryMelodiesSubjects1988}
interval motion \citep{ortmannTonalDeterminantsMelodic1933, pembrookInterferenceTranscriptionProcess1986}
length of melody \citep{longRelationshipsPitchMemory1977, pembrookInterferenceTranscriptionProcess1986}
number of presentations \citep{hofstetterComputerBaesedRecognitionPerceptual1981} {[}\citep{pembrookInterferenceTranscriptionProcess1986}{]}
context of presentation \citep{schellenbergEffectTonalRhythmicContext1985}
listener experience \citep{longRelationshipsPitchMemory1977, ouraMemoryMelodiesSubjects1988} \citep{schellenbergEffectTonalRhythmicContext1985, taylorStrategiesMemoryShort1983}
familarity of style \citep{schellenbergEffectTonalRhythmicContext1985}

\citet{pembrookInterferenceTranscriptionProcess1986} provides an extensive detailing of a systematic study to melodic dictation where they used tonality, melody length, and type of motion as variables in their experiment.
They additionally also restricted their experimental melodies to those that were singable.
The authors found all three variables to be significant predictors with tonality explaining 13\% of the variance, length explaining 3\% of the variance and type of motion explaining 1\% of the variance.
The paper also claims that people on average can hear and remember 10-16 notes, which is worth commenting that these 10--16 notes are dependent on the experimental context of the melodies played with the quarter note set to 90 beats per minute.

Given the lack of consistent methodlogies in adminstration and scoring of these experiments it becomes difficult to find ways to generalize basic findings like expected effect sizes-- especially when the original materials and data have not been recorded-- but there is often interesting theoretical insights to be gleaned.
For example \citep{ouraConstructingRepresentationMelody1991a} used a sample of eight people to suggest that when taking melodic dictation, individuals use a system of pattern matching that interfaces with their long term memory in order to complete dictation tasks.
While this paper does not bring with it exhaustive evidence supporting this claim, the idea is explored in detail in Chapter 6 the idea of pattern matching is used in conjunction with Cowan's embeded process model of working memory.

More recently the music education community has also began to do research around melodic dictation using both qualitative and quantitiatve methdologies.
\citep{paneyTeachingMelodicDictation2014} interviewed high school teachers on methods they used to teach melodic dictation and \citep{gillespieMelodicDictationScoring2001} has done work on investigating methods as to best score melodic dictation.
Other work by \citep{pembrookSendHelpAural1990} surveyed various methodlogies used by instructurs in aural skills settings.
Some of these studies consider aural skills as a totality like
\citep{norrisRelationshipSightSinging2003} who provided quantiative evidence to suggest most aural skills pedagoge's intuition that there is some sort of relationship between melodic dictation and sight singing.
Looking at the notorious subset of students with absolute pitch (AP), \citep{dooleyAbsolutePitchCorrelates2010} provided evidence demonstrated that students with AP tend to outperform their non-AP colleagues in tests of dication.

{[}SOME OF THESE ARE CLEARLY EXPERIMENTAL AND NEED TO BE PUT THERE{]}

Continuing exploring the pedagogical liteature, Naton Buonviri and colleagues have also made melodic dictation a central focus of some recent papers.
\citet{paneyTeachingMelodicDictation2014} interviewed high school teachers on methods that they used to teach melodic dictation.
\citet{buonviriEffectsMusicNotation2015} interviewed six sophomore music majors to find sucessful strategies that students enaged with when completing melodic dications.
\citet{paneyEffectDirectingAttention2016} reported beneficial effects to direct student's attention and guide them through melodic dictation exercises suggesting that some sort of mental organizaton of the dication process is helpful.
\citet{buonviriMelodicDictationInstruction2015} found that having students sing a preparatory singing pattern after hearing the target melody, essentially a distractor task, hindered performance on melodic dictation.
\citet{buonviriEffectsPreparatorySinging2015}\ldots{}
\citet{buonviriEffectsMusicNotation2015} found no effects of test presentation format (visual versus aural-visual) using a melodic memory paradigm.
\citet{buonviriEffectsTwoListening2017} reported no significant advatage to listening strategies while partaking in a melodic dictation test.

\hypertarget{recent-computational-musicology-work-papers-and-findings}{%
\subsubsection{Recent Computational Musicology Work papers and findings}\label{recent-computational-musicology-work-papers-and-findings}}

Using symbolic features of the melodies themselves is not a novel approach as noted in the above literature attempting to predict performance on melodic dictations.
Much of this work pre-dates recent advances in computational musicology such as the advent of technology like David Huron's Humdrum \citep{huronHumdrumToolkitReference1994} and Michael Cutberth's Music21 \citep{cuthbertMusic21ToolkitComputerAided2010} which now allows music researchers to systematically digitize symbolic musical material.
In addition to creating accesible frameworks for encoding, the computational power availble exponentially exceeds that of what was availble in the 20th century and has opened up new possibilities in the computational modeling of music.

While I reserve a longer discussion on the histories of computational musicology for the fourth chapter, relevant to this study is the additional ways it is now possible to abstract features from symbolic melodies beyond what was capable in studies such as \citet{ortmannTonalDeterminantsMelodic1933} and \citet{taylorStrategiesMemoryShort1983}.

An abstracted feature of a melody is an emergent property of the melody that results from performing some sort of calculation on the melody?
This type of feature abstraction is in contrast to much of the work done in the field of music information retrivial which often relies on the recorded audio for feature abstraction and is addressed under Experimental features LINK THAT IN!.
Abstracted symbolic features of melodies can largely be conceptualized as being \textbf{static} or \textbf{dynamic}.
The above papers tended to use more simplisitc methods of figuring out parameters such as counting the notes by hand but with the advent of new encoding systems and more powerful computing power it is now possible to take on much more rigerious computational analyses.

\hypertarget{static-views-of-computational-features-fantastic}{%
\subsubsection{Static Views of Computational Features/ FANTASTIC}\label{static-views-of-computational-features-fantastic}}

Static features of melodies work by summerizing some aspect of the melody as if it were to be experience in suspended animation.
Using static features helps quantify something that might be intuitive about a melody or piece of encoded music.
For example, something like David Huron's contour class used in a study investigating melodic arches \citep{huronMelodicArchWestern1996} using the Essen Folk Song Collection \citep{schaffrathEssenFolkSong1995} can only be understood as a feature of the melody itself once the melody has been sounded and is recalled would be a static feature of a melody.
Other examples include a melody's global note density, normalized pairwise variability index (CITATION), and a melody's tonalness as calculated by one of the various key profile algorithms (KRUMHANSL,ALBRECT AND SHANAHAN)
These measures are useful when describing melodies and are predictive of various behavioral phenomena as detailed below, but at this point it has not been well established to what degree these summary features can be directly related to aspects of human behavior.

The quintessential and most comprehensive toolbox example of this is Daniel Müllensiefen's Feature ANalysis Technology Accesing STatistics (In a Corpus) or FANTASTIC \citep{mullensiefenFantasticFeatureANalysis2009}.
FANTASTIC is software that is capable summerizing musical material at for monophonic melodies.
In additon to computing 37 features such as contour variation, tonalnesss, note density, note length, and measures inspired by computational linguistics (THAT BOOK OF GERAINT), FANTASTIC also calculates m-types (melodic-rhyhmic motives) that are based on the frequency distributions of melodic segements found genres of music.
This is inspired by fact that repeition is key structure of music \citep{huronSweetAnticipation2006}

Work using the FANTASTIC toolboox has been sucessful in predicting court case decisions \citep{mullensiefenCourtDecisionsMusic2009}, predicting chart sucesses of songs on the Beatles' \emph{Revolver} \citep{kopiezAufSucheNach2011}, memory for old and new melodies in signal detection experiments \citep{mullensiefenRoleFeaturesContext2014}, memory for earworms \citep{jakubowskiDissectingEarwormMelodic2017, williamsonEarwormsThreeAngles2012}, memorability of pop music hook \citep{balenCorpusAnalyisTools2015}.
In experimental studies, FANTASTIC has also been used to determine item difficulty \citep{bakerPerceptionLeitmotivesRichard2017, harrisonModellingMelodicDiscrimination2016} and has even been the basis of the development of a computer assistted platform for studying memory for melodies \citep{rainsfordMUSOSMUsicSOftware2018}.

\hypertarget{dynamic}{%
\subsubsection{Dynamic}\label{dynamic}}

In addition to using summary based features on melodies, it is also possible to model the perception of musical materials by using a dynamic approach that is dependent on the unfolding of musical material.
First explored in CONKLIN, and then first published as a dynamic model of expectation in his doctoral dissertation, Marcus Pearce's Informaton Dynamics Of Melody IDyOM models musical expectancy using various information theoretic concepts inspired by Claude Shannon (SHANNON).
The model takes an unsupervised machine learning approach and calculates the information content of the amount of DECLARED n-grams in the corpus.
As a model exploring expection for melody IDyOM has has been applied to a variety of settings LIST THEM HERE.
The domain general application of IDyOM has given creedence to Meyer's assertion that the enculturation of musical styles stems from statistical exposure to melodies and be somewhat refelective of the cogitive processes used in muscical perception.
IDyOM has also been recently extended to look at expectation in multi-part chorales (SAUVE WORK) and expectations of harmony (HARRISON WORK).

\citep{pearceAuditoryExpectationInformation2012a, pearceStatisticalLearningProbabilistic2018}

The advantage of using a dynamic approach is that it theoretically reflects real time perception of music with the structural characeristics of the music mapping on to real human behavior.

\hypertarget{experimental}{%
\subsection{Experimental}\label{experimental}}

\begin{itemize}
\item
  Advantage of vocal melodies paper \textbar{} Weiss 2015
\item
  Voice is better than instruments Weiss Trehub Schellenberg 2012
\item
  Engaging, conspecific signal Weiss Peretz 2019 , not due to expressivity. even see this in amusics.
\item
  Not affected by musical training or exposure
\item
  MIR stuff
\item
  Stuff out of education stuff that is also experimental
\item
  WM W LVH, two span tasks and claim it effects langauge tasks
\item
  WM Halpern 2017 with pearce, LTM stuff , re look this up
\end{itemize}

\hypertarget{modeling-and-polymorphism-of-ability}{%
\section{Modeling and Polymorphism of Ability}\label{modeling-and-polymorphism-of-ability}}

Given the current state of cognitive psychology and psychometrics, as well as recent advances in computational musicology, the possibilities for now operationalizing and then modeling aspects of melodic dictation are as advanced as they ever have been.
Given that we can now assign numbers to basically every factor that is thought to contribute to this process from concepts of musicianship, to features of a melody, to the variable size in an individual's working memory capacity all of these thigns can be put into some sort of model.
While this will bring the community closer to formally modeling all of this and lead to a clearer understanding, before going ahead and doing this it is worth pointing out that many of the concepts discussed above are highly complex concepts like musicianship and tonalness and rest on lots of assumptions.
Musicianship, for example, or any measure of musical training is not something that can be measured directly such as a person's height or weight, but has to be inferred based on the logical assumptions of the person doing the measurments.
So while the rest of this study will rely on this, it is important to note that people shouldn't confuse abstracted concepts with real things.

The most illustrative example of this comes from a study by HARRISON ET AL who created a latent variable model of aural skills that was able to predict 74\% of the variance in aural skills performance.
This latent trait that the authors created may be helpful in explaining the patterns of covariance in data, but this would be to reifiy a statistical abstraction as an ontologically true idea.
This idea has been discussed before critqiuing ideas such as \emph{g} \citep{gouldMismeasureMan1996, kovacsProcessOverlapTheory2016} and has recently been the subject of critique in music psychology OUR GOLDMSI PAPER.

The same arguments put forward in this literature also are relevant here.
In order to have a complete, causal model of \emph{how} melodic dictation works, it is important to understand melodic dictation as a set of musical abilities that are related to other musical abilities, though may not be related.
This idea is not new even in music psychology, the past two decades have seen calls for a more polymorphic definition of musical ability \citep{levitinWhatDoesIt2012, peretzModularityMusicProcessing2003} which in its modeling will require more concrete ways of defining how it works than just correlating variable together that are helpful at prediction without saying exactly how that process happens.

\hypertarget{conclusions}{%
\section{Conclusions}\label{conclusions}}

In this chapter I first described what is melodic dictation using Karpinki's verbal model, noted what the things were that were missing from this model as a stepping off point, then went on to suggest a taxonomy of these based on what already has done.
I suggest there are both individual as well as musical features that need to be understood in order to have a comprensive understanding of melodic dictation.
Of the two sets of features, individual features can be either cognitive or environmental and musical features can be either structural or experimental.
This taxonomy does not consist of exclusive categories and certainly permits interactions between any and all of the levels.

It would be impossible given the scope of this study to effectively quantify each and every factor and how it interacts at every level, but the degree to which there is the most literature and you can get the most bang for you buck seems like the obvious stepping off point.
Rest of this dissertation will systehsize these areas and put forth novel research contributing to the modeling and subsequent understanding of melodic dictation.
Understanding melodic dictation will help with both understanding melodic perception and help our pedagogy.

\hypertarget{add-in}{%
\subsection{Add In}\label{add-in}}

\begin{itemize}
\tightlist
\item
  \citep{hansenWorkingMemoryMusical2013}
\item
  Dowling 1991 \citep{dowlingTonalStrengthMelody1991}
\end{itemize}

\hypertarget{individual-differences}{%
\chapter{Individual Differences}\label{individual-differences}}

\hypertarget{rationale}{%
\section{Rationale}\label{rationale}}

The first two steps of Gary Karpinski's model of melodic dictation \citep{karpinskiAuralSkillsAcquisition2000, karpinskiModelMusicPerception1990} rely exclusively on the mental representation of melodic information.
Karpinski conceptualizes the first stage of \emph{hearing} as solely involving the physical motions on the tympanic membrane, as well as the listener's attention to the musical stimulus.
This stage is distinguished from that of \emph{short-term melodic memory} which refers to the amount of melodic information that can be represented in conscious awareness.
Given that neither stage of the first two steps of Karpinski's model requires any sort of musical expertise, every individual with normal hearing and cognition should be able to partake in the first two steps of melodic dictation.\footnote{This whole model needs critique under the WMC literature. It's kind of strange to think that the act of something hitting your ear is different than attention (the way that Cowan thinks about WMC and again that you can split up the representation in memory from that of what the characteristics are of the melody like the meter and scale degrees, which have been argued to be part of intrinsic qualia) also there is a big problem here about stuff being actively rehearsed or not}
The ability to hear, then remember musical information is where all students of melodic dictation are presumed to start.
From this baseline, students recieve explicit education in music theory and aural skills and to develop the ability to link what is heard to what can then be musically understood and then notated.

While the majority of beginning students of melodic dictation are assumed to start at the same ability, recent reserach from music psychology suggests that individual differences in musical perception exist and must be accounted for from a psychological and pedagogical standpoint (Citations Here).
In order to fully capture the diversity of listening abilities amongst students of melodic dictation, a complete account melodic dictation music include individual differences in ability.
Understanding how differences at the individual level vary also will help pedagoges know what can be reasonably expected of students with different experiences and abilities.

Attempting to investigate all four parts of melodic dictation from hearing, to short-term melodic memory, to musical understanding, to notation is cumbersome both from a theoretical perspective and practically unfeasible due to the amount of variables that might contribute to this process.
In order to obtain a clearer picture of what mechanisms contribute to this process, these steps must be be investigated in turn.
This chapter investigates the first two steps of the Karpinski model with an experiment investigating individual factors that contribute to an individual's musical memory that do not depend on their knowledge of Western musical notation.
By understanding which, if any, individual factors play a role in this process, it will inform what can be reasonably expected of individuals when other musical variables are introduced.

\hypertarget{individual-differences-1}{%
\section{Individual Differences}\label{individual-differences-1}}

\begin{enumerate}
\def\labelenumi{\Roman{enumi}.}
\setcounter{enumi}{1}
\item
  Way MD works
  As stated bove, all expect the same baseline
  Get better with extractive lsitening and chunking
  Def extractive lsitening, this also would be question of AUDIOTRY DISTRACTION ALA EMILY
  Def Chunking as well
  Logic increases the both, get better at melodic dication.
  But does the literature support that?
\item
  Mem 4 melodies
  M4M says that no musical advantage in this, tho not dictation
  example 1 2 3
  But bc m4m is not MD, can't jsut go for it
  MD is layered
  m4m is jsut a chocie
  Looking at other lit, yes, there is an effect of training
  see this with general ingelligecne (examples)
  see this with memory (talamin)
  Mem exp when short an tonal, just like MD!
  has been interpreted as fx of traning
  but probably reversed in direciton
  Cognition
  Obvious a link here, assumed to be FX
  could be some combination of Train, int, wmc
  expecially imporant to consider effects of WM
  see all of bersz
  really need ot look at all at once to distentangle
  need paradigm that does xyz
  Other people have started to do this, okda slevc
  How
  first assume that M4M is MD 12
  Experiment with individual factors
  Path analysis
\item
  Experiment
  V. Results and MODEL discussion
\item
  Relavnace to dicatation
  A. recap looked at MM via m4m SD
  Assume xyz
  When looking at results found yes on wmc
  this means berz was right
  also corrobrates okda slevc?
  This should inform further research, esp modling addressed in later chapters
  Why didnt others do as well?
  Why gf sucks? wrong skill (pot maybe)
  not senative to specific at MD
  looking at large modeling, might not pick that up
  What does this mean for music theory pedaoggy?
  WMC improtant
  note distribution of wmc (not going to have clinical, but obvs important in our modeling)
  The thing is that capacity can not expand (chennete)
  Also likely not clincial populations
  Answer after maybe consider that distractor thing (related to extractive listening, not focus in but tuen out)
  Want to see if chunking is question of peceptual fluency in dividual
  coudl be that as PF goes up, better chunk
  thsu better m4m and performance in these tasks
  Ran some extra regressions to look at that at individual level and objective perofrmance?
  Diff coudl just be exccerbated by WMC
  will investigate question of PF with corpus evdience in next chapter
\end{enumerate}

\hypertarget{individual-differences-2}{%
\section{Individual Differences}\label{individual-differences-2}}

While explicit musical training is proposed to get people better at this\ldots{}
Literature on individual differences says other things might contribute.
For example xyz
So while pedagogically, yes training, a full model of md needs to acknowledge the diversity of listening abilities.
This is especially relvant bc some literature does not suggest musician advantage

As stated above, the first two steps of Karpinski's model of melodic dictation are not exclusive to trained musicians.
These steps involve first hearing the melody and then retaining the melody in conscious awareness.
From a pedagogical and psychological standpoint, it would be safe to assume that individuals differ in their ability to memorize musical material.
Some individuals perform well, while others do not.
Presumably these differences in performance can be attributed to a number of factors.

One of the first factors --and perhaps most obvious to consider-- that might explain differences in ability would be an individual's musical training.

Musicans tend to have better memory
they also do better with other cognitive tasks
but really this is reversed
If you look at memory for melodies, musicians do not have an advantage
Seems to be much more like domain expertise thing

As noted in the previous chapter, people with some degree of musical training tend to outperform their less musically trained peers on many cognitive tasks both in terms of memory and general problem solving \citep{schellenbergMusicNonmusicalAbilities2017}.

A meta-analysis by Talamini and colleagues \citep{talaminiMusiciansHaveBetter2017} highlighted differences in memory are especially pronouced when the stimuli used in the experiments was tonal.
This finding suggests a relationship between muscial training and an increase in memory for musical material.

Additionally\footnote{Could drop the stuff on intelligence}, musically trained individuals perform better on other tests of cognitive ability.
Most prominant in many of these studies are findings that link intelligence and musical training.
As reviewed in \citet{schellenbergMusicNonmusicalAbilities2017}, both children and adults who engage in musical activity tend to score higher on general measurses of intelligence than their non-musical peers (Gibson, Folley and Park, 2009; Hille et al., 2011; Schellenberg, 2011a; Schellenberg and Mankarious, 2012).
The finding importantly comes with it a correlation between duration of musical training and the extent of the increases in intelligence (Degé, Kubicek and Schwarzer, 2011a; Degé, Wehrum, Stark and Schwarzer, 2015; Corrigall and Schellenberg, 2015; Corrigall, Schellenberg and Misura, 2013; Schellenberg, 2006).
While many of these studies are correlational, other researchers have further investigated this relationship in experimental settings in attempt to control for confounding variables like socio-economic status and parental involvment in out of school activities (Corrigall et al., 2013; Degé et al., 2011a; Schellenberg, 2006, 2011a, 2011b; Schellenberg and Mankarious, 2012), but findings have been mixed.

Schellenberg \citep{schellenbergMusicNonmusicalAbilities2017} notes that in many of these studies there is a problem of too small of a sample size in his review (Corrigall and Trainor, 2011; Parbery-Clark et al., 2011; Strait, Parbery-Clark, Hittner and Kraus, 2012) in that studies that are typically smaller do not reach statistical significance.
Also referenced in Schellenberg's review is evidence that when professional musicians are matched with non-musicians from the general population these associations are NON EXISTANT (CITE).
Interpreting the current literature Schellenberg's review suggests researchers might consider the hypothesis that higher functioning kids that take music lessons and they tend to stay in lessons longer which leads to the observed differences in intelligence.
Additionally, Schellenberg remains skeptical of any sorts of causal factors regarding increases in IQ (e.g., François et al., 2013; Moreno et al., 2009) noting methodlogical problems like how short exposure times were in studies claiming increases in effects or researchers who not holding pre-exisiting cognitive abilitis constant (Mehr, Schachner, Katz and Spelke, 2013).

--- there is a reverse here

So while the literatre above provides a body of evidence to suggest that musically trained individuals outperform their non-muscial peers-- with the direction of causality yet to be firmly established-- musical training is not and imporantly cannot be the sole determining factor in how well an individual remembers a melody.
Presumably other factors contribute in people's ability to retain musical information.
One could consider a simple thought experiment where there might be people without musical training who, just due to their innate abilities, are able to outperform people with musical training based on their cognitive ability.

These are the first steps as well in any melodic dictation task.
this is cognitive ability, defined as XXXX to do task
But in addition to congitive ability, reason to believe that musical training will also affect this

So in order to get a better idea of melodic dictation, need to look at how most people do when having to do short term melody memory and eventually manipulate it.
In order to do this, need to look at variables that might be at play.

\hypertarget{musical-training-1}{%
\subsection{Musical Training}\label{musical-training-1}}

Tacit in many pedagogical assumptions is that most people start out roughly with the same ability to remember musical material and that this capacity increases with musical training as noted above.
Karpinski defines this baseline limit of the amount of melodic information that can enter \emph{short-term melodic memory} as ``somewhere between five and nine notes'', referencing Miller's magic number seven\footnote{Should I footnote here saying why this was not meant to be taken literally?} \citep{millerMagicalNumberSeven1956}.
Karpinski supports this claim with one experimental study by Marple (n.d.!!!) who corroborated Miller's limits (p.78) and additionally noted that the binding of musical features beyond that of pitch extended the range to approximelty six to ten notes.
Similar findings were reported by Tallarico (XXX), Long (XXXX), and Pembrook (XXXX) who claimed the note limit to be between seven and eleven notes.\footnote{For reasons noted in previous chapters (SECTION), it is important to stress the maximum about of notes able to be memorized should not be direclty interpreted as a one to one mapping of the amount of items that can be held in short term memory.
  Breifly reviewing earlier points, the first reason is that Miller's number 7 was never meant to be taken literally as a number used to investigate the items of memory, but rather a rheatorical device for a keynote address he was asked to give.
  Secondly, lieterature on the capacity limits of memory need to account for chunking mechanisms, most of which are bountiful in the musical domain as noted in SECTION.}

In order to extend the capacity for musical memory, Karpinski puts forward two possible stragegies.
The first is extractive listening and the other is chunking.
Extrative listening is a processes described as \ldots{}
Chunking on the other hand is a listener's ability to group certain parts of sensory information together, so that more information can take up less of the finite space in short-term muscial memory.
According to Karpinkski, chunking increases with increases in musical understanding.

This is also backed up with a host of literature showing htat musicians tend to outperform
Especially when the task in musical
-- look at these examples here

Taken as a whole, much of the literature could be interpreted to suggest that listeners begin as a tabula rasa, then as they engage with more musical activity, this activity affords them the ability to in perform better on musical tasks.
While this might seem like an intuitive explanation for the findings, three large problems exist with this interpretation.

The first is that how musicianship is measured varies from study to study, thus making interpreation of the data much harder.
In fact, music psychology in general suffers from having not standardized a way to measure musical enagement.
Talamini note this problem in the aforementioned meta-analysis saying that

\begin{quote}
Quote
\end{quote}

and suggest that music psychology might consider moving towards more ``catch all measures'' of musicality such as the Goldsmiths Musical Sophistication Index CITE or the MUSEBAQ.
Both tools use latent variable approaches that attempt to smooth over some of the problems with measuring such a messy construct.
But one problem with adopting these tools is that it forces some sort of ontological commitment CITE FROM POT that has been critized by other CITE MEEE.

\begin{itemize}
\tightlist
\item
  but one problem is that maybe that there more domain general processes accounting for this
\item
  one way that other people have looked at it os OS
\item
  They think about it as LV with three separate sets they define as xyz
\item
  thing is, this is very close to WM (what they call separate)
\end{itemize}

The second problem with interpreting the literature as practice driving all these effects is that this might confound the direction of causalitiy.
As noted above, most of the evidence suggesting a relationship here derrives from cross sectional designs and is limited in its ability to posit causal relationships.

Thirdly, these studies as a whole lack consistent control of covariates that might confound the findings (OS).
Factors such as \ldots{} (Stuff from OS)

Considering these three confounds, it is only possible then to consider literature in one of three ways according to OS.

1
2
3

Thing that makes the mnost sense is probably what GS says that it exacerbates pre-existing differences.

but note a lot of this is general fluid intelligence, not WM!

So if this is the case, we need to look into other factors that might then also play a role

OS looked at this a bit ago with the Gold-MSI and the idea of executive functioning
-- paper summary here

Here they break up EF into a few different categories,
-- note here the parallels on WM and melodic dictation.

One of the three skills of EF is updating.
Defined as x
And has been conceptualized by some as WMC.
And if you think about it is basically MD.

In fact, this parallel is not even that new

Whether conceptualzed as the updating component of executive function (MIYAKE) or as working memory capacity (COWAN), working memory tasks share many similarities with tasks used in the music perception literature.
Berz \citep{berzWorkingMemoryMusic1995}, in his 1995 article pointed this out.

Looking at working memory we are not the first to point this out.
Berz did it here and noted that could just be wmc.
Berz contiues in vain of Baddely and Hitch
They solve the problem with having loops.
Berz continues in this tradiition by setting up another loop for music
But this is a problem for a few reasons.

if you define it as complex, need to be doing complex

For those reasons, decide instead to adopt the Embedded Process Model by Cowan.
It works like this.
And will eventually be more suited for what we are trying to do.
Also mention here the problem with Karpinski model that it doesnt say waht is actively rehearsed.

So the task now is now to investrigate how all of these factors come together.
First is to get a task that uses the first two steps of the Karpinski model that is accesible to the non-musically trained.
Then idea is to parse out the data to see which of the variables contributes to this task.

Having established a link between wmc tasks and md, need to now find a task that will mirror this bc it contributes
Link back to MMD!

\hypertarget{dictation-without-dictation}{%
\subsection{Dictation without Dictation}\label{dictation-without-dictation}}

One of the most popular paradigms used in music perception research, specifically the memory for melody literature, are same-different memory tasks \citep{halpernMemoryMelodies2010}.
These tasks require individuals to hear a melody, retain it in memory, then hear a second melody either at the exact same pitch level, or transposed, and then make a judgment if the two melodies were the same or different.
It is important to note this type of task requires both rention of musical material and a secondary cognitive task: deciding if the melodies were identical or not.
In many ways, this type of task mirrors the first two steps of melodic dictation.
Same-different paradigms require individuals to first hear a finite amount of musical material, then while held in concious representation, perform some sort of mental action on the contents of memory by making a similarity judgment.
If one were to acknowledge these similariies in mental processes, using same-different paradigms could provide a way to investigate the first two steps of Karpinkski's model of melodic dictation in the general population which may or may not have musical training.

Additionally, using a same different paradigm also enables the possibility to then look at which of the aforementioned factors best predict how individuals do in this process.
Interstingly, in contrast to results reported by \citep{talaminiMusiciansHaveBetter2017} looking at memory for more simple stimuli, \citep{halpernMemoryMelodies2010} note that ``although musical experts exceed nonexperts in some aspects of remembering music, frequently this outcome does not occur'' when referring to more ecological settings.
The musician's advantage dissapears.
Using this paradigm, it could then become possible to further look into what factors in addition to musical training best predict memory for melodies.

\hypertarget{cognitive-measures-of-interest}{%
\subsection{Cognitive Measures of interest}\label{cognitive-measures-of-interest}}

Having previously established that many tests of musical ability and aptitude, may in fact be tests of working memory \citep{berzWorkingMemoryMusic1995}, one factor not yet accounted for in many of the measures of musical memory might be working memory.
If operationalized as conceptualized as Cowan above (CITE), one would have ot measure working memory using a task that uses both retention and manipulation of informaiton in memory, or a set of complex span tasks \citep{unsworthAutomatedVersionOperation2005}.
Additionally, since general intelligence is often predictive of performance on a host of cognitive tasks LIST and has been theoretically related to working memory \citep{kovacsProcessOverlapTheory2016}, this measure should also be accounted for.
Finally, in response to claims made by \citep{okadaIndividualDifferencesMusical2018a} having to need to account for specific covariates, it would also be good to keep track of things like socioeconomic status, degree of education, and OTHER VARIABLES.

\hypertarget{path-analysis-here}{%
\subsection{PATH Analysis here}\label{path-analysis-here}}

Given the complex nature being investigated and the theoretical concepts at play such as working memory, general fluid intelligence, and musicial sophisticaiton conceputalized as a latent variable, it follows that the most appropriate method of parsing out the variance in this covariance structure would be to use some form of path analysis.
Path analysis is a type of analysis developed by XXXX which orignally assumed a closed algerbric system which could be used to parse out causal structures amongst covariance relationships WHY.
The sets of variables presented do not make up a closed system as orignally devided by XXX in his investigation of the heretibility of guinia pig traints PEARL PAGE, path analysis using structural equation modeling does allow insight into the degree that variables of interest contribute to complex causal relationships.

\hypertarget{hypotheses}{%
\subsection{Hypotheses}\label{hypotheses}}

If I then assume that a same-different melodic memory paradigm is a stable proxy for the first two steps of Karpinski's model of melodic dictation, then data generated from both objective tests of the Goldsmiths' Musical Sophistication Index can serve as proxy for this measure of interst.
In this analyses, I will use a series of structural equation models in order to investigate how various individual factors contribute to an individual's memory for melody.
Following a stepwise proceedure LIKE EE, these sets of analyses will provide a way what individual factors need to be accounted for in future research.

Given a robust instrument for measuring musciality, and two well established cognive measrues as specifically defined, this study analysis seeks to investigate the degree to which these individual level variables are predictive of a task that is proxy to the first two steps of melodic dictation.

If a large proportion of the variance of musical memory can be attributed to training, then variables related to the Goldsmiths Musical Sophisitication Index should be most predictive with the highest path coeffecients and lead to the best model fit.
If instead cognitive factors do play a role, this should be evident in the path loadings.
Not an either or, more like a both.

\hypertarget{overview-of-experiment}{%
\section{Overview of Experiment}\label{overview-of-experiment}}

\hypertarget{participants}{%
\subsection{Participants}\label{participants}}

Two hundred fifty-four students enrolled at Louisiana State University completed the study.
Students were mainly recruited in the Department of Psychology and the School of Music.
The criteria for inclusion in the analysis were no self-reported hearing loss, not actively taking medication that would alter cognitive performance, and the removeal of any univariate outliers (defined as individuals whose performance on any task was greater than 3 standard deviations from the mean score of that task).
Using these criteria, eight participants were not eligible due to self reporting hearing loss, one participant was removed for age, and six participants were eliminated as univariate outliers due to performance on one or more of the tasks of working memory capacity.
Thus, 239 participants met the criteria for inclusion.
The eligible participants were between the ages of 17 and 43 (M = 19.72, SD = 2.74; 148 females).
Participants volunteered, received course credit, or were paid \$20.

\hypertarget{materials}{%
\subsection{Materials}\label{materials}}

\hypertarget{cognitive-measures}{%
\subsubsection{Cognitive Measures}\label{cognitive-measures}}

All variables used for modeling approximated normal distributions.
Processing errors for each task were positively skewed for the complex span tasks similar to \citet{unsworthComplexWorkingMemory2009}.
Positive and significant correlations were found between recall scores on the three tasks measuring working memory capacity (WMC) and the two measuring general fluid intelligence (Gf).
The WMC recall scores negatively correlated with the reported number of errors in each task, suggesting that rehearsal processes were effectively limited by the processing tasks \citep{unsworthComplexWorkingMemory2009}.

\hypertarget{measures}{%
\subsubsection{Measures}\label{measures}}

\hypertarget{goldsmiths-musical-sophistication-index-self-report-gold-msi}{%
\paragraph{Goldsmiths Musical Sophistication Index Self Report (Gold-MSI)}\label{goldsmiths-musical-sophistication-index-self-report-gold-msi}}

Participants completed a 38-item self-report inventory and questions consisted of free response answers or choosing a
selection on a likert scale that ranged from 1-7. \citep{mullensiefenMusicalityNonMusiciansIndex2014}.
The complete survey with all questions used can be found at goo.gl/dqtSaB.

\hypertarget{tone-span-tspan}{%
\paragraph{Tone Span (TSPAN)}\label{tone-span-tspan}}

Participants completed a two-step math operation and then tried to remember three different tones in an alternating sequence (based upon \citet{unsworthAutomatedVersionOperation2005}).
We modelled the three tones after \citet{liEstimatingWorkingMemory2013} paper's using frequencies outside of the equal tempered system (200Hz, 375Hz, 702Hz).
The same math operation procedure as OSPAN was used.
The tones was presented aurally for 1000ms after each math operation.
During tone recall, participants were presented three different options H M and L (High, Medium, and Low), each with its own check box.
Tones were recalled in serial order by clicking on each tone's box in the appropriate order.
Tone recall was untimed.
Participants were provided practice trials and similar to OSPAN, the test procedure included three trials of each list length (3-7 tones), totalling 75 letters and 75 math operations.

\hypertarget{operation-span-ospan}{%
\paragraph{Operation Span (OSPAN)}\label{operation-span-ospan}}

Participants completed a two-step math operation and then tried to remember a letter (F, H, J, K, L, N, P, Q, R, S, T, or
Y) in an alternating sequence \citep{unsworthAutomatedVersionOperation2005}.
The same math operation procedure as TSPAN was used.
The letter was presented visually for 1000ms after each math
operation.
During letter recall, participants saw a 4 x 3 matrix of all possible letters, each with its own check box.
Letters were recalled in serial order by clicking on each letter's box in the appropriate order.
Letter recall was untimed.
Participants were provided practice trials and similar to TSPAN, the test procedure included three trials of each list length (3-7 letters), totalling 75 letters and 75 math operations.

\hypertarget{symmetry-span-sspan}{%
\paragraph{Symmetry Span (SSPAN)}\label{symmetry-span-sspan}}

Participants completed a two-step symmetry judgment and were prompted to recall a visually-presented red square on a 4 X 4 matrix \citep{unsworthAutomatedVersionOperation2005}.
In the symmetry judgment, participants were shown an 8 x 8 matrix with random squares filled in blank.
Participants had to decide if the black squares were symmetrical about the matrix's vertical axis and then click the screen.
Next, they were shown a ``yes'' and ``no'' box and clicked on the appropriate box.
Participants then saw a 4 X 4 matrix for 650 ms with one red square after each symmetry judgment.
During square recall, participants recalled the location of each red square by clicking on the appropriate cell in serial order.
Participants were provided practice trials to become familiar with the procedure.
The test procedure included three trials of each list length (2-5 red squares), totalling 42 squares and 42 symmetry judgments.

\hypertarget{gold-msi-beat-perception}{%
\paragraph{Gold-MSI Beat Perception}\label{gold-msi-beat-perception}}

Participants were presented 18 excerpts of instrumental music from rock, jazz, and classical genres \citep{mullensiefenMusicalityNonMusiciansIndex2014}.
Each excerpt was presented for 10 to 16s through headphones and had a tempo ranging from 86 to 165 beats per
minute.
A metronomic beep was played over each excerpt either on or off the beat.
Half of the excerpts had a beep on the beat, and the other half had a beep off the beat.
After each excerpt was played, participants answered if the metronomic beep was on or off the beat and provided their confidence: ``I am sure'', ``I am somewhat sure'', or ``I am guessing''.
The final score was the proportion of correct responses on the beat judgment.

\hypertarget{gold-msi-melodic-memory-test}{%
\paragraph{Gold-MSI Melodic Memory Test}\label{gold-msi-melodic-memory-test}}

Participants were presented melodies between 10 to 17 notes long through headphones \citep{mullensiefenMusicalityNonMusiciansIndex2014}.
There were 12 trials, half with the same melody and half with different melodies.
During each trial, two versions of a melody were presented.
The second version was transposed to a different key.
In half of the second version melodies, a note was changed a step up or down from its original position in the structure of the melody.
After each trial, participants answered if the two melodies had identical pitch interval structures.

\hypertarget{number-series}{%
\paragraph{Number Series}\label{number-series}}

Participants were presented with a series of numbers with
an underlying pattern.
After being given two example problems to solve, participants had 4.5 minutes in order to solve 15 different problems.
Each trial had 5 different options as possible answers \citep{thurstonePrimaryMentalAbilities1938}.

\hypertarget{ravens-advanced-progressive-matrices}{%
\paragraph{Raven's Advanced Progressive Matrices}\label{ravens-advanced-progressive-matrices}}

Participants were presented a 3 x 3 matrix of geometric patterns with one pattern missing \citep{ravenManualRavenProgressive1994}. Up to eight pattern choices were given at the bottom of the screen.
Participants had to click the choice that correctly fit the pattern above.
There were three blocks of 12 problems, totalling 36 problems.
The items increased in difficulty across each block.
A maximum of 5 min was allotted for each block, totalling 15 min.
The final score was the total number of correct responses across the three blocks.

\hypertarget{procedure}{%
\subsection{Procedure}\label{procedure}}

Participants in this experiment completed eight different tasks, lasting about 90 minutes in duration.
The tasks consisted of the Gold-MSI self-report inventory, coupled with the Short Test of Musical Preferences, and a supplementary demographic questionnaire that included questions about socioeconomic status, aural skills history, hearing loss, and any medication that might affect their ability to perform on cognitive tests.
Following the survey they completed three WMC tasks: a novel Tonal Span, Symmetry span, and Operation span task; a battery of perceptual tests from the Gold-MSI (Melodic Memory, Beat Perception, Sound Similarity) and two tests of general fluid intelligence (Gf): Number Series and Raven's Advanced Progressive Matrices.

Each task was administered in the order listed above on a desktop computer.
Sounds were presented at a comfortable listening level for the tasks that required headphones.
All participants provided informed consent and were debriefed.
Only measures used in modeling are reported below.

\hypertarget{results}{%
\subsection{Results}\label{results}}

\hypertarget{descriptive-data-screening-correlational}{%
\subsubsection{Descriptive, Data Screening, Correlational}\label{descriptive-data-screening-correlational}}

The goal of the analyses was to examine the relationships among the measures and constructs of WMC, general fluid intelligence, musical sophistication (operationalized as the General score from the Gold-MSI), in relation to the two objective listening tests on the Gold-MSI.
Before running any sort of modeling, we inspected our data to ensure in addition to outlier issues as mentioned above, the data exhibited normal distributions.
I report both correlation values, as well as visually displaying our distributions in Figure 1.

Before running any modeling, I checked our data for assumptions of normality since violations of normality can strongly affect the covariances between items.
While some items in Figure 1 displayed a negative skew, many of the individual level items from the self report scale exhibited high
levels of Skew and Kurtosis beyond the generally accepted ± 2 \citep{fieldDiscoveringStatisticsUsing2012}, but none of the items with the unsatisfactory measures are used in the general factor.

\hypertarget{modeling}{%
\subsubsection{Modeling}\label{modeling}}

\hypertarget{measurement-model}{%
\paragraph{Measurement Model}\label{measurement-model}}

We then fit a measurement model to examine the underlying structure of the variables of interest used to assess the latent constructs (general musical sophistication, WMC, general fluid intelligence) by performing a confirmatory factor analysis (CFA) using the lavaan package \citep{rosseelLavaanPackageStructural2012} using R \citep{teamLanguageEnvironmentStatistical2015}.
Model fits in can be found in Table 3.
For each model, latent factors were constrained to have a mean of 0 and variance of 1 in order to allow the latent covariances to be interpreted as correlations.
Since the objective measures were on different scales, all variables were converted to z scores before running any modeling.

\begin{itemize}
\tightlist
\item
  MODEL HERE
\end{itemize}

Variables are defined as follows: gen: general factor latent variable; wmc: working memory capacity; gf: general fluid intelligence; zIS: ``Identify What is Special''; zHO: ``Hear Once Sing Back''; zSB: ``Sing Back After 2-3''; zDS: ``Don't Sing In Public''; zSH: ``Sing In Harmony''; zJI:''Join In''; zNI: ``Number of Instrumetns''; zRP:''Regular Practice''; zNCS: ``Not Consider Self Musician''; zNcV: ``Never Complimented''; zST: ``Self Tonal''; zCP: ``Compare Performances''; zAd: ``Addiction''; zSI: ``Search Internet''; zWr: ``Writing About Music''; zFr: ``Free Time''; zTP: ``Tone Span''; zMS: ``Symmetry Span''; zMO: ``Operation Span''; zRA: ``Ravens''; zAN: ``Number Series''.

\hypertarget{structural-equation-models}{%
\subsubsection{Structural Equation Models}\label{structural-equation-models}}

Following the initial measurement model, we then fit a series of SEMs in order to investigate both the degree to which factor loadings changed when variables were removed from the model as well as the model fits.
We began with a model incorporating our three latent variables (general musical sophistication, WMC, general fluid intelligence) predicting our two objective measures (beat perception and melodic memory scores) and then detailed steps we took in order to improve model fit.
For each model, we calculated four model fits: χ2 , comparative fit index (CFI), root mean square error (RMSEA), and Tucker Lewis Index (TLI).
In general, a non-significant χ2 indicates good model fit, but is overly sensitive to sample size.
Comparative Fit Index (CFI) values of .95 or higher are considered to be indicative of good model fits as well as Root Mean Square Error (RMSEA) values of .06 or lower, Tucker Lewis Index (TLI) values closer to 1 indicate a better fit. (Beajean, 2014).

After running the first model (Model 1), we then examined the residuals between the correlation matrix the model expects and our actual correlation matrix looking for residuals
above .1.
While some variables scored near .1, two items dealing with being able to sing (``I can hear a melody once and sing it back after hearing it 2 -- 3 times'' and ``I can hear a melody once and sing it back'') exhibited a high level of correlation amongst the residuals (.41) and were removed for Model 2 and model fit improved significantly (χ2 (41)=123.39,
p \textless{} . 001).

After removing the poorly fitting items, we then proceeded to examine if removing the general musical sophistication self-report measures would significantly improve model fit for Model 3.
Fit measures for Model 3 can be seen in Table 3 and removing the self-report items resulted in a significantly better model fit (χ2 (171)=438.8, p \textless{} . 001).
Following the rule of thumb that at least 3 variables should be used to define any latent-variable (Beajuean, 2014) we modelled WMC as latent variable and Gf as a composite average of the two tasks administered in order to improve model fit.
This model resulted in significant improvement to the model (χ2 (4)=14.37, p \textless{} . 001).
Finally we examined the change in test statistics between Model 2 and a model that removed the cognitive measures-- a model akin to one of the original models reported in \citep{mullensiefenMusicalityNonMusiciansIndex2014}-- for Model 5.
Testing between the two models resulted in a significant improvement in model fit (χ2 (78)=104.75, p \textless{} . 001).
Figure 3 displays Model 4, our nested model with the best fit indices.

\begin{tabular}{l|r|r}
\hline
semModelNames & df & chi\\
\hline
CFA & 186 & 533.60\\
\hline
Model 1 & 222 & 586.30\\
\hline
Model 2 & 181 & 462.90\\
\hline
Model 3 & 10 & 24.11\\
\hline
Model 4 & 6 & 9.74\\
\hline
Model 5 & 130 & 358.16\\
\hline
\end{tabular}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/measurementModel} 

}

\caption{CFA Measurement Model}\label{fig:measurementmodel}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/sem1} 

}

\caption{Full Model, All Variables Included}\label{fig:model1}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/sem2} 

}

\caption{Full Model, Highly Correlated Residual Items}\label{fig:model2}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/sem3} 

}

\caption{Self Report Removed, Only Cognitive Measures}\label{fig:model3}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/sem4} 

}

\caption{Cognitive Measures, Gf as Observed}\label{fig:model4}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/sem5} 

}

\caption{General Self Report Only}\label{fig:model5}
\end{figure}

\hypertarget{discussion}{%
\subsection{Discussion}\label{discussion}}

\begin{itemize}
\tightlist
\item
  Add relevance to melodic dictation
\end{itemize}

\hypertarget{measurement-model-1}{%
\paragraph{Measurement Model}\label{measurement-model-1}}

After running a confirmatory factor analysis on the variables of interest, the model fit was below the threshold of what is considered a ``good model fit'' as shown in Table 1 with references to above model fits.
This finding is to be expected since no clear theoretical model has been put forward that would suggest that the general musical sophistication score, when modelled with two cognitive measures should have a good model fit.
This model was run to create a baseline measurement.

\hypertarget{structural-equation-model-fitting}{%
\paragraph{Structural Equation Model Fitting}\label{structural-equation-model-fitting}}

Following a series of nested model fits, we were able to improve model fits on a series of SEMs that incorporated both measures of WMC and measures of general fluid intelligence.
Before commenting on new models, it is worth noting that the Model 5 does not seem to align with the findings from the original 2014 paper by \citep{mullensiefenMusicalityNonMusiciansIndex2014}l.
While the correlation between the objective tasks is the same (r = .16), the factor loadings from this paper suggest lower values for both Beat Perception (.37 original, .27 this paper) as well as Melodic Memory (.28 original, .18 this paper).
Note that two items were removed dealing with melody for memory for this model; when those items were re-run with the data, the factor
loadings did not deviate from these numbers.

The first two models we ran resulted in minor improvements to model fit.
While difference in models was significant (\(\chi^2\) (41)=123.39, p \textless{} . 001 ), probably due to the number of parameters that were now not constrained, the relative fit indices of the models did not change drastically.
It was not until the self-report measures were removed from the model, and then manipulated according to latent variable modeling recommendations, was there a marked increase in the relative fit indices.
Fitting the model with only the cognitive measures, we were able to enter the bounds of acceptable relative fit indices that were noted above.
In order to find evidence that the cognitive models (Models 3 and 4) were indeed a better fit than using the General factor, we additionally ran a comparison between our adjusted measurement model and a model with only the self report.
While both of our nested models were significantly different, the cognitive models exhibited superior relative fit indices.
Lastly, turning to Figure 3, we note that our latent variable of WMC exhibited much larger factor loadings predicting the two objective, perceptual tests than our measure of general fluid intelligence.
We also note that the factor loading predicting the Beat Perception task (.36) was higher than that of the Melodic Memory task (.21).
These rankings mirror that of the original \citep{mullensiefenMusicalityNonMusiciansIndex2014} paper and merit further examination in order to disentangle what processes are contributing to both tasks.

These results align with predictions made with Process Overlap Theory \citep{kovacsProcessOverlapTheory2016}, which predict that higher executive loads are needed for tasks of perception.
While we failed to predict which task would load higher --we assumed that the ability to maintain and manipulate information in the Melodic Memory task would be better predicted by WMC than the Beat Perception task-- this might be due to the fact that performing well in a melodic memory task demands a certain amount of musical training that is not captured by either cognitive measure.
In the future, we are interested in exploring more theoretically-driven models that use specific, task oriented predictors in order to explain the relationships between the perceptual tasks and the cognitive measures.
Given the results here that suggest that measures of cognitive ability play a significant role in tasks of musical perception, we suggest that future research should consider taking measures of cognitive ability into account, so that other variables of interest are able to be shown to contribute above and beyond baseline cognitive measures.

In this chapter we fit a series of structural equation models in order to investigate the degree to which baseline cognitive ability was able to predict performance on a musical perception task.
Our findings suggest that measures of WMC are able to account for a large amount of variance beyond that of self report in tasks of musical perception.

\hypertarget{computation-chapter}{%
\chapter{Computation Chapter}\label{computation-chapter}}

--- ISMIR
; symbolic music processing
; computational music theory and musicology
; cognitive MIR
; datasets
; evaluation beyond just notes
; training and education application
---

\hypertarget{rationale-1}{%
\section{Rationale}\label{rationale-1}}

Music theorists use their pedagogical intutions to build the appropriate curricula for their aural skills pedagogy.
Teaching aural skills typically starts with simpler exercises, with a limited number of notes and rhythms, and then slowly progresses to more difficult repertoire.
Evidence for this progression is evident in aural skills text books.
Of the major aural skills textbooks such as the Ottmann, the Berkowitz, Kaprinski, Clendenning Marvin, DobreaGrindal Cleland, each is structured in such a way that musical material earlier on in the book is considered easier than that near the end.
This is true of almost any ETUDE book; open a random page and the difficulty of the study will most likely scale accordingly to it's relative position in the textbook.
But what makes melodies become more difficult to perform?

Intuitivily, music theorists have a general understanding of what makes a melody difficult.
It would be safe to assume that more complex melodies are more difficult to perform.
Factors that might contribute to this complexity could be attributes such as the melody's note density, the number of notes availble to be chosen from (is it pentatonic, diatnoic, is the entire chromatic aggregate present?), or even more intuitivly understood factors like how tonal the melody sounds or the intricacies of the rhythms involved.
Given all these factors, there is no definitive combination of features that could always predict how complex a melody is.
In many ways, questions of melodic complexity are very much like questions of melodic similarity: it depends on both who is aksing the question and for what reasons (CITE SELF).

Looking at the melodies presented in Figures X and Y, a music theorist can sucessfully intuit which melody is more complex, and presumably, more difficulty to dictate.

FIGURES 1 and 2

While I reserve and extended discussion of what features might characterize why one melody is more difficult than the other to dictate for below, I find it safe to assume that these melodies do differ in a fundamentally different way when perfored in a similar fashion.

Additionally, many readers of this dissertaiton can probably draw from anecdotal evidence of their own as to how student's at various stages of their aural training might fair when asked to dictate both melodies.
For some, melody Y might be overwhelmingly difficult.
In fact, it might be overwhelmignly difficulty for the vast majority of people.
Student are quick establish if they belive that a melody they are being tested on is too difficult, and importantly from a pedagogical standpoint, we as educators need to be able to know how difficutly melodies are given our students in order to asses a degree of fariness in our grading of student's performance.
While of course with each student and grading, there are inveitably many other variables at play ranging from personal abilities to the goals of the instructor in the scope of their course, there are intuitive benchmarks that students are expected to be able to complete throughout their education.
As students progress, they are expected to be able to dictate more and more complex melodies, yet exactly what makes a melody complex is often left to the expertise and intuition of a theorist.

In this chapter I examine how tools from computational musicology can be used to help model an aural skill's notion of complexity in melodies.
First, I establish that theorists agree on the differences in melodic complexity using results from a small survey of XX theorists.
Second, explore how both static and dynamic computationally derrived measures can be used to approximate the theorist's intution.
Third and finally, I use evidence afforded by research in computational musicology to posit that the distributional patterns in a corpus of muisc can be used to create a more linear path to sucess amongst students.
I demonstrate how by combining evidence from the statistical learning hypothesis, the proabilistic prediction hypothesis, and the perceptual fluency hypothesis can be used to demonstrate why some musical sequences in a melody of a certain complexity are easier to remember than others, and how this can be modelled computationally.
Using this logic, I then create a new compendium of short melodies, sorted by their perceptual complexity, that can be used for teaching applications.

\hypertarget{agreeing-on-complexity}{%
\section{Agreeing on Complexity}\label{agreeing-on-complexity}}

Returning to melodies X and Y from above, an aural skills pedagogue most likely has some sort of intution of which of the two melodies would be easier to dictate.
Melody X --- describe melody X.
On the other hand, Melody Y --- describe melody Y.
Yet both melodies are same range, same style, have one chromatic note, and use the exact same set of rhythms, just reorganized to be played at different times.
Considering these differences, it would be safe to assume that melody X is probably easier to dictate than melody, all other features held constant\footnote{I assume that both melodies were to be played}.

In fact, aural skills pedagoges tend to agree for the most part on questions of difficulty of dication.
To demonstrate this, I surveyed XX aural skills pedagogues who have all taught at least two years of aural skills at the University level asking them questions presented in TABLE X on a sample of XX melodies found in the BERKOWITZ.
Overall, the sample exhibited a \_\_\_\_\_ degree of interrater reliability as measured by X\footnote{Reference here about what is good and what is bad.}
Plotting the respondant's answers across the textbook, with the book progressing from less to more difficult, there is a trend of pedagoges to AGREE ON THE SIMPLER ONES THEN HAVE MORE DIAGREEMENT LATER ON?
Central to my argument is a VERY LINEAR TREND of rating of complexity that correlate with both PAGE NUMBER from the textbook that it was drawn from, as well as an even better fitting model of THE EXACT NUMBER OF THE MELODY.
Also intersting is point here about not everyone agreeing on what is hard????

Taken together, both anecdotal and the evidence for this survey suggest that aural skills pedagoges tend to agree on how complex a melody is for use in an aural skills setting.
This sense of difficutly or complexity tracks as the book progresses, (duh), but to attribute the cause of a melody being difficult as its position in the book would be making a pretty hilarious error.
And importnatly, even experts seem not to agree about the appropirateness of the complex melodies.
And to compound this problem, literature from psychology (Kanneman and Tversky, that medicice doctor study) importantly highlight the fact that it's important to be skeptical of expert opinions.
Drawing from the medical liteature-- a body of research where getting somethign wrong probably matters more-- have evidence of stuff happenign with heart attcks.
Undelrying logic being that people think they have the key, but really they do not.
And what solved this was a computaional problem.
The rest of the chapter goes on to investigate if similar computaionally derrived tools can help aural skills pedagogy.

\hypertarget{modeling-complexity}{%
\section{Modeling Complexity}\label{modeling-complexity}}

The ability to quantify what theorists generally agree to be melodic complexity depends on the ability to distill complexity into it component parts.
Earlier, when comparing melodies X and Y, some of the features put forward that might contribute to this measure were features like note density, range, what scale the melody draws its notes from, how expected the notes are, and the rhthmic scaffordling that the melody hangs on.
The combination of these smaller features makes up what could be concieved of as complexity.

As with any computionally derrived measures, what is input into the model is very important.
And just because something is salient, does not mean that it should be entered.
Attemping to derive features of melodies goes back a long way.
-- ortmann
and some actual music ed studies since then.
It assumes some sort of suspended animation of listening, but close to something.
While phenomicaloglcally correct, measures like this are becoming more common in music psychology research.

\hypertarget{static}{%
\subsection{Static}\label{static}}

\begin{itemize}
\tightlist
\item
  idea with an abstracted feature is that it is something perceptable to the listener, but possibly hard to quantify given traditional tools of music theory.
\end{itemize}

Often, these abstracted features come inspired from other domains like computational linguistics.
Perhaps one of the most popular features in recent decades is the normalized pairwise variability index or nPVI.
The nPVI is a measure of xxxx in a langauge.
Shown in FIGURE X, it quantifies the xxx by the yyy in order to proivde some sort of metric that can serve as a stand in of the perception that some langauges sound perceptually different.
In lingusitics, the nPVI has been used to delinate stress timed langauges from something else\ldots{}
Recenlty in the past x years, music science researchers have used the nPVI to attempt to investigate claims about the relationship between speeach and laguage (CITE ALL HERE).

While results are mixed and some people argue against nPVI (Nat), it does serve as a very good example of a static comuputationall derrived measure.
Just like taking the average weight in a population, the nPVI summerizes a phrase and importantly assumes that this measure is representive of the entire phrase the calcuation was performed upon.

or stuff here of meredith and that chapter.

Perhaps the most complete set of computational measures as applied to music perception comes from Daniel Mullensiefens' whole name FANTASTIC toolbox CITE.
According to FANTATIC's techncial report,

\begin{quote}
``FANTASTIC is a program\ldots{}that analyzes melodies by computing features The aim is to characterise a melody or a melodic phrase by a set of numerical or categorical values reflecting different aspects of musical structure. This feature representation of melodies can then be applied in Music Information Retrieval algorithms or computational
models of melody cognition.'' (pp.~4)
\end{quote}

Drawing from fields both central and perphircal to music science, FANTASTIC computes a collection of XX features to analyze features of melodies and continues from a tradition of feature extraction in music research FROM Lomax (1977), Steinbeck (1982), Jesser (1990), Sagrillo (1999), Eerola and Toiviainen (2004) and since it's release has been sucessful at \ldots{}\ldots{} SINCE THEN.
Addittionally, FANTASATIC also provides a framework for comparing the features of a melody with a parent corpus from which the melody belongs.

Returning to the Aural Skills classroom, many of these features can be used to approximate the intutions of complexity as agreed upon by theorists.
Below, I SHOW A SERIES OF PLOTS WHERE the continously measured abstracted features of FANTASTIC are plotted against the measures of perceieved complexity and difficulty of expert aural skills pedagoges with their respective correlations in TABLE X.

GIANT FIGURE HERE

GIANT TABLE HERE

From this, it becomes evident that some features like \_\_\_\_\_\_\_\_\_\_ and \_\_\_\_\_\_\_\_ succede quite well in explaining the variance of the rated complexity measures, while others like \_\_\_\_\_\_\_\_\_\_ and \_\_\_\_\_\_\_\_ do not.
I suggest that the reasons that \_\_\_\_\_\_\_\_ measures are sucessful in explainig is because \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_.

In modeling this problem univariatly (one v one), it quicly becomes evident that no single static measure from FANTASTIC is able to complete mirror that of the complexity measrues.
Following past research (HARRISON, ME, OTHER PAPER THAT I CANT THINK OF), I used a PRINCIPAL COMPONENTS ANALYSIS to distill a single measure of complexity from THIS THIS AND THIS, that I then plot against the ratings and show that using this measure in a regression context increases our correlatio/R2 up a bit.

Hopefully at this point, I can make the argument that I am getting quite close at fully explaining the variance in ratings.
Finally, using the features of FANTASTIC, I can also use a random forrest method (LIKE THESE DANIEL PAPERS) to use some machine learning methods to see what best explains this data.

USING A \ldots{}. details of Random Forrest here\ldots{}\ldots{}

And importanly\ldots{}.. Variable imporacne plot which works by running multiple models and seeing how much variance is accounted for when the model is left out.

We see that X Y Z are of highest importance, and this corroborates literature from other FANTASTIC measures that say XYZ.

This is important for pedagogy, this can be used to help provide objective measure of difficulty.
Can also be a starting point for work on linking these various features with listener response.
And thus help people desgin curricula and also then better understand human pecerception and role of melody in the aural skills clasroom.

Though while sucessful at modeling, using various linear combinations of these static abstracted features still assumes that listerns experience melodies in some sort of perceptual suspended animation.
In order to have more phenomenologically approriate model that incorporates computainoally derived features, it is important to turn to dynamic models of music perception.

\hypertarget{dynamic-1}{%
\subsection{Dynamic}\label{dynamic-1}}

The Information Dynamic of Muscic (IDyOM) model of Marcus Pearce is a computational model of auditory cognition that \ldots{} PEARCE 2018.
Unlike measures from FANTASTIC, that calculate summary statistics on melodies, IDyOM works by\ldots{}.
As mentioned in Chapter 1, IDyOM is based both on the SLH and PPH.
THey state\ldots{}.
Due to the fact that IDyOM makes its calculations based on a series of n-grams that the model learns, IDyOM is able to output measures of expectedness for each symbolic token used in its calculations.
As a model, IDyOM has been sucessful at modeling\ldots{}.
The findings are robust, yet the measures of information content based on Shannon entropy have yet to be used to actually quantify memory (even ala Miller 1956).

Using measures of IC from IDyOM is a novel application of the IDyOM model, but makes sense.
Literature from sequential learning notes that much of our learning for materials happens implicitly
This happens from both langauge and music.
Roherhimer reference
Margulis reference

If true, repeatitive stimuli are then easier to processly as they are going to tax memory less.
Lking aslo follow that IC measures of expctedness (if we assume expct are easier ot procecss) can be used in memory measure

Take for example the following two 8grams
THey are listed with their informaiton contnet.
One is the opening of this tune
The other is this really famous tone row

\hypertarget{frequency-facilitation-hypothesis}{%
\section{Frequency Facilitation Hypothesis}\label{frequency-facilitation-hypothesis}}

Can see that both are sequences of eight notes, but one consists of more predictiable notes.
Can model this in terms of information content and as information accumulates over the phrase.
Intutivly, returning to logic from above, one will be easier to remember and dictate.
And This is due to processing fluency, thus less of a tax on memory.
So I am saying that you can use the IC measures of informaiton as actul measures of information.

More formally, the evidence can be summerized in what I am calling DFH.
Which states:

\begin{quote}
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  some sequences are easier to process, means they are more expected.
\item
  IC measures of expectancy can be used as prox for memory
\item
  Given sequence of N lenght melody, ease of dication which is loads on memory is relative to its degree of expectedness in IC and link it back to corpus by saying that it is relative to it's n-gram distribution frequency.
\end{enumerate}
\end{quote}

From this hypothesis, we derrive a couple of testiable predciitions that we can continue to
use tools from computational musicology and subsequestntly music psychology to answer.

\hypertarget{distributional-patterns-in-corpus}{%
\subsection{Distributional Patterns in Corpus}\label{distributional-patterns-in-corpus}}

If this is true, we should see this again throughotu the corpus
And If IC measures can be used as memory proxy bc of processing due to SLH and PPH, would follow that n-grams woudl be processed relative to their distribution in a corpus.
And they will have lower IC.
To show this I extract out of a corpus of 2 3 5 10 grams form both sides of frequency distribution of a corpus to present side by side show intuitive.

we can also plot this continously

\hypertarget{memory-facilitation}{%
\subsection{Memory Facilitation}\label{memory-facilitation}}

Predictions here OR add in mini experiment that will be test for ISMIR paper.

go for further psychology experiemnts in this,
lays basis for the computational model
and more direct pedagogical applicaitons are discussed below

\hypertarget{peagogical-applcation}{%
\subsection{Peagogical applcation}\label{peagogical-applcation}}

Here is how you would do it pedagoically
If this is case, would mean that this is a new way to organize melodies in book.
Could instead list all melodies and have them as snippets.
Orgnize them like books in terms of ascending difficulty.
And this is how i have done it.
Samples from all the n-grams in this table

TABLE

Can find all the things in the back.
Would suggest learning these patterns as supplment to melody learning.
It would do the following things.
I list melodies in the back.
I would predict that if people learned this as supplment, they would do a lot better.
Also get rid of some sort of performance anxiety in little successes.

\hypertarget{chapter-conclusions}{%
\section{Chapter Conclusions}\label{chapter-conclusions}}

In this chapter I have demonstrated how tools from computational musicology can be used as an aide in aural skills pedagogy.
After first establishing the extent to which aural skills pedagogues on various melody parameters, I then show how two families of computationally derrived features can stand in for a pedaguges intution.
First, using the FANTASTIC toolbox, I show how different combintations of static abstracted features can help explain theorists agreed upon complexity.
This first will help with selection of melodies and also provides insights as to which features of the melodies contribute most to percieved difficutly.
Second, I demonstrated how assumptions derived from the IDyOM framework can serve as a basis for the intutions of why smaller sequences of notes within melodies are more or less difficult to dictate.
Using the logic that sequences that are easier to process are more expected, and that computed measures of information content can be used as a proxy for memory, I show that it follows that given the sequence of an N lenght melody, the ease of dictaiton that it loads on memory is relative to both its degree of expectednes quantified in terms of informaiton content and link it back to hte corpus by linking THAT to it's n-gram distribbutional freuqency.
This chain of thinking then allowed me to put forward a new sequence of melody segments that can be arragned, like other theory textbooks, in terms of their increasing complexiy.
I argue that using this smaller, snippit approach, will allow students to not be overwhelmed in their learning by taking a more linear path to dictation, before moving on to more more ecologically valid melodies.
I finish by disucssiong how this might be implemented in the classroom.

THER PEOPLE WHO HAVE DONE THIS

\begin{itemize}
\tightlist
\item
  look into Wiggins et al., 1993, for history of representation
\end{itemize}

Folk music

\begin{itemize}
\item
  Bartok 1936?
\item
  Bartok and Lord 1951
\item
  Lomax 1977 ; Lomax, A. (1977). Universals in song. The World of Music, 19, 117--129.
\item
  Steinbeck 1982
\item
  Jesser 1992
\item
  Sagrillo 1999
\item
  GET AND READ PAT SAVAGE ARTICLE
\end{itemize}

Popular Music

\begin{itemize}
\tightlist
\item
  Moor 2006
\item
  Kramarz 2006
\item
  Furnes 2006
\item
  Riedemann ????
\end{itemize}

Computational Musicology

\begin{itemize}
\tightlist
\item
  Eerola eta al 2007 and 2007
\item
  McCay 2005
\item
  Huron 2006
\item
  Frieler 2008
\item
  JAZZOMAT PROJECRT OUTPUT
\end{itemize}

\hypertarget{hello-corpus}{%
\chapter{Hello, Corpus}\label{hello-corpus}}

-- chapter will basically just be reference manual for corpus?
NO

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

After make argument about two different types of corpora, say we have implicit
If it is implicit, follows that there should be some sort of overlap
kind of like a markov chain and leonard meyer.

\begin{itemize}
\tightlist
\item
  Point is to introduce corpus
\item
  Origin outside music, current state IN music, limitations
\item
  Describe features of music in corpus
\item
  Compare it to the features in the Essen Collection
\item
  then talk about using IC idea from previosu chapter, how that can be applied pegagoically
\end{itemize}

Corpus work helps inform how we see the world bc it generalizes.
This was first done in linguistics.
Paragraph here on gw book.
Note that there is general split between sylometery studies and using it for NLP.

Similar paths were taken in music.
We have the MP trajectory that he outlines.
Many of the early studies just summerise.
Though they are largely situated in Meyer idea that genre is complex system of relations.

Interesting point here is the scoping between what a corpsu represents.
On one side we can have the collected works of a dead musician.
Something like all of the trumpet solos of clifford brown.
Basically have a population here, tho incomplete.
Theoretically could have all the studio recordings, transcribe them, total collection.
While we could sample from this, problem is that the set is finite.
There is no recreating of more clifford brown solos since he died.

Could further zoom out and look at a genre.
This is very much like the Nick cook chopin studies.
Mirrored here with Shanahan's Debussy studies.
Also shanahan chord change studies
Idea here is that you are looking for changes over time.
But again with clifford, can't really re-do the 20th century.
Maybe find one or two more tunes, but no way to re write cultural conceptions.
Almost tautologically we have kept what we wanted and used that for our definition.

Zooming out even further we have corpora like the Essen Collection.
here it is meant to be represntitive of folk songs of Europe.
Presumably this is also bounded by space and time
And it begs the question of should songs today be added to the Essen collection?
What does this corpus actually represent?

Societal conception of what our prototypes might derrive from?
Or Colleciton of folk songs for historical purposes? (Ala lomax, bartok, whatever)
At this level, represents the genre or style.

We see this assumption that these collections or styles are representing something we implicitly have in the work of MP IDyOM.
He trains his auditory computational model of cognition on a corpus that is limited in space and time, but then generalizes those probabilities into human behavior.

And the thing is that they are very sucessful.
We can look at the many things from Pearce 2018 where IDyOM is able to get it right.
Western, Jazz, Chinese music.

So knowing that there is a move from the single dead composer to a cognitive conception of style.
Would be good to have digital representaitons of sytles in brain.
Could argue that sight singing melodies in the bel canto tradition follow this.
Of course there is cultural baggage with it.
But as stated in previous chapters, lots of people do sight singing and sight reading.
And these melodies presumably come from the same culture they train musicians for,

Interesting question as what it would mean to add a batch of sgiht sininging melodies ot other corpora.

But here it is, a corpus of XXX melodies.

\hypertarget{why-i-dont-follow-a-random-sampling-method}{%
\subsection{Why I don't follow a random sampling method}\label{why-i-dont-follow-a-random-sampling-method}}

Also I will explicitly say here I don't follow a random sampling method as perscribed by London.
In his article comes to the conclusion that 300 or so pieces is needed for representive sample.
Show how the cannon needs to be connected to something perceptual.

** LONDON OF WHAT DOES IT MEAN TO BE REPRESENTIVE
-- london suggests 300 pieces for classical music representation
-- says that the point is the build a corpus that is ``broadly representative of the classical
composers, styles, and genres that are most familiar to the 21st century listener.''
-- says that this then comes from what we hear
--- JL deliniates between passive and active consumption
-- point is that these sources ``reflect our musical environment''
-- the implicit assumption being that works outside this sampling would be described as not belonging, but is that not really just a not natural category since if we were to decide that it did belong, it would get lumped in? Clearly the deliniating point here is that of questions of musical similarity and questions of musical similarity are questions of subjectivity. So have to realize with this loop that unless we can additionally provide external evidence that melodies are perceptually different, hard to maintain the borders of what the corpus represents.

-- Also does this take into account that the listener would change based on exposure and training
-- get direct and indirect uses of classical music and sample from that
-- def notes problems in sampling esp w value judgments and lenght of recordings

Matthews (2009) -- looking at american programming

Burden of learning to play the piece in terms of diversity?\textgreater{}?

VOLUME 31 OF MUSIC PERCEPTION

Moving away from performance like live stuff or score based.

Also when assempling a representive corpus, if you have a lot from one genere where the music is very similar, would htat not be an example of having your residuals correlated?

And is there also an implicit assumption that there is something intrisic to each of these melodies that would allow them to be perceptually distiguishable from the other ones.
Style crossovers are very hard , things like post modern jukebox.

So instead I am introducing novel corpus of 600 melodies that are not tethered to a large cannon in the way that something like a London corpus, or Barlow and Morgenstern, or Essen would presumably be.
In the above context they come from small subcultre most related to bel canto singing.

I first describe the corpus as a whole.

Descriptive statistics.
Number of songs.
Lenghts of songs.
Key distributions.
facet wrap of tone profiles.

But then I compare this to the Essen folk song and Densmore and China.

Would be interested to know if you train an IDyOM model on three distinct corpora, will the melody from each of the corpus reflect the different expectationa values in terms of cultural difference.

Make the argument that all corpora are implicit genres.
Find that pearce paper about historical listening.

\hypertarget{brief-review-of-chapter-4-on-corpus-language-to-reflect-journal-submission}{%
\section{Brief review of Chapter 4 on corpus (Language to reflect journal submission)}\label{brief-review-of-chapter-4-on-corpus-language-to-reflect-journal-submission}}

\hypertarget{corpus-outside-of-music}{%
\subsection{Corpus outside of music}\label{corpus-outside-of-music}}

\hypertarget{corpus-in-music}{%
\subsection{Corpus in Music}\label{corpus-in-music}}

\hypertarget{the-point-is-that-it-implicitly-represents-humand-knowledge}{%
\subsection{The point is that it implicitly represents humand knowledge}\label{the-point-is-that-it-implicitly-represents-humand-knowledge}}

\hypertarget{idyom-1}{%
\subsection{IDyOM 1}\label{idyom-1}}

\hypertarget{idyom-2}{%
\subsection{IDyOM 2}\label{idyom-2}}

\hypertarget{idyom-3}{%
\subsection{IDyOM 3}\label{idyom-3}}

\hypertarget{huron-suggestions-that-starts-of-melodies-relate-to-mental-rotaiton}{%
\subsection{Huron suggestions that starts of melodies relate to mental rotaiton}\label{huron-suggestions-that-starts-of-melodies-relate-to-mental-rotaiton}}

\hypertarget{other-huron-claims}{%
\subsection{Other Huron claims}\label{other-huron-claims}}

\hypertarget{note-problem-with-using-corpus-is-making-corpus}{%
\section{Note problem with using corpus is making corpus}\label{note-problem-with-using-corpus-is-making-corpus}}

\hypertarget{many-are-used-on-essen}{%
\subsection{Many are used on Essen}\label{many-are-used-on-essen}}

\hypertarget{brinkman-says-essen-sucks}{%
\subsection{Brinkman says Essen Sucks}\label{brinkman-says-essen-sucks}}

\hypertarget{if-going-to-make-generlizable-claims-need-to-always-have-new-data}{%
\subsection{If going to make generlizable claims, need to always have new data}\label{if-going-to-make-generlizable-claims-need-to-always-have-new-data}}

\hypertarget{solem-duty-to-encode-and-report-on-corpus}{%
\section{Solem duty to encode and report on corpus}\label{solem-duty-to-encode-and-report-on-corpus}}

\hypertarget{justin-london-article-on-what-makes-it-into-a-corpsu}{%
\subsection{Justin London Article on what makes it into a corpsu}\label{justin-london-article-on-what-makes-it-into-a-corpsu}}

\hypertarget{though-i-just-encoded-the-whole-thing-because-in-my-heart-of-hearts-im-a-bayesian}{%
\subsection{Though I just encoded the whole thing because in my heart of hearts I'm a Bayesian}\label{though-i-just-encoded-the-whole-thing-because-in-my-heart-of-hearts-im-a-bayesian}}

\hypertarget{the-corpus}{%
\section{The Corpus}\label{the-corpus}}

\hypertarget{history-of-sight-singign-books}{%
\subsection{History of Sight Singign books}\label{history-of-sight-singign-books}}

\hypertarget{assumed-to-be-where-long-term-store-comes-from-adumbrate-computational-model}{%
\subsection{Assumed to be where long term store comes from (adumbrate computational model)}\label{assumed-to-be-where-long-term-store-comes-from-adumbrate-computational-model}}

\hypertarget{lots-of-melodies-in-ascending-order-of-difficulty-grouped-appropriately-though-utah-guy}{%
\subsection{Lots of melodies in ascending order of difficulty, grouped appropriately though? Utah guy}\label{lots-of-melodies-in-ascending-order-of-difficulty-grouped-appropriately-though-utah-guy}}

\hypertarget{why-i-encoded-it-in-xml}{%
\subsection{Why I encoded it in XML}\label{why-i-encoded-it-in-xml}}

\hypertarget{is-it-legal}{%
\subsection{Is it legal?}\label{is-it-legal}}

\hypertarget{descriptive-stats-of-corpus}{%
\section{Descriptive Stats of Corpus}\label{descriptive-stats-of-corpus}}

\hypertarget{why}{%
\subsection{Why?}\label{why}}

\hypertarget{for-pedagogical-purposes}{%
\subsubsection{For pedagogical purposes}\label{for-pedagogical-purposes}}

\hypertarget{for-experimental-purposes}{%
\subsubsection{For experimental purposes}\label{for-experimental-purposes}}

\hypertarget{for-computational-idexing-get-me-melody-with-x-tonal-score}{%
\subsubsection{For computational idexing (get me melody with x tonal score)}\label{for-computational-idexing-get-me-melody-with-x-tonal-score}}

\hypertarget{could-serve-as-representation-of-implicitly-learned-expectations-for-future-modeling}{%
\subsubsection{Could serve as representation of implicitly learned expectations for future modeling}\label{could-serve-as-representation-of-implicitly-learned-expectations-for-future-modeling}}

\hypertarget{feature-level}{%
\subsection{Feature Level}\label{feature-level}}

\hypertarget{what-features-are-normally-distributed}{%
\subsubsection{What features are normally distributed}\label{what-features-are-normally-distributed}}

\hypertarget{correlated-feature-problem}{%
\subsubsection{Correlated feature problem}\label{correlated-feature-problem}}

\hypertarget{big-facet-wrap-of-the-whole-thing}{%
\subsubsection{big \textasciitilde{}facet wrap of the whole thing}\label{big-facet-wrap-of-the-whole-thing}}

\hypertarget{could-do-dimensonality-reduction-baker-harrison-others-but-then-loose-understanding}{%
\subsubsection{Could do dimensonality reduction (Baker, Harrison, others) but then loose understanding}\label{could-do-dimensonality-reduction-baker-harrison-others-but-then-loose-understanding}}

\hypertarget{n-gram}{%
\subsection{n-gram}\label{n-gram}}

\hypertarget{big-solfege-n-gram-table}{%
\subsubsection{Big solfege n-gram table}\label{big-solfege-n-gram-table}}

\hypertarget{dependent-on-representation-notes-solfege-mint}{%
\subsubsection{Dependent on representation (notes, solfege, mint)}\label{dependent-on-representation-notes-solfege-mint}}

\hypertarget{shiny-app-of-n-gram-heatmap-with-peter}{%
\subsubsection{Shiny app of n-gram heatmap with Peter}\label{shiny-app-of-n-gram-heatmap-with-peter}}

\hypertarget{idea-would-be-that-hotter-n-grams-lend-them-selves-to-better-chunking-but-need-better-word-than-chunking}{%
\subsubsection{Idea would be that hotter n-grams lend them selves to better chunking (but need better word than chunking)}\label{idea-would-be-that-hotter-n-grams-lend-them-selves-to-better-chunking-but-need-better-word-than-chunking}}

\hypertarget{experiments}{%
\chapter{Experiments}\label{experiments}}

\hypertarget{rationale-2}{%
\section{Rationale}\label{rationale-2}}

\hypertarget{have-done-all-this-and-have-not-actually-talked-about-dictation-yet}{%
\subsection{Have done all this and have not actually talked about dictation yet}\label{have-done-all-this-and-have-not-actually-talked-about-dictation-yet}}

\hypertarget{clearly-many-factors-contribte-to-this-whole-thing-and-need-to-be-taken-into-a-model}{%
\subsection{Clearly many factors contribte to this whole thing and need to be taken into a model}\label{clearly-many-factors-contribte-to-this-whole-thing-and-need-to-be-taken-into-a-model}}

\hypertarget{dictation-is-basically-a-within-subjects-design-experiment}{%
\subsection{Dictation is basically a within subjects design Experiment}\label{dictation-is-basically-a-within-subjects-design-experiment}}

\hypertarget{get-very-ecological-and-dirty-and-run-it}{%
\subsubsection{Get very ecological and dirty and run it}\label{get-very-ecological-and-dirty-and-run-it}}

\begin{itemize}
\tightlist
\item
  Paney 2016 had 30 second timing
\end{itemize}

\hypertarget{factors}{%
\subsection{Factors}\label{factors}}

\hypertarget{cognitive-1}{%
\subsubsection{Cognitive}\label{cognitive-1}}

\hypertarget{wmc}{%
\paragraph{WMC}\label{wmc}}

\hypertarget{gf}{%
\paragraph{GF}\label{gf}}

\hypertarget{training}{%
\subsubsection{Training}\label{training}}

\hypertarget{goldsmiths-msi}{%
\paragraph{Goldsmiths MSI}\label{goldsmiths-msi}}

\hypertarget{musical}{%
\subsubsection{Musical}\label{musical}}

\hypertarget{fantastic}{%
\paragraph{FANTASTIC}\label{fantastic}}

\hypertarget{idyom}{%
\paragraph{IDyOM}\label{idyom}}

\hypertarget{investigate-melodies-with-this-context-and-set-scoring}{%
\subsubsection{Investigate melodies with this context and set scoring}\label{investigate-melodies-with-this-context-and-set-scoring}}

\hypertarget{mirror-design-to-see-if-effects-of-melody-are-there}{%
\subsubsection{Mirror design to see if effects of melody are there}\label{mirror-design-to-see-if-effects-of-melody-are-there}}

\hypertarget{experiments-1}{%
\section{Experiments}\label{experiments-1}}

\hypertarget{experiment-i}{%
\subsection{Experiment I}\label{experiment-i}}

\hypertarget{participants-1}{%
\subsubsection{Participants}\label{participants-1}}

\hypertarget{procedure-1}{%
\subsubsection{Procedure}\label{procedure-1}}

\hypertarget{materials-1}{%
\subsubsection{Materials}\label{materials-1}}

\hypertarget{scoring}{%
\subsubsection{Scoring}\label{scoring}}

\hypertarget{results-1}{%
\subsubsection{Results}\label{results-1}}

\hypertarget{modeling-1}{%
\subsubsection{Modeling}\label{modeling-1}}

\hypertarget{discussion-1}{%
\subsubsection{Discussion}\label{discussion-1}}

\hypertarget{experiment-ii}{%
\subsection{Experiment II}\label{experiment-ii}}

\hypertarget{participants-new}{%
\subsubsection{Participants (New)}\label{participants-new}}

\hypertarget{procedure-same}{%
\subsubsection{Procedure (Same)}\label{procedure-same}}

\hypertarget{materials-swapped-but-controlled}{%
\subsubsection{Materials (Swapped but controlled)}\label{materials-swapped-but-controlled}}

\hypertarget{scoring-same}{%
\subsubsection{Scoring (Same)}\label{scoring-same}}

\hypertarget{results-2}{%
\subsubsection{Results}\label{results-2}}

\hypertarget{modeling-same}{%
\subsubsection{Modeling (same)}\label{modeling-same}}

\hypertarget{general-discussion}{%
\subsection{General Discussion}\label{general-discussion}}

\hypertarget{what-happened}{%
\subsubsection{What happened}\label{what-happened}}

\hypertarget{assumption-of-all-of-this-is-that-many-things-are-happening-linearly-in-combination-with-each-other}{%
\subsubsection{Assumption of all of this is that many things are happening linearly in combination with each other}\label{assumption-of-all-of-this-is-that-many-things-are-happening-linearly-in-combination-with-each-other}}

\hypertarget{additionally-the-mixed-effects-framework-works-better-with-more-data}{%
\subsubsection{Additionally the mixed effects framework works better with more data?}\label{additionally-the-mixed-effects-framework-works-better-with-more-data}}

\hypertarget{also-how-we-score-it-is-going-to-mess-wiht-the-dvs}{%
\subsubsection{Also how we score it is going to mess wiht the DVs}\label{also-how-we-score-it-is-going-to-mess-wiht-the-dvs}}

\hypertarget{really-what-is-needed-is-computational-model}{%
\subsection{Really what is needed is Computational Model}\label{really-what-is-needed-is-computational-model}}

\hypertarget{computational-model}{%
\chapter{Computational Model}\label{computational-model}}

\hypertarget{levels-of-abstraction}{%
\section{Levels of Abstraction}\label{levels-of-abstraction}}

In his 2007 article \emph{Models of Music Similarity} \citep{wigginsModelsMusicalSimilarity2007}, Geraint Wiggins distinguishes between \emph{descriptive} and \emph{explanatory} models in describing the modeling of human behavior.
Descriptive models assert what will happen in response to an event.
For example, as discussed in the previous chapter, as the note density of a melody increases and the tonalness of a melody decreases, a melody may become harder to dictate.
While the increase in note density is assumed to drive the decrease in dictation scores, merely stating that there is an established relationship between one variable and the other says nothing about the inner workings of this process.
An explanatory model on the other hand not only describes what will happen, but additionally notes why and how this process occurs.
For example, much of the work musical expectation demonstrates that as an individual's exposure to a musical style increases, so does their ability to predict specific events within a given musical texture \citep{pearceStatisticalLearningProbabilistic2018a}.

Not only does more exposure predict more accurate responses, but many of these models of musical expectation derive their underlying predictive power from the brain's ability to implicitly track statistical regularities in musical perception \citep{saffranStatisticalLearningTone1999, margulisRepeatHowMusic2014}.
The \emph{how} derives from the tracking of statistical regularities in musical information and the \emph{why} derives from evolutionary demands; Organisms that are able to make more accurate predictions about their environment are more likely to survive and pass on their genes \citep{huronSweetAnticipation2006}.

Wiggins writes that although there can be both explanatory and descriptive theories, depending on the level of abstraction, a theory may be explanatory at one level, yet descriptive at another.
Using the mind-brain dichotomy, he asserts that the example of a theory of musical expectation could be explanatory at the level of behavior as noted above, but says nothing about what is happening at the neural level.
Both descriptive and explanatory theories are needed: descriptive theories are used to test explanatory theories and by stringing together different layers of abstraction, we can arrive at a better understanding of how the world works.

Returning to melodic dictation, under Wiggins' framework the Karpinski model of melodic dictation \citep{karpinskiAuralSkillsAcquisition2000, karpinskiModelMusicPerception1990} qualifies as a descriptive model.
The model says what happens over the time course of a melodic dictation-- specifying four discrete stages discussed in earlier chapters-- but does not explicitly state \emph{how} or \emph{why} this process happens.
In order to have a more complete understanding of melodic dictation, an explanatory model is needed.

In this chapter I introduce an explanatory model of melodic dictation.
The model is inspired by work from both computational musicology and cognitive psychology.
From computational musicology I draw on the work of Marcus Pearce's IDyOM \citep{pearceConstructionEvaluationStatistical2005} and from cognitive psychology I draw from Nelson Cowan's Embedded Process model of working memory \citep{cowanEvolvingConceptionsMemory1988, cowanMagicalMysteryFour2010} to explain the perceptual components.
In addition to quantifying each step, the model incorporates flexible parameters that could be adjusted in order to accommodate individual differences, while still relying on a domain general process.
By relying on cognitive mechanisms based in statistical learning, rather than a rule based system for music analysis \citep{lerdahlGenerativeTheoryTonal1986, narmourAnalysisCognitionBasic1990, narmourAnalysisCognitionMelodic1992, temperleyCognitionBasicMusical2004} this model allows for the heterogeneity of musical experience among a diversity of music listeners.

\hypertarget{model-overview}{%
\section{Model Overview}\label{model-overview}}

The model consists of three main modules, each with its own set of parameters:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Prior Knowledge
\item
  Selective Attention
\item
  Transcription and Re-entry
\end{enumerate}

Inspired by Bayesian computational modeling, the \emph{Prior Knowledge} module reflects the previous knowledge an individual brings to the melodic dictation.
The \emph{Selective Attention}-- somewhat akin to Karpinski's extractive listening-- segments incoming musical information by using the window of attention as conceptualized as the limits of working memory capacity as a sensory bottleneck to constrict the size of musical chunk that an individual could to transcribe.
Once musical material is in the focus of attention, the \emph{Transcription} function pattern matches against the \emph{Prior Knowledge's} corpus of information in order to find a match of explicitly known musical information.
The \emph{Transcription} function will recursively truncate what musical information is in \emph{Selective Attention} if no match is found.
In addition to \emph{Transcription}, there is also a \emph{Re-entry} function that will restart the entire loop.
This process reflects, but does not actually mirror the exact cognitive process used in melodic dictation, yet seems to be phenomenologically similar to the decision making process used when attempting notate novel melodies.
Based on both the prior knowledge and individual differences of the individual, the model will scale in ability, with the general retrieval mechanisms in place.
The exact details of the assumptions, parameters, and complete formula of the model are discussed below.

\hypertarget{verbal-model}{%
\section{Verbal Model}\label{verbal-model}}

Below I describe my model's assumptions, parameters, as well as the steps taken when the model is run.
After detailing the inner workings of each of the assumptions and the modules, described in roughly the order that they occur, I present the model using psudeocode with the terminology described below.
I discuss the issues of assumptions and representations as they arise in describing the model.

\hypertarget{model-representational-assumptions}{%
\subsection{Model Representational Assumptions}\label{model-representational-assumptions}}

In order to write a computer program that mirrors the melodic dictation process, how the mind perceives and represents about musical information must be defined \emph{a priori}.
Before delving into questions of representation, this model assumes that the musical surface\footnote{As conceptualized as either a Schenkerian foreground \citep{schenkerFreieSatz1935} or defined by \citet{lerdahlGenerativeTheoryTonal1986}} as represented by the notes via Western musical notation are salient and can be perceived as distinct perceptual phenomena.
Although there is work that suggests that different cultures and levels of experience might not categorize melodic information universally \citep{mcdermottIndifferenceDissonanceNative2016}, other work suggests that experiencing pitches as discrete, categorical phenomena is categorized as a statistical human universal \citep{savageStatisticalUniversalsReveal2015}.
For the purposes of this model I assume that individuals do in fact perceive the musical surface similarly to the written score.

Knowing that it is melodic information or melodic data that needs to be represented, the question then becomes what is the best way in which to represent it.
This issue becomes increasingly complex when considering literature suggesting that the human mind represents musical information in a variety of different forms \citep{krumhanslCognitiveFoundationsMusical2001, levitinCurrentAdvancesCognitive2009}.

For the purposes of this model and further examples I choose to represent musical information using both the pitch (note and scale degree) and timing (rhythm and inter-onset-interval) representation described in \citet{pearceStatisticalLearningProbabilistic2018a}.
Future research comparing this model's output using different representations will also contribute to conversations regarding pedagogy in that if one form of data representation mirrors human behavior better than others, it would provide more than evidence in support of the pedagogy of one system over another.
How the model represents musical information is the first important parameter value that needs be chosen before running the model and this establishes the \emph{Prior Knowledge}.

\hypertarget{contents-of-the-prior-knowledge}{%
\subsection{Contents of the Prior Knowledge}\label{contents-of-the-prior-knowledge}}

The \emph{Prior Knowledge} consists of a corpus of digitally represented melodies taken to reflect the implicitly understood structural patterns in a musical style that the listener has been exposed to.
The logic of representing an individual's prior knowledge follows the assumptions of both the Statistical Learning Hypothesis (SLH) and the Probabilistic Prediction Hypothesis (PPH), both core theoretical assumptions of the Information Dynamic of Music (IDyOM) model of Marcus Pearce \citep{pearceConstructionEvaluationStatistical2005, pearceStatisticalLearningProbabilistic2018a}.
Using a corpus of melodies to represent an individual's prior knowledge relies on the Statistical Learning Hypothesis which states:

\begin{quote}
musical enculturation is a process of implicit statistical learning in which listeners progressively acquire internal models of the statistical and structural regularities present in the musical styles to which they are exposed, over short (e.g., an individual piece of music) and long time scales (e.g., an entire lifetime of listening). p.2 (Pearce, 2018)
\end{quote}

The logic here is that the more an individual is exposed musical material, the more they will implicitly understand it which leads the corroborating probabilistic prediction hypothesis which states:

\begin{quote}
while listening to new music, an enculturated listener applies models learned via the SLH to generate probabilistic predictions that enable them to organize and process their mental representations of the music and generate culturally appropriate responses. p.2 (Pearce, 2018).
\end{quote}

Taken together and then quantified using Shannon information content \citep{shannonMathematicalTheoryCommunication1948}, it then becomes possible using the IDyOM framework to have a quantifiable measure that reliably predicts the amount of perceived unexpectedness in a musical melody that can change pending on the musical corpus that the model is trained on.
As a model IDyOM has been successful mirroring human behavior in melodies in various styles \citep{pearceStatisticalLearningProbabilistic2018a}, harmony-- outperforming \citep{harrisonDissociatingSensoryCognitive2018} sensory models of harmony \citep{bigandEmpiricalEvidenceMusical2014}--, and is also being developed to handle polyphonic materials \citep{sauvePredictionPolyphonyModelling2017}.

Stepping beyond the assumptions of IDyOM, the prior knowledge also needs to have a implicit/explicitly known parameter which indicates whether or not an pattern of music-- or n-gram\footnote{n-grams refer to the amount of musical objects in a string. For example a bi-gram or 2-gram, would be an interval. Tri-grams or 3-grams would consist of two intervals and so on.} pattern-- is explicitly learned.
This threshold can be set relative to the entire distribution of all n-grams in the corpus.

\hypertarget{modeling-information-content}{%
\subsection{Modeling Information Content}\label{modeling-information-content}}

Having established that the models' first parameters to be decided are the representation of strings and the implicit/explicit threshold, the next decision that has to be made is how the model decides segmentation for the second stage of \emph{Selective Attention}.
Although there has been a large amount of work on different ways to segment the musical surface using rule based methods \citep{lerdahlGenerativeTheoryTonal1986, margulisModelMelodicExpectation2005, narmourAnalysisCognitionBasic1990, narmourAnalysisCognitionMelodic1992}, which rely on matching a music theorist's intuition with a set of descriptive rules somewhat like the boundary formation rules put forward in \emph{A Generative Theory of Tonal Music}, as noted by Pearce \citep{pearceStatisticalLearningProbabilistic2018a}, rule based models often fail at when applied to music outside the Western art music canon.
Additionally, since melodic dictation is an active memory process, rather than a semi-passive process of listening, this model needs to be able to quantify musical information on two conditions.
The first is that it must be dependent on prior musical experience.
The second is that it should allow for a movable boundary for selective attention so that musical information that is memory can be actively maintained while carrying out another cognitive process, that of notating the melody.

In order to create this metric, I rely on IDyOM's use of information content \citep{shannonMathematicalTheoryCommunication1948} which quantifies the information content of melodies based on corpus of materials.
For example, when trained against a corpus of melodies, this excerpt in Figure 7.1 from the fourth movement of Schubert's \emph{Octet in F Major} (D.803) lists the information content of the excerpt calculated for each note atop the notation\footnote{The following musical examples is taken from \citet{pearceStatisticalLearningProbabilistic2018a} reflects a model where IDyOM was configured to predict pitch with an attribute linking melodic pitch interval and chromatic scale degree (pitch and scale degree) using both the short-term and long-term models, the latter trained on 903 folk songs and chorales (data sets 1, 2, and 9 from table 4.1 in \citep{schaffrathEssenFolkSong1995} comprising 50,867 notes.}
Appearing in Figure 7.2, I plot the cumulative information content of the melody, along with both an arbitrary threshold for the limits of working memory capacity and where the subsequent segmentation boundary for musical material to be put in the \emph{Selective Attention} buffer would be.
These values chosen show a small example of how the \emph{Selective Attention} module works.
The advantage of operationalizing how an individual hears a melody like this is that melodies with lower information content, derived from an understanding of having more predictable patterns from the corpus, will allow for larger chunks to be put inside of the selective attention buffer.
Additionally, individuals with higher working memory capacity would be able to take in more musical information.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/SchubertF} 

}

\caption{Cadential Excerpt from Schubert's Octet in F Major}\label{fig:unnamed-chunk-3}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=0.75\linewidth]{img/cumSchubert} 

}

\caption{Cumulative Information in Schubert Octet Excerpt}\label{fig:unnamed-chunk-4}
\end{figure}

It is important to highlight that the notes above the melody here are dependent on what is current in the \emph{Prior Knowledge} module.
A corpus of \emph{Prior Knowledge} with less melodies would lead to higher information content measures for each set of notes, while a prior knowledge that has extensive tracking of the patterns would lead to lower information content.
This increase in predictive accuracy mathematically reflects the intuition that those with more listening experience can process greater chunks of musical information.

\hypertarget{setting-limits-with-transcribe}{%
\subsection{Setting Limits with Transcribe}\label{setting-limits-with-transcribe}}

With each note then quantified with a measure of information content, it then becomes possible to set a limit on the maximum amount of information that the individual would be able to hold in memory as defined by the \emph{Selective Attention} module.
A higher threshold would allow for more musical material to be put in the attentional buffer, and a lower threshold would restrict the amount of information held in an attentional buffer.
By putting a threshold on this value, this serves as something akin to a perceptual bottleneck based on the assumption that there is a capacity limit to that of working memory \citep{cowanEvolvingConceptionsMemory1988, cowanMagicalMysteryFour2010}.
Modulating this boundary will help provide insights into the degree to which melodic material can be retained between high and low working memory span individuals.

In practice, notes would enter the attentional buffer until the information content from the melody is equal to the memory threshold.
At this point, the notes that are in the attentional buffer are segmented and will be actively maintained in the \emph{Selective Attention} buffer.
In theory, the maximum of the attentional buffer should not be reached since the individual performing the dictation would still need mental resources and attention to actively manipulate the information in the attentional buffer for the process of notating.

\hypertarget{pattern-matching}{%
\subsection{Pattern Matching}\label{pattern-matching}}

With subset of notes of the melody represented in the attentional buffer, whether or not the melody becomes notated depends on whether or not the melody or string in the buffer can be matched with a string that is explicitly known in the corpus.
Mirroring a search pattern akin to Cowan's Embedded Process model \citep{cowanEvolvingConceptionsMemory1988, cowanMagicalMysteryFour2010}, the individual would search across their long term memory, or \emph{Prior Knowledge} for anything close to or resembling the pattern in the \emph{Selective Attention} buffer.
Cowan's model differs from other more module based models of working memory like those of \citet{baddeleyWorkingMemory1974} by positing that working memory should be conceptualized as a small window of conscious attention.
As an individual directs their attention to concepts represented in their long term memory, they can only spotlight a finite amount of information where categorical information regarding what is in the window of attention not far from retrieval.
An example of this bottle necking is given after a formal statement of the model.
Using this logic, longer pattern strings n-grams would be less likely to be recalled exactly since they occur less frequently in the prior knowledge.

When searching for a pattern match, the \emph{Transcription} module is at work.
If a pattern match that has been moved to \emph{Selective Attention} is immediately found, the contents of \emph{Selective Attention} would be considered to be notated.
The model would register that a loop had taken place and document the n-gram match.
Of course, finding an immediate pattern match each time is highly unlikely and the model needs to be able to compensate if that happens.

If a pattern is not found in the initial search that is \emph{explicitly} known, one token of the n-gram would be dropped off the string and the search would happen again.
This recursive search would happen until an explicit long term memory match is made.
Like humans taking melodic dictation, the computer would have the best luck finding patterns that fall within the largest density of a corpus of intervals distribution.
Additionally, like students performing a dictation, if a student does not explicitly know an interval, or a 2-gram, the dictation would not be able to be completed.
If this happens, both the model and student would have to move on to the next segment via the \emph{Re-entry} function.

Eventually there would be a successful explicit match of a string in the \emph{Transcription} module and that section of the melody would be considered to be dictated.
The model here would register that one iteration of the function has been run and the chunk transcribed would then be recorded.
After recording this history, the process would happen again starting at either the next note from where the model left off, the note in the entire string with the lowest information content, or n-gram left in the melody with that is most represented in the corpus.
This parameter is defined before the model is run and the question of dictation re-entry certainly warrants further research and investigation.

This type of pattern search is also dependent on the way that the \emph{Prior Knowledge} is represented.
In the example here, both pitch and rhythmic information are represented in the string.
Since there is probably a very low likelihood of finding an exact match for every n-gram with both pitch and rhythm, this pattern search can happen again with both rhythms and pitch information queried separately.
If not found, exact pitch-temporal matches are found and the search is run again on either the pitch or rhythmic information separately; this would be computationally akin to Karpinski's proto-notation that he suggests students use in learning how to take melodic dictation \citep[p.88]{karpinskiAuralSkillsAcquisition2000}.
This feature of the model would predict that more efficient dictations would happen when pitch and interval information is dictated simultaneously.
Running the model prioritizing the secondary search with either pitch or rhythmic information will provide new insights into practical applications of dictation strategies.
Using this separate search feature as an option of the model seems to match with the intuitions strategies that someone dictating a melody might use.

\hypertarget{dictation-re-entry}{%
\subsection{Dictation Re-Entry}\label{dictation-re-entry}}

Upon the successful pattern match of a string, the \emph{Selective Attention} and \emph{Transcription} module would need too then be run again.
This process is done via the \emph{Re-entry} function.
As noted above, re-entry in the melody could be a highly subjective point of discussion.
The model could either re-enter at the last note where the function successfully left off, the note in the melody with the lowest information content, the n-gram most salient in the corpus, or theoretically any other type of way that could be computationally implemented.
Entering at the last note not transcribed is logical from a computational standpoint, but this linear approach seems to be at odds with anecdotal experience.
Entering at the note with the lowest information content seems to provide a intuitive point of re-entry in that it would then be easier to transcribe.
Entering at the most represented n-gram seems to match the most with intuition in that people would want to tackle the easier tasks first, but this rests on the assumption that humans are able to reliably detect the sections of a melody that are easiest to transcribe based on implicitly learned statistical patterns.
For example, some people might instead choose to go to the end of a melody after successful transcription of the start of the melody.
This might be because this part of the melody is most active in memory due to a recency effect, or it could be that that cadential gestures are more common in being represented in the prior knowledge.

\hypertarget{completion}{%
\subsection{Completion}\label{completion}}

Given the recursive nature of this process, if all 2-grams are explicitly represented in the \emph{Prior Knowledge} then the target melody should be transcribed.
If only represented using such a small chunk, the model will have to loop over the melody many times, thus indicating that the transcriber had a high degree of difficulty dictating the melody.
If there is a gap in explicit knowledge in the prior knowledge, only patches of the melody will be recorded and the melody will not be recorded in its entirety.
An easier transcription will result in less iterations of the model with larger chunks.
Though the current instantiation of the model does not incorporate how multiple hearings might change how a melody is dictated, one could constrain the process to only allow a certain number of iterations to reflect this.
Of course as a new melody is learned it is slowly being introduced into long term memory and could be completely be capable of being represented in long term memory without being explicitly notated at the end of a dictation with time running out and thus not possible to be completed.
This of course then would be imposing some sort of experimental constraint on the process and since this is meant to be a cognitive computational model of melodic dictation this caveat would complicate the model.
Future research could be done to optimize the choices that the model makes in order to satisfy whatever constraints are imposed and could be an interesting avenue of future research, but are beyond the initial goals of the model.

\hypertarget{model-output}{%
\subsection{Model Output}\label{model-output}}

The model then outputs each n-gram transcribed and can be counted as a series with less attempts mapping to an easier transcription.
I believe that this lines with many intuitions about the process of melodic dictation.
It first creates a linear mapping of attempts to dictate with difficulty of the melody.
It relies on a distinction between explicit and implicit statistical knowledge.
It is based on the Embedded Process Model from working memory and attention, so is part of a larger generative model, giving more credibility that this \emph{could} be how melodic dictation works.

\hypertarget{formal-model}{%
\section{Formal Model}\label{formal-model}}

Below I present the computational model in psudeocode as described in Figure 7.3.
First listed are the defined inputs, the functions needed to run the algorithm, and then the sequence the model runs.
To aid distinguishing between functions and objects, I put functions in italics and objects in bold.
Below the model in Figure 7.4, I provide a brief walk through of one iteration of the model.

\hypertarget{computational-model-1}{%
\subsection{Computational Model}\label{computational-model-1}}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/Model} 

}

\caption{Formal Model}\label{fig:unnamed-chunk-5}
\end{figure}

\hypertarget{example}{%
\subsection{Example}\label{example}}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/Model Graphic} 

}

\caption{Model Example}\label{fig:unnamed-chunk-6}
\end{figure}

The example above shows one iteration of the model run using the musical example from above using a hypothetical corpus for the pattern matching.
Using the model above, the following inputs were defined \emph{a priori}:

\begin{itemize}
\tightlist
\item
  The \textbf{Prior Knowledge} is a hypothetical corpus of symbolic strings representing all n-grams of melodies
\item
  The \textbf{Threshold} is set to \textbf{five} exact matches in the \textbf{Prior Knowledge}
\item
  The \textbf{WMC} is set at 17
\item
  The \textbf{Target Melody} is the Schubert excerpt from above
\item
  The \textbf{String Position} object is used to track the position in the dictation
\item
  The \textbf{Difficulty} object starts at 0
\item
  The \textbf{Dictation} object is \texttt{NULL} to begin, and each new n-gram successfully transcribed is annexed to it
\end{itemize}

Figure 7.4 progresses from left to right over the course of time.
The algorithm begins by first running the \texttt{listen()} function on the \textbf{Target Melody}.
First the model checks that there are notes to transcribe; this being the first loop of the model, this is statement will be \texttt{FALSE} so the next step is taken.
Notes of the \textbf{Target Melody} are read in to the \textbf{Selective Attention} buffer until the information content of the melody exceeds that of the working memory threshold.
This is depicted graphically in the leftmost panel of Figure 7.4.
Each note unfolding over time fills up the \textbf{Selective Attention} working memory buffer.
When the amount of information reaches the perceptual bottleneck-- as indicated by the dashed line-- the \textbf{Selective Attention} buffer stops receiving information.
At this point the model will mark where in the melody it stopped taking in new information for later.
Here the contents in \textbf{Selective Attention} are moved to the \texttt{transcribe()} function.

With the contents of \textbf{Selective Attention} passed to \texttt{transcribe()}, the model adds one to the counter indicating the first search is about to run.
Moving to the middle panel of Figure 7.4, the symbol string of notes in the first column are indexed against the \textbf{Prior Knowledge}.
Only if a five note pattern has appeared more than or equal to five times, as determined by the \textbf{Threshold} input, will the corresponding \texttt{EXPLICIT} column be \texttt{TRUE}.
In this case, this pattern has occurred over the threshold of 5 and thus a successful match is found.
It is at this step that the search resembles that of Cowan's model of working memory as active attention.
The pattern being searched for is compared against a vast amount of information, with cues from the contents of what is in \textbf{Selective Attention} grouping similar patterns together.
At the neural level, this is most likely a much more complex process, but to show this grouping I note that this search is at least organized by the first pitch.
I assume it would be reasonable that patterns starting on G as \(\hat{5}\)\footnote{As determined by being calculated against the corpus with both pitch and scale degree information} might happen together.
Since this string does have a \texttt{TRUE} match with \texttt{EXPLICIT}, the contents of \textbf{Selective Attention} are considered notated.
At this point the model would record the 5-gram, along with the string that it was matched with.
the function would then re-run the \texttt{listen} function via the \texttt{notateReentry()} function at the next point in the melody as tracked by the \textbf{String Position} object.

If there were not to have been an exact match, the model would remove one token from the melody and performed the search again on the knowledge of all 4-grams and add one to the \textbf{Difficulty} counter.
This process would happen recursively until a match is found.
If no match is found in either the complex representation, or that of the two rhythm and pitch corpora, the fifth step of \texttt{transcribe()} would trigger \texttt{notateReentry()} to be run without documenting the n-gram currently being dictated.
This would be akin to a student not being able to identify a difficult interval, thus having to restart the melody at a new position.
Decisions about re-entry warrant further research and discussion, but this model for the sake of parsimony, assumes linear continuation.
As notated in §7.3.5, other modes of re-entry could be incorporated into the model.

This looping process would occur again and again until the entire melody is notated.
With each iteration of each n-gram notated, the difficulty counter would increase in relation to the representation of that string in the corpus.
This provides an algorithmic implementation of a theorist's intuition that less common n-grams or intervals (2-grams) are going to lead to higher difficulty in dictation.
Also worth noting is steps 3 and 4 in the \texttt{transcribe()} function are akin to Karpinski's proto-notation.
Further research might consider advantages in the order of searching the \textbf{Prior Knowledge} corpora.

\hypertarget{conclusions-1}{%
\section{Conclusions}\label{conclusions-1}}

In this chapter, I presented an explanatory, computational model of melodic dictation.
The model combines work from computational musicology and work from cognitive psychology.
In addition to being a complete model that explicates every step of the dictation process, the model seems to match phenomenological intuitions as to the process of melodic dictation.
Given the current state of the model, it makes predictions about the dictation process and can eventually be implemented and tested against human behavioral data to provide evidence in support of its verisimilitude.
For example, the model predicts:

\begin{itemize}
\tightlist
\item
  Segments of melodies are likely successfully to be dictated relative to the frequency distribution of their prior knowledge.
\item
  Higher working memory span individuals will be able to dictate bigger chunks of melodies, and thus perform better at dictation
\item
  Using an \emph{atomistic} dictation will result not as effective dictations than attempting to identify larger patterns
\item
  Determining the difficulty of melodies of equal length is predictable from the frequency the melody's cumulative n-gram distribution.
\item
  Some \emph{atonal} melodies will be easier to dictate than tonal melodies if they consist of patterns that are more frequent in a listener's prior knowledge
\item
  Higher exposure to sight-singing results in more explicitly learned patterns, thus the ability to identify larger patterns of music
\end{itemize}

Although many of these hypotheses might seem intuitive to any instructor that has taught aural skills before, work from this dissertation provides a theory as to why each appears to be true.
Future research beyond this dissertation will explore further predictions of this work in more detail.
Most importantly from a pedagogical standpoint, the model and underlying theory gives exact language as to how and why melodic dictation works, which can serve as a valuable pedagogical and research contribution.

\hypertarget{reference-log}{%
\chapter{Reference Log}\label{reference-log}}

\hypertarget{to-incorporate}{%
\section{To Incorporate}\label{to-incorporate}}

\begin{itemize}
\tightlist
\item
  \citep{margulisModelMelodicExpectation2005} -- Margulis Model
\item
  \citep{nicholsScoreOneJazz2018} -- Specialty jazz background helps in tasks, WMC
\item
  \citep{NASM201718HandbookPdf2018} -- Fix intext
\item
  \citep{schumann1860musikalische} -- Quote about why people should do ear training
\item
  \citep{smith1934solfege} -- Quote from K2001 about why people should do ear training
\item
  \citep{longRelationshipsPitchMemory1977} -- Musical Characteristics predict memory
\item
  \citep{taylorStrategiesMemoryShort1983} -- Great citation that lots of things change memory, even structural!
\item
  \citep{tallaricoStudyThreePhase1974} -- Long boring talk on STM, LTM
\item
  \citep{ouraConstructingRepresentationMelody1991a} -- Awful experimental design that says people use structual tones
\item
  \citep{buonviriExplorationUndergraduateMusic2014} -- Call for experimental, suggestions as to what factors might contribute, use of deductive reasoning, qualitative
\item
  \citep{buonviriEffectsPreparatorySinging2015} -- People need to focus right away, not establish, distractors
\item
  \citep{buonviriEffectsMusicNotation2015} -- Showing people visual music does not help much.
\item
  \citep{buonviriEffectsTwoListening2017} -- Listening helps with other things, no best strategy in terms of writing
\item
  \citep{buonviriMelodicDictationInstruction2015} -- Literature to say people are bad at teaching melodic dictation and we don't know a lot about it, also interesting stuff about what solfege systems people use
\item
  \citep{davidbutlerWhyGulfMusic1997a} -- Call for music educators to do aural skills research, notes problem with aural skills pedagogy in lack of direction, also nice Nicholas Cook quotes on point of theory
\item
  \citep{furbyEffectsPeerTutoring2016} -- music ed study with weird stats, has references to follow up on with advantages of pitch systems and people who reccomend things for sight singing
\item
  \citep{pembrookInterferenceTranscriptionProcess1986} -- Effects of melodies, also how people do it. Interesting that they too effect of melodies, but talka bout things in terms of notes and not in terms of information content. Thought ot have an experiment where the n-grams that are more common are easier to write down. Lots of good charts too.
\item
  \citep{paneyEffectDirectingAttention2016} -- It's not good if you tell people what to do when they are dictating, article has a lot of good review for dictation materials to add to the `toRead' folder.
\item
  \citep{fournierCognitiveStrategiesSightsinging2017a} -- Good references that people are awful at Aural Skills, Also suggestions that people are not that great at transfer, and some stuff to suggest academic abililty is intertwined in all of this. Good reference for when starting to talk about untangling the mess that is aural skills.
\item
  \citep[ 1995]{berzWorkingMemoryMusic} -- Add on a new module to the WMC model of baddel with music, presents some evidence for why this theoretically should be included, but actually takes examples of dictation. A lot of this article felt like things that i was reinventing\ldots{}not good.
\item
  \citep{atkinsonSomeThoughtsOnTrying} -- Proof some other people are starting to think in terms of pedagogical schemas
\item
  \citep{klonoskiPerceptualLearningHierarchy2000} -- Music cognition needs to talk to aural skills more, also need to unbind theory routine with aural skills and think of things more as in a perceptual learning hierarchy
\item
  \citep{klonoskiImprovingDictationAuralSkills2006} -- great quotes that when people get something wrong with aural skills, what does that even mean, lack of transfer effects, article ends with ways to get better at things
\item
  \citep{pembrookSendHelpAural1990} -- Survey of what people in the late 1980s were doing in terms of aural skills pedagogy
\item
  \citep{karpinskiModelMelodicPerception1990} -- addresses why Gary Karpinski thinks we should teach melodic dictation
\item
  \citep{potterIdentifyingSucessfulDictation1990} -- dictation teacher surprised that people don't keep up their dictaiton skills quote
\end{itemize}

\hypertarget{chapter-3}{%
\section{Chapter 3}\label{chapter-3}}

\begin{itemize}
\item
  \citep{cowanWorkingMemoryCapacity2005} -- This book will probably serve as cornerstone of chapter in terms of creating relevant literature in addition to EE course readings on WMC. Provides history of WMC models and notes how attention based model as opposed to Baddely loop might actually be better theoretical model for talking about fact that WMC could just be something related to attention if not that. Provides extensive listing on problems with chunking that are all relevant to music, but then also supports it. Shows that Miller 1956 is a generally bad citation, own author even says that in Miller 1989 (check and add) and says limit is probably about 4 (use Cowan 2001 for ctation find that). Lots of good ideas like how music is always serial recall, examples of how to model the process, great discussions on zooming out and categorical nature of music within span of WMC ideas.
\item
  \citep{ockelfordMusicModuleWorking2007} -- uses case of savant to argue bits of Berz WM Music Model
\end{itemize}

\bibliography{book.bib,packages.bib}


\end{document}
